# Lec 08-04. GPT

- 강의영상 : https://www.youtube.com/watch?v=o_Wl29aW5XM&list=PLetSlH8YjIfVzHuSXtG4jAC2zbEAErXWm&index=20

- 강의자료 : 

- 이미지
	- http://jalammar.github.io/illustrated-gpt2/

- mkdate : 2020-06-24

---

## GPT

Generative Pre-Training of a Lanuage Model

- Backgrounds : unlabeled dataset에 비해 턱없이 적은 labeled dataset
- unlabeled text corpora를 통해 language model 을 pre-training
- labeled text corpora for a specific task를 활용해 fine-tuning

### ELMo - GPT 차이점

- ELMo : Bi-directional LSTM 활용한 language model
- GPT : Transformer의 decoder 부분만 사용, backward 사용하지 않고 forward 에 대해서도 masking 사용

## Unsupervised pre-training

### multi-layer Transformer decoder

![image](https://user-images.githubusercontent.com/57997672/85580021-5663d200-b676-11ea-9590-bfd7a8ebed08.png)

transformer decoder의 masked multi-head attention을 여러 개 쌓음

- token embedding matrix
- position embedding matrix
- context vector of tokens
- layers

### Decoder-only block

![image](https://user-images.githubusercontent.com/57997672/85580339-9a56d700-b676-11ea-9722-185b96bf09ab.png)

- 기존 Transformer와 다르게 Masked Self-Attetion 다음에 바로 Feed-Forward Neural Network 

- Decoder Block을 몇 층 쌓는지는 Hyper-parameter 문제

## Supervised fine-tuning

- label 이 있는 sequential data (input tokens) 

- pre-trained model을 통해 transformer block의 activation을 찾고, linear output layer에서 결과값을 예측
	- sequential tokens - linear layer - softmax

- ELMo의 경우 unsupervised learning으로 language model 만들고 이후, supervised learning

- GPT는 현재 downstream task의 목적함수 뿐만 아니라 pre-trained language model도 동시에 업데이트

	- supervised model의 생성 성능 향상
	- 학습 속도 향상
	
## Task-specific input Transformations

![image](https://user-images.githubusercontent.com/57997672/85583382-2e29a280-b679-11ea-89a1-ad9ca6aa68c8.png)



