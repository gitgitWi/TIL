{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DR9-ZHOvqZ4J",
        "colab_type": "text"
      },
      "source": [
        "# PyTorch로 시작하는 딥 러닝 입문\n",
        "\n",
        "- Book: https://wikidocs.net/book/2788\n",
        "- Chapter : https://wikidocs.net/53560\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4EsDm_IqwIX",
        "colab_type": "text"
      },
      "source": [
        "## 01. Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRy7bJJjqmmm",
        "colab_type": "text"
      },
      "source": [
        "개념\n",
        "\n",
        "### Data Definition\n",
        "\n",
        "#### Train-set, Test-set\n",
        "\n",
        "- 모든 train-set은 `torch.tensor` 형태를 갖고 있어야\n",
        "\n",
        "#### Hypothesis\n",
        "\n",
        "- $y = Wx + b$ \n",
        "- $H(x)=Wx+b$\n",
        "\n",
        "#### Compute Loss\n",
        "\n",
        "- Cost function == loss function == error function == objective function\n",
        "- MSE ; Mean Squared Error, 평균 제곱 오차\n",
        "- MSE를 최소화하는 곡선\n",
        "\n",
        "#### Optimizer - Gradient Descent\n",
        "\n",
        "- basic optimizer algorithm\n",
        "- learning rate; 학습률, W-cost 곡선에서 W 값을 얼마나 크게 움직일지 결정\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGWBGEcXqtE9",
        "colab_type": "text"
      },
      "source": [
        "### PyTorch로 구현하기\n",
        "\n",
        "#### Initial Setting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkp1TQVhqtHk",
        "colab_type": "code",
        "outputId": "376cf27c-3995-4372-dfc3-5c5f1f3b1aae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "<torch._C.Generator at 0x7f00f4e247b0>"
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "64cWG5giwc2g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import numpy.random as rd"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXHz0yyEw9EI",
        "colab_type": "text"
      },
      "source": [
        "#### train-set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kz8BF9oqtKJ",
        "colab_type": "code",
        "outputId": "68a8d533-1bea-4147-c7bd-fee7514cedc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "x_train = torch.FloatTensor(np.arange(1, 4).reshape(3,-1))\n",
        "y_train = torch.FloatTensor(np.arange(1, 4).reshape(3,-1)*2)\n",
        "\n",
        "x_train, y_train"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[1.],\n",
              "         [2.],\n",
              "         [3.]]), tensor([[2.],\n",
              "         [4.],\n",
              "         [6.]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sx0Mb2OqtMP",
        "colab_type": "text"
      },
      "source": [
        "#### 가중치 W, 편향 b 초기화\n",
        "\n",
        "`torch.zeros`\n",
        "\n",
        "- `requires_grad=True` : 학습을 통해 값이 변경되는 변수임을 명시"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5uERp78qnmx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.zeros?"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hh2y7COJqnjf",
        "colab_type": "code",
        "outputId": "3d3b0546-f53d-4766-8bbc-f741acd08ca9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "W = torch.zeros(1, requires_grad=True)\n",
        "W"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VSBSM3BAqnq4",
        "colab_type": "code",
        "outputId": "5ea69b79-d72c-4608-f018-d860ed044e67",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "b = torch.zeros(1, requires_grad=True)\n",
        "b"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uh2GWZh0qnst",
        "colab_type": "text"
      },
      "source": [
        "#### set Hypothesis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEzRw9iUqnvu",
        "colab_type": "code",
        "outputId": "1839e80f-6549-4faa-ca5e-c95629955e56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "hypo = x_train * W + b\n",
        "hypo"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.],\n",
              "        [0.],\n",
              "        [0.]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JV_rFwGvqnze",
        "colab_type": "text"
      },
      "source": [
        "#### declare cost function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HGesTZQxqn1Q",
        "colab_type": "code",
        "outputId": "9cac3daf-c5c8-4d9e-ecef-577e698793e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cost = torch.mean( (hypo - y_train) ** 2 )\n",
        "cost"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(18.6667, grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PS8ubzFVya2N",
        "colab_type": "text"
      },
      "source": [
        "#### implement Gradient Descent\n",
        "\n",
        "```python\n",
        "optim.SGD(\n",
        "    params,\n",
        "    lr=torch.optim.optimizer._RequiredParameter instance, \n",
        "    momentum=0, \n",
        "    dampening=0, \n",
        "    weight_decay=0, \n",
        "    nesterov=False\n",
        "    )\n",
        "```\n",
        "\n",
        "- lr : learning rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ei5r90Crywn1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "??optim.SGD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsWgECieyoxd",
        "colab_type": "code",
        "outputId": "ae187b89-f8fe-4fdd-ff5d-5082347ae94c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "optimizer = optim.SGD([W, b], lr=0.01)\n",
        "optimizer"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGD (\n",
              "Parameter Group 0\n",
              "    dampening: 0\n",
              "    lr: 0.01\n",
              "    momentum: 0\n",
              "    nesterov: False\n",
              "    weight_decay: 0\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9i0YV0Z1UYW",
        "colab_type": "text"
      },
      "source": [
        "- `optimizer.zero_grad()`\n",
        "    - 미분 값 gradient를 0으로 초기화\n",
        "    - Pytorch는 gradient 값을 이전 gradient 값에 누적시키는 특징이 있으므로.. (??)\n",
        "- `cost.backward()` : 비용함수 미분하여 gradient 계산\n",
        "- `optimizer.step()` : update W, b"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UvIH5tfhyu5Q",
        "colab_type": "code",
        "outputId": "cb5f2c63-8465-4848-8a86-4eeaf28f6e3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "epochs = 3000\n",
        "\n",
        "for epoch in range(epochs + 1) :\n",
        "    hypo = x_train * W + b\n",
        "    cost = torch.mean((hypo - y_train) ** 2)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    cost.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 100 == 0 :\n",
        "        print (f\"Epoch {epoch:4d} / {epochs} \\nW : {W.item():.3f}, b : {b.item():.3f}, cost : {cost.item():.6f}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch    0 / 3000 \n",
            "W : 0.187, b : 0.080, cost : 18.666666\n",
            "Epoch  100 / 3000 \n",
            "W : 1.746, b : 0.578, cost : 0.048171\n",
            "Epoch  200 / 3000 \n",
            "W : 1.800, b : 0.454, cost : 0.029767\n",
            "Epoch  300 / 3000 \n",
            "W : 1.843, b : 0.357, cost : 0.018394\n",
            "Epoch  400 / 3000 \n",
            "W : 1.876, b : 0.281, cost : 0.011366\n",
            "Epoch  500 / 3000 \n",
            "W : 1.903, b : 0.221, cost : 0.007024\n",
            "Epoch  600 / 3000 \n",
            "W : 1.924, b : 0.174, cost : 0.004340\n",
            "Epoch  700 / 3000 \n",
            "W : 1.940, b : 0.136, cost : 0.002682\n",
            "Epoch  800 / 3000 \n",
            "W : 1.953, b : 0.107, cost : 0.001657\n",
            "Epoch  900 / 3000 \n",
            "W : 1.963, b : 0.084, cost : 0.001024\n",
            "Epoch 1000 / 3000 \n",
            "W : 1.971, b : 0.066, cost : 0.000633\n",
            "Epoch 1100 / 3000 \n",
            "W : 1.977, b : 0.052, cost : 0.000391\n",
            "Epoch 1200 / 3000 \n",
            "W : 1.982, b : 0.041, cost : 0.000242\n",
            "Epoch 1300 / 3000 \n",
            "W : 1.986, b : 0.032, cost : 0.000149\n",
            "Epoch 1400 / 3000 \n",
            "W : 1.989, b : 0.025, cost : 0.000092\n",
            "Epoch 1500 / 3000 \n",
            "W : 1.991, b : 0.020, cost : 0.000057\n",
            "Epoch 1600 / 3000 \n",
            "W : 1.993, b : 0.016, cost : 0.000035\n",
            "Epoch 1700 / 3000 \n",
            "W : 1.995, b : 0.012, cost : 0.000022\n",
            "Epoch 1800 / 3000 \n",
            "W : 1.996, b : 0.010, cost : 0.000013\n",
            "Epoch 1900 / 3000 \n",
            "W : 1.997, b : 0.008, cost : 0.000008\n",
            "Epoch 2000 / 3000 \n",
            "W : 1.997, b : 0.006, cost : 0.000005\n",
            "Epoch 2100 / 3000 \n",
            "W : 1.998, b : 0.005, cost : 0.000003\n",
            "Epoch 2200 / 3000 \n",
            "W : 1.998, b : 0.004, cost : 0.000002\n",
            "Epoch 2300 / 3000 \n",
            "W : 1.999, b : 0.003, cost : 0.000001\n",
            "Epoch 2400 / 3000 \n",
            "W : 1.999, b : 0.002, cost : 0.000001\n",
            "Epoch 2500 / 3000 \n",
            "W : 1.999, b : 0.002, cost : 0.000000\n",
            "Epoch 2600 / 3000 \n",
            "W : 1.999, b : 0.001, cost : 0.000000\n",
            "Epoch 2700 / 3000 \n",
            "W : 2.000, b : 0.001, cost : 0.000000\n",
            "Epoch 2800 / 3000 \n",
            "W : 2.000, b : 0.001, cost : 0.000000\n",
            "Epoch 2900 / 3000 \n",
            "W : 2.000, b : 0.001, cost : 0.000000\n",
            "Epoch 3000 / 3000 \n",
            "W : 2.000, b : 0.001, cost : 0.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hvxn9rK900VQ",
        "colab_type": "text"
      },
      "source": [
        "## 02. 자동 미분 Autograd\n",
        "\n",
        "`requires_grad=`, `backward()` 등\n",
        "\n",
        "- `requires_grad=` : `grad` 에 gradient 값 저장\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yi7kiNjP3QMX",
        "colab_type": "code",
        "outputId": "8950faec-a522-4a43-a459-21ecc0be046e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "auto_w = torch.tensor(2.0, requires_grad=True)\n",
        "\n",
        "for i in range(4) : \n",
        "    auto_y = auto_w**2\n",
        "    auto_z = 2 * auto_y + 5\n",
        "    auto_z.backward()\n",
        "    print (f\"수식 auto_z를 auto_w로 미분한 값 : {auto_w.grad}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "수식 auto_z를 auto_w로 미분한 값 : 8.0\n",
            "수식 auto_z를 auto_w로 미분한 값 : 16.0\n",
            "수식 auto_z를 auto_w로 미분한 값 : 24.0\n",
            "수식 auto_z를 auto_w로 미분한 값 : 32.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJe_YMdr67B_",
        "colab_type": "code",
        "outputId": "d2eafff5-e023-4bd0-dae5-c7b08be8b4b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "noauto_w = torch.tensor(2.0, requires_grad=False)\n",
        "\n",
        "for i in range(4) : \n",
        "    noauto_y = auto_w**2\n",
        "    noauto_z = 2 * noauto_y + 5\n",
        "    noauto_z.backward()\n",
        "    print (f\"수식 auto_z를 auto_w로 미분한 값 : {noauto_w.grad}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "수식 auto_z를 auto_w로 미분한 값 : None\n",
            "수식 auto_z를 auto_w로 미분한 값 : None\n",
            "수식 auto_z를 auto_w로 미분한 값 : None\n",
            "수식 auto_z를 auto_w로 미분한 값 : None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zt0L7JbD7f6X",
        "colab_type": "text"
      },
      "source": [
        "## 03. Multivariable Linear Regression\n",
        "\n",
        "### 03-01. Data Definition\n",
        "\n",
        "### 03-02. Implement in PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXC8mw9C-K-i",
        "colab_type": "text"
      },
      "source": [
        "make dummy train-set\n",
        "\n",
        "_train-set에 3자리수 데이터를 주면 답을 못찾고 nan만 뿜어댐..._"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtI5R81b9X7X",
        "colab_type": "code",
        "outputId": "38d97790-5a9f-4400-d377-ec4f22869f19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "x1_train = torch.FloatTensor(np.random.randint(10, 100, (5, 1)))\n",
        "print (x1_train)\n",
        "\n",
        "x2_train = torch.FloatTensor(np.random.randint(10, 100, (5, 1)))\n",
        "print (x2_train)\n",
        "\n",
        "x3_train = torch.FloatTensor(np.random.randint(10, 100, (5, 1)))\n",
        "print (x3_train)\n",
        "\n",
        "# x1_train = torch.FloatTensor([[73], [93], [89], [96], [73]])\n",
        "# x2_train = torch.FloatTensor([[80], [88], [91], [98], [66]])\n",
        "# x3_train = torch.FloatTensor([[75], [93], [90], [100], [70]])\n",
        "# y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
        "\n",
        "# x2_train = torch.randint(101, 1000, (5,1), dtype=float)\n",
        "# x3_train = torch.randint(101, 1000, (5,1), dtype=float)\n",
        "\n",
        "y_train = torch.FloatTensor (x1_train + x2_train + x3_train)\n",
        "\n",
        "# x1_train = torch.FloatTensor([[776],         [923],         [555],         [946],         [920]])\n",
        "# x2_train = torch.FloatTensor([[879],        [887],        [367],        [482],        [867]])\n",
        "# x3_train = torch.FloatTensor([[313],        [676],        [265],        [560],        [534]])\n",
        "# y_train = torch.FloatTensor ([[1968], [2486], [1187], [1988], [2321]])\n",
        "\n",
        "print (y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[11.],\n",
            "        [97.],\n",
            "        [54.],\n",
            "        [31.],\n",
            "        [75.]])\n",
            "tensor([[38.],\n",
            "        [13.],\n",
            "        [25.],\n",
            "        [46.],\n",
            "        [77.]])\n",
            "tensor([[16.],\n",
            "        [80.],\n",
            "        [96.],\n",
            "        [83.],\n",
            "        [34.]])\n",
            "tensor([[ 65.],\n",
            "        [190.],\n",
            "        [175.],\n",
            "        [160.],\n",
            "        [186.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QP4auwW09mV_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print (x1_train, \"\\n\", x2_train,\"\\n\", x3_train,  \"\\n\", y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bu2L_V1r-PoK",
        "colab_type": "text"
      },
      "source": [
        "initialize w1, w2, w3, b"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRky51Rs9np3",
        "colab_type": "code",
        "outputId": "294fc1a4-a0d6-44f5-9c14-50f7d5fc7e7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "w1 = torch.zeros(1, requires_grad=True)\n",
        "w2 = torch.zeros(1, requires_grad=True)\n",
        "w3 = torch.zeros(1, requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)\n",
        "\n",
        "b"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7ycsand-e9B",
        "colab_type": "text"
      },
      "source": [
        "declare optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdFMgCgE-Jc4",
        "colab_type": "code",
        "outputId": "a2cd3ccc-2d60-4ea6-e6f2-c9373609974b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "opti = optim.SGD([w1,w2,w3,b], lr=1e-5)\n",
        "opti"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGD (\n",
              "Parameter Group 0\n",
              "    dampening: 0\n",
              "    lr: 1e-05\n",
              "    momentum: 0\n",
              "    nesterov: False\n",
              "    weight_decay: 0\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 136
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1x4zCnR-i_P",
        "colab_type": "text"
      },
      "source": [
        "execute gradient descent 10,000 times"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IxJFrIE-hnI",
        "colab_type": "code",
        "outputId": "8cde88ec-7f3e-4c73-e95d-83827b36f4b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "epochs = int(1e+5)\n",
        "\n",
        "for epo in range(1, epochs + 1) :\n",
        "    hypo = x1_train * w1 + x2_train * w2 + x3_train * w3 + b\n",
        "    cost = torch.mean((hypo - y_train) ** 2)\n",
        "    # print (cost)\n",
        "\n",
        "    opti.zero_grad()\n",
        "    cost.backward()\n",
        "    opti.step()\n",
        "\n",
        "    if epo % 100 == 0 or epo == 1: \n",
        "        print (f\"Epoch : {epo:4d}/{epochs}\\nw1 : {w1.item():.3f} w2 : {w2.item():.3f} w3 : {w3.item():.3f} b : {b.item():.3f} cost : {cost.item():.6f}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch :    1/100000\n",
            "w1 : 0.190 w2 : 0.124 w3 : 0.211 b : 0.003 cost : 26229.199219\n",
            "Epoch :  100/100000\n",
            "w1 : 1.018 w2 : 0.938 w3 : 1.018 b : 0.019 cost : 3.952382\n",
            "Epoch :  200/100000\n",
            "w1 : 1.007 w2 : 0.988 w3 : 1.000 b : 0.019 cost : 0.140882\n",
            "Epoch :  300/100000\n",
            "w1 : 1.003 w2 : 0.997 w3 : 0.999 b : 0.019 cost : 0.008583\n",
            "Epoch :  400/100000\n",
            "w1 : 1.001 w2 : 0.999 w3 : 0.999 b : 0.019 cost : 0.000809\n",
            "Epoch :  500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000105\n",
            "Epoch :  600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000030\n",
            "Epoch :  700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000022\n",
            "Epoch :  800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch :  900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 1000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 1100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 1200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 1300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 1400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 1500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 1600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 1700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 1800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 1900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 2000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 2100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 2200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 2300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 2400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 2500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 2600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 2700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 2800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 2900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 3000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 3100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 3200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 3300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 3400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 3500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 3600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 3700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 3800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 3900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 4000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 4100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 4200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 4300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 4400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 4500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 4600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 4700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 4800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 4900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 5000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 5100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 5200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 5300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 5400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 5500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 5600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 5700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 5800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 5900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 6000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 6100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 6200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 6300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 6400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 6500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 6600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 6700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 6800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 6900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 7000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 7100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 7200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 7300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 7400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 7500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 7600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 7700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 7800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 7900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 8000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 8100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 8200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 8300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 8400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 8500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 8600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 8700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 8800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 8900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 9000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 9100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 9200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 9300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 9400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 9500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 9600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 9700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 9800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 9900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 10000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 10100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 10200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 10300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 10400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 10500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 10600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 10700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 10800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 10900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 11000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 11100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 11200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 11300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 11400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 11500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 11600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 11700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 11800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 11900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 12000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 12100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 12200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 12300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 12400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 12500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 12600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 12700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000021\n",
            "Epoch : 12800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 12900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 13000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 13100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 13200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 13300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 13400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 13500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 13600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 13700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 13800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 13900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 14000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 14100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 14200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 14300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 14400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 14500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 14600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 14700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 14800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 14900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 15000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 15100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 15200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 15300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 15400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 15500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 15600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 15700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 15800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 15900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 16000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 16100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 16200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 16300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 16400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 16500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 16600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 16700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 16800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 16900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 17000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 17100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 17200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 17300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 17400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 17500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 17600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 17700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 17800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 17900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 18000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 18100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 18200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 18300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 18400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 18500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 18600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 18700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 18800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 18900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 19000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 19100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 19200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 19300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 19400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 19500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 19600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 19700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 19800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 19900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 20000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 20100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 20200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 20300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 20400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 20500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 20600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 20700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 20800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 20900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 21000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 21100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 21200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 21300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 21400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 21500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 21600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 21700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 21800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 21900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 22000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 22100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 22200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 22300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 22400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 22500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 22600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 22700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 22800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 22900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 23000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 23100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 23200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 23300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 23400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 23500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 23600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 23700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 23800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 23900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 24000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 24100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 24200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 24300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 24400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 24500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 24600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 24700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 24800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 24900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 25000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 25100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 25200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 25300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 25400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 25500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 25600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 25700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 25800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 25900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 26000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 26100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 26200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 26300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 26400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 26500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 26600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 26700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 26800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 26900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 27000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 27100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 27200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 27300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 27400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 27500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 27600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 27700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 27800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 27900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 28000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 28100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 28200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 28300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 28400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 28500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 28600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 28700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 28800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 28900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 29000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 29100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 29200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 29300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 29400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 29500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 29600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 29700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 29800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 29900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 30000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 30100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 30200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 30300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 30400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 30500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 30600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 30700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 30800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 30900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 31000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 31100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 31200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 31300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 31400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 31500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 31600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 31700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 31800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 31900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 32000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 32100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 32200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 32300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 32400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 32500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 32600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 32700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 32800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 32900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 33000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 33100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 33200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 33300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 33400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 33500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 33600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 33700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 33800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 33900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 34000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 34100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 34200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 34300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 34400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 34500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 34600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 34700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 34800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000020\n",
            "Epoch : 34900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.019 cost : 0.000019\n",
            "Epoch : 35000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 35100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 35200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 35300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 35400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 35500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 35600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 35700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 35800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 35900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 36000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 36100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 36200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 36300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 36400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 36500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 36600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 36700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 36800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 36900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 37000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 37100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 37200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 37300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 37400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 37500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 37600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 37700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 37800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 37900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 38000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 38100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 38200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 38300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 38400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 38500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 38600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 38700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 38800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 38900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 39000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 39100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 39200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 39300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 39400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 39500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 39600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 39700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 39800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 39900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 40000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 40100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 40200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 40300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 40400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 40500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 40600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 40700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 40800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 40900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 41000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 41100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 41200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 41300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 41400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 41500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 41600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 41700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 41800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 41900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 42000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 42100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 42200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 42300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 42400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 42500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 42600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 42700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 42800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 42900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 43000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 43100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 43200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 43300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 43400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 43500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 43600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 43700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 43800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 43900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 44000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 44100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 44200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 44300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 44400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 44500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 44600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 44700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 44800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 44900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 45000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 45100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 45200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 45300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 45400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 45500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 45600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 45700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 45800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 45900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 46000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 46100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 46200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 46300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 46400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 46500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 46600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 46700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 46800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 46900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 47000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 47100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 47200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 47300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 47400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 47500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 47600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 47700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 47800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 47900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 48000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 48100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 48200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 48300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 48400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 48500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 48600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 48700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 48800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 48900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 49000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 49100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 49200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 49300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 49400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 49500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 49600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 49700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 49800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 49900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 50000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 50100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 50200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 50300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 50400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 50500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 50600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 50700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 50800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 50900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 51000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 51100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 51200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 51300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 51400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 51500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 51600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 51700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 51800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 51900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 52000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 52100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 52200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 52300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 52400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 52500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 52600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 52700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 52800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 52900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 53000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 53100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 53200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 53300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 53400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 53500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 53600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 53700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 53800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 53900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 54000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 54100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 54200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 54300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 54400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 54500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 54600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 54700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 54800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 54900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 55000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 55100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 55200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 55300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 55400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 55500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 55600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 55700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 55800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 55900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 56000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 56100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 56200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 56300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 56400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 56500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 56600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 56700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 56800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 56900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 57000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 57100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 57200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 57300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 57400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 57500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 57600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 57700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 57800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 57900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 58000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 58100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 58200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 58300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 58400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 58500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 58600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 58700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000019\n",
            "Epoch : 58800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 58900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 59000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 59100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 59200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 59300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 59400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 59500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 59600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 59700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 59800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 59900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 60000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 60100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 60200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 60300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 60400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 60500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 60600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 60700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 60800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 60900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 61000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 61100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 61200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 61300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 61400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 61500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 61600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 61700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 61800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 61900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 62000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 62100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 62200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 62300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 62400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 62500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 62600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 62700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 62800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 62900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 63000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 63100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 63200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 63300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 63400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 63500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 63600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 63700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 63800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 63900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 64000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 64100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 64200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 64300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 64400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 64500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 64600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 64700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 64800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 64900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 65000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 65100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 65200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 65300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 65400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 65500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 65600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 65700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 65800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 65900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 66000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 66100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 66200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 66300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 66400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 66500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 66600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 66700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 66800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 66900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 67000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 67100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 67200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 67300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 67400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 67500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 67600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 67700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 67800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 67900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 68000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 68100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 68200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 68300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 68400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 68500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 68600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 68700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 68800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 68900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 69000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 69100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 69200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 69300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 69400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 69500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 69600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 69700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 69800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 69900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 70000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 70100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 70200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 70300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 70400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 70500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 70600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 70700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 70800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 70900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 71000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 71100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 71200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 71300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 71400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 71500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 71600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 71700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 71800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 71900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 72000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 72100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 72200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 72300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 72400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 72500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 72600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 72700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 72800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 72900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 73000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 73100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 73200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 73300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 73400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 73500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 73600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 73700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 73800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 73900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 74000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 74100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 74200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 74300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 74400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 74500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 74600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 74700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 74800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 74900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 75000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 75100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 75200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 75300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 75400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 75500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 75600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 75700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 75800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 75900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 76000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 76100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 76200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 76300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 76400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 76500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 76600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 76700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 76800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 76900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 77000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 77100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 77200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 77300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 77400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 77500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 77600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 77700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 77800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 77900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 78000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 78100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 78200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 78300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 78400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 78500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 78600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 78700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 78800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 78900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 79000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 79100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 79200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 79300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 79400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 79500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 79600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 79700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 79800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 79900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 80000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 80100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 80200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 80300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 80400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 80500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 80600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 80700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 80800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 80900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 81000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 81100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 81200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 81300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 81400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 81500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 81600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 81700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 81800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 81900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 82000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 82100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 82200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 82300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 82400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 82500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 82600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000017\n",
            "Epoch : 82700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000017\n",
            "Epoch : 82800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 82900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000018\n",
            "Epoch : 83000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000017\n",
            "Epoch : 83100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000017\n",
            "Epoch : 83200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000017\n",
            "Epoch : 83300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000017\n",
            "Epoch : 83400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000017\n",
            "Epoch : 83500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000017\n",
            "Epoch : 83600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000017\n",
            "Epoch : 83700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000017\n",
            "Epoch : 83800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000017\n",
            "Epoch : 83900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000017\n",
            "Epoch : 84000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.018 cost : 0.000017\n",
            "Epoch : 84100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 84200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 84300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 84400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 84500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 84600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 84700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 84800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 84900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 85000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 85100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 85200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 85300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 85400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 85500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 85600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 85700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 85800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 85900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 86000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 86100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 86200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 86300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 86400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 86500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 86600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 86700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 86800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 86900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 87000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 87100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 87200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 87300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 87400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 87500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 87600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 87700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 87800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 87900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 88000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 88100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 88200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 88300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 88400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 88500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 88600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 88700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 88800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 88900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 89000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 89100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 89200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 89300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 89400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 89500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 89600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 89700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 89800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 89900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 90000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 90100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 90200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 90300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 90400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 90500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 90600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 90700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 90800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 90900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 91000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 91100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 91200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 91300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 91400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 91500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 91600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 91700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 91800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 91900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 92000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 92100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 92200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 92300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 92400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 92500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 92600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 92700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 92800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 92900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 93000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 93100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 93200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 93300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 93400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 93500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 93600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 93700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 93800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 93900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 94000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 94100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 94200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 94300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 94400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 94500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 94600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 94700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 94800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 94900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 95000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 95100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 95200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 95300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 95400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 95500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 95600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 95700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 95800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 95900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 96000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 96100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 96200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 96300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 96400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 96500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 96600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 96700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 96800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 96900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 97000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 97100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 97200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 97300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 97400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 97500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 97600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 97700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 97800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 97900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 98000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 98100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 98200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 98300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 98400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 98500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 98600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 98700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 98800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 98900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 99000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 99100/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 99200/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 99300/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 99400/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 99500/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 99600/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 99700/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 99800/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 99900/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n",
            "Epoch : 100000/100000\n",
            "w1 : 1.000 w2 : 1.000 w3 : 1.000 b : 0.017 cost : 0.000017\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sCSavQtB2ep",
        "colab_type": "text"
      },
      "source": [
        "### 03-03. 벡터와 행렬 연산으로 바꾸기\n",
        "\n",
        "- 위의 코드는 변수x와 가중치w 수가 수천 개일 경우, 굉장히 비효율적\n",
        "- 이를 위해 벡터의 내적(Dot Product) 사용할 필요\n",
        "- 종속변수 x의 수 = samples (data 단위) * features (독립변수 x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-D8zRBSeODp",
        "colab_type": "text"
      },
      "source": [
        "### 03-04. 행렬 연산 고려하여 PyTorch로 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "du9K5ESEeSv6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bf01f776-f614-41fd-d7ae-f25ba9d316ca"
      },
      "source": [
        "x_train  =  torch.FloatTensor([[73,  80,  75], \n",
        "                               [93,  88,  93], \n",
        "                               [89,  91,  90], \n",
        "                               [96,  98,  100],   \n",
        "                               [73,  66,  70]])  \n",
        "y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])\n",
        "\n",
        "x_train.shape, y_train.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([5, 3]), torch.Size([5, 1]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVgxJlWMfbzC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "W = torch.zeros((3,1), requires_grad=True)\n",
        "b = torch.zeros(1, requires_grad=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuZqySQAlPsq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "d76bac9e-bc66-4d8a-e1f2-86b6320d7caa"
      },
      "source": [
        "opti = optim.SGD([W, b], lr=1e-5)\n",
        "opti"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SGD (\n",
              "Parameter Group 0\n",
              "    dampening: 0\n",
              "    lr: 1e-05\n",
              "    momentum: 0\n",
              "    nesterov: False\n",
              "    weight_decay: 0\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KOet71Zlg0R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7fb8dc22-dc41-48dc-e7bd-a7c1906b4d2c"
      },
      "source": [
        "epochs = int(1e+5)\n",
        "\n",
        "for epo in range(1, epochs+1) :\n",
        "    hypo = x_train.matmul(W) + b\n",
        "\n",
        "    cost = torch.mean((hypo - y_train) ** 2)\n",
        "\n",
        "    opti.zero_grad()\n",
        "    cost.backward()\n",
        "    opti.step()\n",
        "\n",
        "    if epo % 100 == 0 :\n",
        "        print (f\"Epoch : {epo:4d}/{epochs}\\nhypothesis : {hypo.squeeze().detach()} cost : {cost.item():.6f}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch :  100/100000\n",
            "hypothesis : tensor([152.7695, 183.6982, 180.9592, 197.0628, 140.1332]) cost : 1.564299\n",
            "Epoch :  200/100000\n",
            "hypothesis : tensor([152.7277, 183.7271, 180.9466, 197.0518, 140.1727]) cost : 1.498234\n",
            "Epoch :  300/100000\n",
            "hypothesis : tensor([152.6870, 183.7551, 180.9344, 197.0410, 140.2112]) cost : 1.435647\n",
            "Epoch :  400/100000\n",
            "hypothesis : tensor([152.6474, 183.7825, 180.9225, 197.0305, 140.2487]) cost : 1.376296\n",
            "Epoch :  500/100000\n",
            "hypothesis : tensor([152.6089, 183.8091, 180.9109, 197.0202, 140.2852]) cost : 1.320047\n",
            "Epoch :  600/100000\n",
            "hypothesis : tensor([152.5714, 183.8349, 180.8997, 197.0102, 140.3208]) cost : 1.266736\n",
            "Epoch :  700/100000\n",
            "hypothesis : tensor([152.5350, 183.8601, 180.8888, 197.0004, 140.3554]) cost : 1.216203\n",
            "Epoch :  800/100000\n",
            "hypothesis : tensor([152.4995, 183.8846, 180.8781, 196.9908, 140.3891]) cost : 1.168279\n",
            "Epoch :  900/100000\n",
            "hypothesis : tensor([152.4651, 183.9085, 180.8678, 196.9815, 140.4220]) cost : 1.122853\n",
            "Epoch : 1000/100000\n",
            "hypothesis : tensor([152.4315, 183.9316, 180.8578, 196.9724, 140.4540]) cost : 1.079796\n",
            "Epoch : 1100/100000\n",
            "hypothesis : tensor([152.3989, 183.9542, 180.8481, 196.9634, 140.4852]) cost : 1.038984\n",
            "Epoch : 1200/100000\n",
            "hypothesis : tensor([152.3672, 183.9761, 180.8386, 196.9547, 140.5156]) cost : 1.000274\n",
            "Epoch : 1300/100000\n",
            "hypothesis : tensor([152.3363, 183.9975, 180.8293, 196.9462, 140.5452]) cost : 0.963566\n",
            "Epoch : 1400/100000\n",
            "hypothesis : tensor([152.3063, 184.0182, 180.8204, 196.9378, 140.5740]) cost : 0.928776\n",
            "Epoch : 1500/100000\n",
            "hypothesis : tensor([152.2771, 184.0384, 180.8117, 196.9297, 140.6021]) cost : 0.895772\n",
            "Epoch : 1600/100000\n",
            "hypothesis : tensor([152.2488, 184.0581, 180.8032, 196.9217, 140.6294]) cost : 0.864470\n",
            "Epoch : 1700/100000\n",
            "hypothesis : tensor([152.2211, 184.0772, 180.7950, 196.9139, 140.6560]) cost : 0.834805\n",
            "Epoch : 1800/100000\n",
            "hypothesis : tensor([152.1943, 184.0957, 180.7870, 196.9063, 140.6820]) cost : 0.806655\n",
            "Epoch : 1900/100000\n",
            "hypothesis : tensor([152.1682, 184.1138, 180.7792, 196.8988, 140.7073]) cost : 0.779955\n",
            "Epoch : 2000/100000\n",
            "hypothesis : tensor([152.1428, 184.1314, 180.7717, 196.8915, 140.7319]) cost : 0.754632\n",
            "Epoch : 2100/100000\n",
            "hypothesis : tensor([152.1181, 184.1485, 180.7644, 196.8844, 140.7559]) cost : 0.730605\n",
            "Epoch : 2200/100000\n",
            "hypothesis : tensor([152.0941, 184.1652, 180.7572, 196.8774, 140.7793]) cost : 0.707816\n",
            "Epoch : 2300/100000\n",
            "hypothesis : tensor([152.0707, 184.1814, 180.7503, 196.8706, 140.8021]) cost : 0.686199\n",
            "Epoch : 2400/100000\n",
            "hypothesis : tensor([152.0481, 184.1971, 180.7436, 196.8639, 140.8243]) cost : 0.665688\n",
            "Epoch : 2500/100000\n",
            "hypothesis : tensor([152.0260, 184.2124, 180.7370, 196.8574, 140.8459]) cost : 0.646216\n",
            "Epoch : 2600/100000\n",
            "hypothesis : tensor([152.0045, 184.2274, 180.7307, 196.8510, 140.8670]) cost : 0.627752\n",
            "Epoch : 2700/100000\n",
            "hypothesis : tensor([151.9836, 184.2419, 180.7245, 196.8447, 140.8876]) cost : 0.610217\n",
            "Epoch : 2800/100000\n",
            "hypothesis : tensor([151.9633, 184.2559, 180.7186, 196.8386, 140.9076]) cost : 0.593582\n",
            "Epoch : 2900/100000\n",
            "hypothesis : tensor([151.9436, 184.2697, 180.7127, 196.8326, 140.9271]) cost : 0.577790\n",
            "Epoch : 3000/100000\n",
            "hypothesis : tensor([151.9244, 184.2830, 180.7071, 196.8267, 140.9461]) cost : 0.562796\n",
            "Epoch : 3100/100000\n",
            "hypothesis : tensor([151.9058, 184.2959, 180.7016, 196.8210, 140.9646]) cost : 0.548565\n",
            "Epoch : 3200/100000\n",
            "hypothesis : tensor([151.8876, 184.3086, 180.6963, 196.8154, 140.9827]) cost : 0.535049\n",
            "Epoch : 3300/100000\n",
            "hypothesis : tensor([151.8700, 184.3208, 180.6911, 196.8098, 141.0003]) cost : 0.522219\n",
            "Epoch : 3400/100000\n",
            "hypothesis : tensor([151.8529, 184.3327, 180.6861, 196.8044, 141.0174]) cost : 0.510023\n",
            "Epoch : 3500/100000\n",
            "hypothesis : tensor([151.8362, 184.3443, 180.6812, 196.7991, 141.0341]) cost : 0.498441\n",
            "Epoch : 3600/100000\n",
            "hypothesis : tensor([151.8200, 184.3556, 180.6765, 196.7939, 141.0504]) cost : 0.487442\n",
            "Epoch : 3700/100000\n",
            "hypothesis : tensor([151.8043, 184.3666, 180.6719, 196.7889, 141.0663]) cost : 0.477002\n",
            "Epoch : 3800/100000\n",
            "hypothesis : tensor([151.7890, 184.3773, 180.6674, 196.7839, 141.0818]) cost : 0.467054\n",
            "Epoch : 3900/100000\n",
            "hypothesis : tensor([151.7741, 184.3876, 180.6631, 196.7790, 141.0969]) cost : 0.457613\n",
            "Epoch : 4000/100000\n",
            "hypothesis : tensor([151.7597, 184.3977, 180.6589, 196.7742, 141.1116]) cost : 0.448643\n",
            "Epoch : 4100/100000\n",
            "hypothesis : tensor([151.7456, 184.4075, 180.6548, 196.7695, 141.1259]) cost : 0.440117\n",
            "Epoch : 4200/100000\n",
            "hypothesis : tensor([151.7319, 184.4171, 180.6508, 196.7649, 141.1399]) cost : 0.432000\n",
            "Epoch : 4300/100000\n",
            "hypothesis : tensor([151.7187, 184.4263, 180.6469, 196.7604, 141.1536]) cost : 0.424285\n",
            "Epoch : 4400/100000\n",
            "hypothesis : tensor([151.7058, 184.4353, 180.6432, 196.7560, 141.1668]) cost : 0.416955\n",
            "Epoch : 4500/100000\n",
            "hypothesis : tensor([151.6933, 184.4441, 180.6396, 196.7516, 141.1798]) cost : 0.409965\n",
            "Epoch : 4600/100000\n",
            "hypothesis : tensor([151.6811, 184.4527, 180.6361, 196.7474, 141.1925]) cost : 0.403322\n",
            "Epoch : 4700/100000\n",
            "hypothesis : tensor([151.6692, 184.4609, 180.6327, 196.7432, 141.2048]) cost : 0.396999\n",
            "Epoch : 4800/100000\n",
            "hypothesis : tensor([151.6577, 184.4690, 180.6294, 196.7391, 141.2168]) cost : 0.390981\n",
            "Epoch : 4900/100000\n",
            "hypothesis : tensor([151.6466, 184.4768, 180.6262, 196.7351, 141.2285]) cost : 0.385245\n",
            "Epoch : 5000/100000\n",
            "hypothesis : tensor([151.6357, 184.4844, 180.6230, 196.7311, 141.2399]) cost : 0.379784\n",
            "Epoch : 5100/100000\n",
            "hypothesis : tensor([151.6252, 184.4918, 180.6200, 196.7272, 141.2511]) cost : 0.374588\n",
            "Epoch : 5200/100000\n",
            "hypothesis : tensor([151.6149, 184.4990, 180.6171, 196.7234, 141.2620]) cost : 0.369627\n",
            "Epoch : 5300/100000\n",
            "hypothesis : tensor([151.6050, 184.5060, 180.6143, 196.7197, 141.2726]) cost : 0.364902\n",
            "Epoch : 5400/100000\n",
            "hypothesis : tensor([151.5953, 184.5128, 180.6116, 196.7160, 141.2829]) cost : 0.360404\n",
            "Epoch : 5500/100000\n",
            "hypothesis : tensor([151.5859, 184.5194, 180.6089, 196.7124, 141.2930]) cost : 0.356106\n",
            "Epoch : 5600/100000\n",
            "hypothesis : tensor([151.5768, 184.5258, 180.6063, 196.7089, 141.3029]) cost : 0.352008\n",
            "Epoch : 5700/100000\n",
            "hypothesis : tensor([151.5679, 184.5321, 180.6038, 196.7054, 141.3125]) cost : 0.348083\n",
            "Epoch : 5800/100000\n",
            "hypothesis : tensor([151.5593, 184.5381, 180.6014, 196.7020, 141.3218]) cost : 0.344357\n",
            "Epoch : 5900/100000\n",
            "hypothesis : tensor([151.5510, 184.5440, 180.5990, 196.6986, 141.3310]) cost : 0.340795\n",
            "Epoch : 6000/100000\n",
            "hypothesis : tensor([151.5429, 184.5498, 180.5968, 196.6953, 141.3399]) cost : 0.337389\n",
            "Epoch : 6100/100000\n",
            "hypothesis : tensor([151.5350, 184.5553, 180.5946, 196.6920, 141.3486]) cost : 0.334141\n",
            "Epoch : 6200/100000\n",
            "hypothesis : tensor([151.5274, 184.5607, 180.5924, 196.6889, 141.3571]) cost : 0.331029\n",
            "Epoch : 6300/100000\n",
            "hypothesis : tensor([151.5199, 184.5660, 180.5903, 196.6857, 141.3654]) cost : 0.328056\n",
            "Epoch : 6400/100000\n",
            "hypothesis : tensor([151.5127, 184.5711, 180.5884, 196.6826, 141.3735]) cost : 0.325211\n",
            "Epoch : 6500/100000\n",
            "hypothesis : tensor([151.5057, 184.5761, 180.5864, 196.6796, 141.3814]) cost : 0.322487\n",
            "Epoch : 6600/100000\n",
            "hypothesis : tensor([151.4989, 184.5809, 180.5845, 196.6766, 141.3891]) cost : 0.319876\n",
            "Epoch : 6700/100000\n",
            "hypothesis : tensor([151.4924, 184.5856, 180.5827, 196.6737, 141.3966]) cost : 0.317383\n",
            "Epoch : 6800/100000\n",
            "hypothesis : tensor([151.4860, 184.5901, 180.5810, 196.6708, 141.4040]) cost : 0.314985\n",
            "Epoch : 6900/100000\n",
            "hypothesis : tensor([151.4797, 184.5945, 180.5793, 196.6679, 141.4111]) cost : 0.312704\n",
            "Epoch : 7000/100000\n",
            "hypothesis : tensor([151.4737, 184.5988, 180.5777, 196.6651, 141.4182]) cost : 0.310501\n",
            "Epoch : 7100/100000\n",
            "hypothesis : tensor([151.4679, 184.6030, 180.5761, 196.6623, 141.4250]) cost : 0.308389\n",
            "Epoch : 7200/100000\n",
            "hypothesis : tensor([151.4622, 184.6070, 180.5745, 196.6596, 141.4317]) cost : 0.306360\n",
            "Epoch : 7300/100000\n",
            "hypothesis : tensor([151.4567, 184.6110, 180.5731, 196.6570, 141.4382]) cost : 0.304415\n",
            "Epoch : 7400/100000\n",
            "hypothesis : tensor([151.4514, 184.6148, 180.5716, 196.6543, 141.4446]) cost : 0.302551\n",
            "Epoch : 7500/100000\n",
            "hypothesis : tensor([151.4462, 184.6185, 180.5702, 196.6517, 141.4508]) cost : 0.300749\n",
            "Epoch : 7600/100000\n",
            "hypothesis : tensor([151.4412, 184.6221, 180.5689, 196.6492, 141.4569]) cost : 0.299024\n",
            "Epoch : 7700/100000\n",
            "hypothesis : tensor([151.4363, 184.6256, 180.5676, 196.6466, 141.4628]) cost : 0.297357\n",
            "Epoch : 7800/100000\n",
            "hypothesis : tensor([151.4316, 184.6290, 180.5664, 196.6441, 141.4686]) cost : 0.295760\n",
            "Epoch : 7900/100000\n",
            "hypothesis : tensor([151.4270, 184.6323, 180.5652, 196.6417, 141.4743]) cost : 0.294211\n",
            "Epoch : 8000/100000\n",
            "hypothesis : tensor([151.4226, 184.6355, 180.5640, 196.6393, 141.4798]) cost : 0.292723\n",
            "Epoch : 8100/100000\n",
            "hypothesis : tensor([151.4183, 184.6386, 180.5629, 196.6369, 141.4852]) cost : 0.291294\n",
            "Epoch : 8200/100000\n",
            "hypothesis : tensor([151.4141, 184.6416, 180.5618, 196.6345, 141.4905]) cost : 0.289905\n",
            "Epoch : 8300/100000\n",
            "hypothesis : tensor([151.4101, 184.6446, 180.5608, 196.6322, 141.4957]) cost : 0.288573\n",
            "Epoch : 8400/100000\n",
            "hypothesis : tensor([151.4062, 184.6474, 180.5598, 196.6299, 141.5007]) cost : 0.287278\n",
            "Epoch : 8500/100000\n",
            "hypothesis : tensor([151.4024, 184.6501, 180.5588, 196.6277, 141.5057]) cost : 0.286025\n",
            "Epoch : 8600/100000\n",
            "hypothesis : tensor([151.3987, 184.6528, 180.5579, 196.6254, 141.5105]) cost : 0.284818\n",
            "Epoch : 8700/100000\n",
            "hypothesis : tensor([151.3952, 184.6554, 180.5570, 196.6232, 141.5152]) cost : 0.283657\n",
            "Epoch : 8800/100000\n",
            "hypothesis : tensor([151.3917, 184.6579, 180.5562, 196.6211, 141.5198]) cost : 0.282519\n",
            "Epoch : 8900/100000\n",
            "hypothesis : tensor([151.3884, 184.6604, 180.5553, 196.6189, 141.5244]) cost : 0.281423\n",
            "Epoch : 9000/100000\n",
            "hypothesis : tensor([151.3852, 184.6628, 180.5545, 196.6168, 141.5288]) cost : 0.280357\n",
            "Epoch : 9100/100000\n",
            "hypothesis : tensor([151.3820, 184.6650, 180.5538, 196.6147, 141.5331]) cost : 0.279326\n",
            "Epoch : 9200/100000\n",
            "hypothesis : tensor([151.3790, 184.6673, 180.5530, 196.6127, 141.5373]) cost : 0.278324\n",
            "Epoch : 9300/100000\n",
            "hypothesis : tensor([151.3761, 184.6694, 180.5523, 196.6106, 141.5414]) cost : 0.277351\n",
            "Epoch : 9400/100000\n",
            "hypothesis : tensor([151.3733, 184.6715, 180.5517, 196.6086, 141.5455]) cost : 0.276403\n",
            "Epoch : 9500/100000\n",
            "hypothesis : tensor([151.3705, 184.6736, 180.5510, 196.6066, 141.5494]) cost : 0.275493\n",
            "Epoch : 9600/100000\n",
            "hypothesis : tensor([151.3679, 184.6755, 180.5504, 196.6047, 141.5533]) cost : 0.274590\n",
            "Epoch : 9700/100000\n",
            "hypothesis : tensor([151.3653, 184.6774, 180.5498, 196.6027, 141.5571]) cost : 0.273719\n",
            "Epoch : 9800/100000\n",
            "hypothesis : tensor([151.3629, 184.6793, 180.5493, 196.6008, 141.5608]) cost : 0.272865\n",
            "Epoch : 9900/100000\n",
            "hypothesis : tensor([151.3605, 184.6811, 180.5487, 196.5989, 141.5644]) cost : 0.272041\n",
            "Epoch : 10000/100000\n",
            "hypothesis : tensor([151.3581, 184.6828, 180.5482, 196.5970, 141.5680]) cost : 0.271235\n",
            "Epoch : 10100/100000\n",
            "hypothesis : tensor([151.3559, 184.6845, 180.5477, 196.5951, 141.5715]) cost : 0.270443\n",
            "Epoch : 10200/100000\n",
            "hypothesis : tensor([151.3538, 184.6861, 180.5472, 196.5933, 141.5749]) cost : 0.269675\n",
            "Epoch : 10300/100000\n",
            "hypothesis : tensor([151.3517, 184.6877, 180.5468, 196.5915, 141.5782]) cost : 0.268922\n",
            "Epoch : 10400/100000\n",
            "hypothesis : tensor([151.3497, 184.6892, 180.5464, 196.5897, 141.5815]) cost : 0.268182\n",
            "Epoch : 10500/100000\n",
            "hypothesis : tensor([151.3478, 184.6907, 180.5460, 196.5879, 141.5847]) cost : 0.267460\n",
            "Epoch : 10600/100000\n",
            "hypothesis : tensor([151.3459, 184.6921, 180.5456, 196.5862, 141.5878]) cost : 0.266764\n",
            "Epoch : 10700/100000\n",
            "hypothesis : tensor([151.3441, 184.6935, 180.5452, 196.5844, 141.5909]) cost : 0.266076\n",
            "Epoch : 10800/100000\n",
            "hypothesis : tensor([151.3423, 184.6949, 180.5449, 196.5827, 141.5939]) cost : 0.265395\n",
            "Epoch : 10900/100000\n",
            "hypothesis : tensor([151.3406, 184.6962, 180.5445, 196.5810, 141.5968]) cost : 0.264731\n",
            "Epoch : 11000/100000\n",
            "hypothesis : tensor([151.3391, 184.6974, 180.5442, 196.5793, 141.5997]) cost : 0.264081\n",
            "Epoch : 11100/100000\n",
            "hypothesis : tensor([151.3375, 184.6986, 180.5439, 196.5776, 141.6026]) cost : 0.263441\n",
            "Epoch : 11200/100000\n",
            "hypothesis : tensor([151.3360, 184.6998, 180.5437, 196.5759, 141.6053]) cost : 0.262811\n",
            "Epoch : 11300/100000\n",
            "hypothesis : tensor([151.3345, 184.7009, 180.5434, 196.5743, 141.6081]) cost : 0.262204\n",
            "Epoch : 11400/100000\n",
            "hypothesis : tensor([151.3332, 184.7020, 180.5432, 196.5727, 141.6107]) cost : 0.261595\n",
            "Epoch : 11500/100000\n",
            "hypothesis : tensor([151.3318, 184.7031, 180.5430, 196.5711, 141.6134]) cost : 0.261004\n",
            "Epoch : 11600/100000\n",
            "hypothesis : tensor([151.3305, 184.7041, 180.5427, 196.5695, 141.6159]) cost : 0.260421\n",
            "Epoch : 11700/100000\n",
            "hypothesis : tensor([151.3293, 184.7051, 180.5425, 196.5679, 141.6184]) cost : 0.259846\n",
            "Epoch : 11800/100000\n",
            "hypothesis : tensor([151.3281, 184.7061, 180.5424, 196.5663, 141.6209]) cost : 0.259280\n",
            "Epoch : 11900/100000\n",
            "hypothesis : tensor([151.3270, 184.7070, 180.5422, 196.5648, 141.6234]) cost : 0.258717\n",
            "Epoch : 12000/100000\n",
            "hypothesis : tensor([151.3259, 184.7079, 180.5421, 196.5632, 141.6257]) cost : 0.258173\n",
            "Epoch : 12100/100000\n",
            "hypothesis : tensor([151.3249, 184.7087, 180.5419, 196.5617, 141.6281]) cost : 0.257626\n",
            "Epoch : 12200/100000\n",
            "hypothesis : tensor([151.3239, 184.7096, 180.5418, 196.5602, 141.6304]) cost : 0.257098\n",
            "Epoch : 12300/100000\n",
            "hypothesis : tensor([151.3229, 184.7104, 180.5417, 196.5587, 141.6326]) cost : 0.256560\n",
            "Epoch : 12400/100000\n",
            "hypothesis : tensor([151.3220, 184.7111, 180.5416, 196.5572, 141.6348]) cost : 0.256042\n",
            "Epoch : 12500/100000\n",
            "hypothesis : tensor([151.3211, 184.7119, 180.5415, 196.5557, 141.6370]) cost : 0.255532\n",
            "Epoch : 12600/100000\n",
            "hypothesis : tensor([151.3203, 184.7126, 180.5414, 196.5542, 141.6391]) cost : 0.255025\n",
            "Epoch : 12700/100000\n",
            "hypothesis : tensor([151.3195, 184.7133, 180.5414, 196.5528, 141.6412]) cost : 0.254520\n",
            "Epoch : 12800/100000\n",
            "hypothesis : tensor([151.3187, 184.7139, 180.5413, 196.5513, 141.6433]) cost : 0.254032\n",
            "Epoch : 12900/100000\n",
            "hypothesis : tensor([151.3180, 184.7146, 180.5412, 196.5499, 141.6453]) cost : 0.253541\n",
            "Epoch : 13000/100000\n",
            "hypothesis : tensor([151.3174, 184.7152, 180.5412, 196.5485, 141.6473]) cost : 0.253056\n",
            "Epoch : 13100/100000\n",
            "hypothesis : tensor([151.3167, 184.7158, 180.5412, 196.5471, 141.6493]) cost : 0.252571\n",
            "Epoch : 13200/100000\n",
            "hypothesis : tensor([151.3161, 184.7164, 180.5412, 196.5457, 141.6512]) cost : 0.252103\n",
            "Epoch : 13300/100000\n",
            "hypothesis : tensor([151.3155, 184.7169, 180.5412, 196.5443, 141.6531]) cost : 0.251639\n",
            "Epoch : 13400/100000\n",
            "hypothesis : tensor([151.3149, 184.7174, 180.5412, 196.5429, 141.6550]) cost : 0.251178\n",
            "Epoch : 13500/100000\n",
            "hypothesis : tensor([151.3144, 184.7179, 180.5412, 196.5415, 141.6568]) cost : 0.250718\n",
            "Epoch : 13600/100000\n",
            "hypothesis : tensor([151.3139, 184.7184, 180.5412, 196.5402, 141.6586]) cost : 0.250266\n",
            "Epoch : 13700/100000\n",
            "hypothesis : tensor([151.3134, 184.7189, 180.5413, 196.5388, 141.6604]) cost : 0.249814\n",
            "Epoch : 13800/100000\n",
            "hypothesis : tensor([151.3130, 184.7193, 180.5413, 196.5375, 141.6621]) cost : 0.249370\n",
            "Epoch : 13900/100000\n",
            "hypothesis : tensor([151.3125, 184.7197, 180.5413, 196.5361, 141.6638]) cost : 0.248933\n",
            "Epoch : 14000/100000\n",
            "hypothesis : tensor([151.3122, 184.7201, 180.5414, 196.5348, 141.6655]) cost : 0.248491\n",
            "Epoch : 14100/100000\n",
            "hypothesis : tensor([151.3118, 184.7205, 180.5414, 196.5335, 141.6671]) cost : 0.248057\n",
            "Epoch : 14200/100000\n",
            "hypothesis : tensor([151.3115, 184.7209, 180.5415, 196.5322, 141.6688]) cost : 0.247633\n",
            "Epoch : 14300/100000\n",
            "hypothesis : tensor([151.3111, 184.7212, 180.5416, 196.5309, 141.6704]) cost : 0.247203\n",
            "Epoch : 14400/100000\n",
            "hypothesis : tensor([151.3109, 184.7216, 180.5417, 196.5296, 141.6720]) cost : 0.246783\n",
            "Epoch : 14500/100000\n",
            "hypothesis : tensor([151.3106, 184.7219, 180.5418, 196.5283, 141.6735]) cost : 0.246362\n",
            "Epoch : 14600/100000\n",
            "hypothesis : tensor([151.3103, 184.7222, 180.5419, 196.5270, 141.6751]) cost : 0.245949\n",
            "Epoch : 14700/100000\n",
            "hypothesis : tensor([151.3101, 184.7225, 180.5420, 196.5258, 141.6766]) cost : 0.245534\n",
            "Epoch : 14800/100000\n",
            "hypothesis : tensor([151.3099, 184.7228, 180.5421, 196.5245, 141.6781]) cost : 0.245126\n",
            "Epoch : 14900/100000\n",
            "hypothesis : tensor([151.3098, 184.7230, 180.5422, 196.5233, 141.6796]) cost : 0.244722\n",
            "Epoch : 15000/100000\n",
            "hypothesis : tensor([151.3096, 184.7233, 180.5423, 196.5220, 141.6810]) cost : 0.244326\n",
            "Epoch : 15100/100000\n",
            "hypothesis : tensor([151.3094, 184.7235, 180.5424, 196.5208, 141.6824]) cost : 0.243916\n",
            "Epoch : 15200/100000\n",
            "hypothesis : tensor([151.3093, 184.7237, 180.5426, 196.5195, 141.6839]) cost : 0.243522\n",
            "Epoch : 15300/100000\n",
            "hypothesis : tensor([151.3092, 184.7239, 180.5427, 196.5183, 141.6852]) cost : 0.243129\n",
            "Epoch : 15400/100000\n",
            "hypothesis : tensor([151.3091, 184.7241, 180.5428, 196.5171, 141.6866]) cost : 0.242736\n",
            "Epoch : 15500/100000\n",
            "hypothesis : tensor([151.3091, 184.7243, 180.5430, 196.5159, 141.6880]) cost : 0.242345\n",
            "Epoch : 15600/100000\n",
            "hypothesis : tensor([151.3090, 184.7245, 180.5431, 196.5147, 141.6893]) cost : 0.241958\n",
            "Epoch : 15700/100000\n",
            "hypothesis : tensor([151.3090, 184.7246, 180.5433, 196.5135, 141.6906]) cost : 0.241578\n",
            "Epoch : 15800/100000\n",
            "hypothesis : tensor([151.3089, 184.7248, 180.5435, 196.5123, 141.6919]) cost : 0.241195\n",
            "Epoch : 15900/100000\n",
            "hypothesis : tensor([151.3089, 184.7249, 180.5436, 196.5111, 141.6932]) cost : 0.240823\n",
            "Epoch : 16000/100000\n",
            "hypothesis : tensor([151.3089, 184.7250, 180.5438, 196.5099, 141.6945]) cost : 0.240445\n",
            "Epoch : 16100/100000\n",
            "hypothesis : tensor([151.3090, 184.7252, 180.5440, 196.5087, 141.6957]) cost : 0.240068\n",
            "Epoch : 16200/100000\n",
            "hypothesis : tensor([151.3090, 184.7253, 180.5441, 196.5076, 141.6970]) cost : 0.239703\n",
            "Epoch : 16300/100000\n",
            "hypothesis : tensor([151.3091, 184.7254, 180.5443, 196.5064, 141.6982]) cost : 0.239329\n",
            "Epoch : 16400/100000\n",
            "hypothesis : tensor([151.3091, 184.7254, 180.5445, 196.5052, 141.6994]) cost : 0.238963\n",
            "Epoch : 16500/100000\n",
            "hypothesis : tensor([151.3092, 184.7255, 180.5447, 196.5041, 141.7006]) cost : 0.238598\n",
            "Epoch : 16600/100000\n",
            "hypothesis : tensor([151.3093, 184.7256, 180.5449, 196.5030, 141.7018]) cost : 0.238233\n",
            "Epoch : 16700/100000\n",
            "hypothesis : tensor([151.3094, 184.7257, 180.5451, 196.5018, 141.7029]) cost : 0.237881\n",
            "Epoch : 16800/100000\n",
            "hypothesis : tensor([151.3095, 184.7257, 180.5453, 196.5007, 141.7041]) cost : 0.237519\n",
            "Epoch : 16900/100000\n",
            "hypothesis : tensor([151.3096, 184.7258, 180.5455, 196.4996, 141.7052]) cost : 0.237166\n",
            "Epoch : 17000/100000\n",
            "hypothesis : tensor([151.3098, 184.7258, 180.5457, 196.4984, 141.7063]) cost : 0.236811\n",
            "Epoch : 17100/100000\n",
            "hypothesis : tensor([151.3099, 184.7258, 180.5459, 196.4973, 141.7075]) cost : 0.236462\n",
            "Epoch : 17200/100000\n",
            "hypothesis : tensor([151.3100, 184.7258, 180.5461, 196.4962, 141.7086]) cost : 0.236112\n",
            "Epoch : 17300/100000\n",
            "hypothesis : tensor([151.3102, 184.7259, 180.5463, 196.4951, 141.7096]) cost : 0.235762\n",
            "Epoch : 17400/100000\n",
            "hypothesis : tensor([151.3104, 184.7259, 180.5465, 196.4940, 141.7107]) cost : 0.235414\n",
            "Epoch : 17500/100000\n",
            "hypothesis : tensor([151.3106, 184.7259, 180.5467, 196.4929, 141.7118]) cost : 0.235078\n",
            "Epoch : 17600/100000\n",
            "hypothesis : tensor([151.3108, 184.7259, 180.5469, 196.4918, 141.7128]) cost : 0.234736\n",
            "Epoch : 17700/100000\n",
            "hypothesis : tensor([151.3110, 184.7259, 180.5471, 196.4907, 141.7139]) cost : 0.234389\n",
            "Epoch : 17800/100000\n",
            "hypothesis : tensor([151.3112, 184.7258, 180.5474, 196.4896, 141.7149]) cost : 0.234052\n",
            "Epoch : 17900/100000\n",
            "hypothesis : tensor([151.3114, 184.7258, 180.5476, 196.4886, 141.7159]) cost : 0.233723\n",
            "Epoch : 18000/100000\n",
            "hypothesis : tensor([151.3116, 184.7258, 180.5478, 196.4875, 141.7169]) cost : 0.233386\n",
            "Epoch : 18100/100000\n",
            "hypothesis : tensor([151.3118, 184.7258, 180.5480, 196.4864, 141.7180]) cost : 0.233055\n",
            "Epoch : 18200/100000\n",
            "hypothesis : tensor([151.3121, 184.7257, 180.5483, 196.4854, 141.7189]) cost : 0.232730\n",
            "Epoch : 18300/100000\n",
            "hypothesis : tensor([151.3123, 184.7256, 180.5485, 196.4843, 141.7199]) cost : 0.232394\n",
            "Epoch : 18400/100000\n",
            "hypothesis : tensor([151.3126, 184.7256, 180.5488, 196.4832, 141.7209]) cost : 0.232068\n",
            "Epoch : 18500/100000\n",
            "hypothesis : tensor([151.3129, 184.7256, 180.5490, 196.4822, 141.7218]) cost : 0.231748\n",
            "Epoch : 18600/100000\n",
            "hypothesis : tensor([151.3131, 184.7255, 180.5492, 196.4811, 141.7228]) cost : 0.231419\n",
            "Epoch : 18700/100000\n",
            "hypothesis : tensor([151.3134, 184.7254, 180.5495, 196.4801, 141.7238]) cost : 0.231103\n",
            "Epoch : 18800/100000\n",
            "hypothesis : tensor([151.3137, 184.7254, 180.5497, 196.4790, 141.7247]) cost : 0.230780\n",
            "Epoch : 18900/100000\n",
            "hypothesis : tensor([151.3140, 184.7253, 180.5499, 196.4780, 141.7256]) cost : 0.230458\n",
            "Epoch : 19000/100000\n",
            "hypothesis : tensor([151.3143, 184.7252, 180.5502, 196.4770, 141.7265]) cost : 0.230141\n",
            "Epoch : 19100/100000\n",
            "hypothesis : tensor([151.3146, 184.7251, 180.5504, 196.4760, 141.7275]) cost : 0.229828\n",
            "Epoch : 19200/100000\n",
            "hypothesis : tensor([151.3149, 184.7250, 180.5507, 196.4749, 141.7284]) cost : 0.229515\n",
            "Epoch : 19300/100000\n",
            "hypothesis : tensor([151.3152, 184.7249, 180.5509, 196.4739, 141.7293]) cost : 0.229204\n",
            "Epoch : 19400/100000\n",
            "hypothesis : tensor([151.3155, 184.7249, 180.5512, 196.4729, 141.7302]) cost : 0.228893\n",
            "Epoch : 19500/100000\n",
            "hypothesis : tensor([151.3158, 184.7248, 180.5514, 196.4719, 141.7310]) cost : 0.228584\n",
            "Epoch : 19600/100000\n",
            "hypothesis : tensor([151.3161, 184.7247, 180.5517, 196.4709, 141.7319]) cost : 0.228282\n",
            "Epoch : 19700/100000\n",
            "hypothesis : tensor([151.3165, 184.7245, 180.5519, 196.4698, 141.7328]) cost : 0.227978\n",
            "Epoch : 19800/100000\n",
            "hypothesis : tensor([151.3168, 184.7244, 180.5522, 196.4688, 141.7336]) cost : 0.227673\n",
            "Epoch : 19900/100000\n",
            "hypothesis : tensor([151.3172, 184.7243, 180.5524, 196.4678, 141.7345]) cost : 0.227368\n",
            "Epoch : 20000/100000\n",
            "hypothesis : tensor([151.3175, 184.7242, 180.5527, 196.4669, 141.7354]) cost : 0.227065\n",
            "Epoch : 20100/100000\n",
            "hypothesis : tensor([151.3179, 184.7241, 180.5529, 196.4659, 141.7362]) cost : 0.226765\n",
            "Epoch : 20200/100000\n",
            "hypothesis : tensor([151.3182, 184.7240, 180.5532, 196.4649, 141.7370]) cost : 0.226474\n",
            "Epoch : 20300/100000\n",
            "hypothesis : tensor([151.3185, 184.7238, 180.5535, 196.4639, 141.7378]) cost : 0.226178\n",
            "Epoch : 20400/100000\n",
            "hypothesis : tensor([151.3189, 184.7237, 180.5537, 196.4629, 141.7387]) cost : 0.225881\n",
            "Epoch : 20500/100000\n",
            "hypothesis : tensor([151.3193, 184.7236, 180.5540, 196.4619, 141.7395]) cost : 0.225582\n",
            "Epoch : 20600/100000\n",
            "hypothesis : tensor([151.3196, 184.7234, 180.5542, 196.4609, 141.7403]) cost : 0.225290\n",
            "Epoch : 20700/100000\n",
            "hypothesis : tensor([151.3200, 184.7233, 180.5545, 196.4600, 141.7411]) cost : 0.225002\n",
            "Epoch : 20800/100000\n",
            "hypothesis : tensor([151.3204, 184.7232, 180.5548, 196.4590, 141.7419]) cost : 0.224715\n",
            "Epoch : 20900/100000\n",
            "hypothesis : tensor([151.3208, 184.7231, 180.5550, 196.4581, 141.7427]) cost : 0.224423\n",
            "Epoch : 21000/100000\n",
            "hypothesis : tensor([151.3211, 184.7229, 180.5553, 196.4571, 141.7435]) cost : 0.224142\n",
            "Epoch : 21100/100000\n",
            "hypothesis : tensor([151.3215, 184.7227, 180.5555, 196.4561, 141.7443]) cost : 0.223856\n",
            "Epoch : 21200/100000\n",
            "hypothesis : tensor([151.3219, 184.7226, 180.5558, 196.4552, 141.7451]) cost : 0.223570\n",
            "Epoch : 21300/100000\n",
            "hypothesis : tensor([151.3223, 184.7224, 180.5560, 196.4542, 141.7458]) cost : 0.223287\n",
            "Epoch : 21400/100000\n",
            "hypothesis : tensor([151.3227, 184.7223, 180.5563, 196.4533, 141.7466]) cost : 0.223012\n",
            "Epoch : 21500/100000\n",
            "hypothesis : tensor([151.3230, 184.7222, 180.5566, 196.4523, 141.7474]) cost : 0.222734\n",
            "Epoch : 21600/100000\n",
            "hypothesis : tensor([151.3235, 184.7220, 180.5569, 196.4514, 141.7482]) cost : 0.222448\n",
            "Epoch : 21700/100000\n",
            "hypothesis : tensor([151.3238, 184.7218, 180.5571, 196.4504, 141.7489]) cost : 0.222182\n",
            "Epoch : 21800/100000\n",
            "hypothesis : tensor([151.3242, 184.7217, 180.5574, 196.4495, 141.7496]) cost : 0.221904\n",
            "Epoch : 21900/100000\n",
            "hypothesis : tensor([151.3246, 184.7215, 180.5576, 196.4486, 141.7504]) cost : 0.221631\n",
            "Epoch : 22000/100000\n",
            "hypothesis : tensor([151.3250, 184.7214, 180.5579, 196.4476, 141.7511]) cost : 0.221361\n",
            "Epoch : 22100/100000\n",
            "hypothesis : tensor([151.3255, 184.7212, 180.5582, 196.4467, 141.7519]) cost : 0.221082\n",
            "Epoch : 22200/100000\n",
            "hypothesis : tensor([151.3259, 184.7210, 180.5585, 196.4458, 141.7526]) cost : 0.220817\n",
            "Epoch : 22300/100000\n",
            "hypothesis : tensor([151.3263, 184.7209, 180.5587, 196.4449, 141.7533]) cost : 0.220551\n",
            "Epoch : 22400/100000\n",
            "hypothesis : tensor([151.3267, 184.7207, 180.5590, 196.4439, 141.7540]) cost : 0.220286\n",
            "Epoch : 22500/100000\n",
            "hypothesis : tensor([151.3271, 184.7205, 180.5592, 196.4430, 141.7548]) cost : 0.220017\n",
            "Epoch : 22600/100000\n",
            "hypothesis : tensor([151.3275, 184.7204, 180.5595, 196.4421, 141.7555]) cost : 0.219750\n",
            "Epoch : 22700/100000\n",
            "hypothesis : tensor([151.3279, 184.7202, 180.5598, 196.4412, 141.7562]) cost : 0.219482\n",
            "Epoch : 22800/100000\n",
            "hypothesis : tensor([151.3283, 184.7200, 180.5600, 196.4403, 141.7569]) cost : 0.219226\n",
            "Epoch : 22900/100000\n",
            "hypothesis : tensor([151.3287, 184.7198, 180.5603, 196.4394, 141.7576]) cost : 0.218965\n",
            "Epoch : 23000/100000\n",
            "hypothesis : tensor([151.3291, 184.7197, 180.5606, 196.4385, 141.7583]) cost : 0.218712\n",
            "Epoch : 23100/100000\n",
            "hypothesis : tensor([151.3296, 184.7195, 180.5609, 196.4376, 141.7590]) cost : 0.218445\n",
            "Epoch : 23200/100000\n",
            "hypothesis : tensor([151.3300, 184.7193, 180.5611, 196.4367, 141.7597]) cost : 0.218189\n",
            "Epoch : 23300/100000\n",
            "hypothesis : tensor([151.3304, 184.7191, 180.5614, 196.4358, 141.7604]) cost : 0.217934\n",
            "Epoch : 23400/100000\n",
            "hypothesis : tensor([151.3308, 184.7189, 180.5616, 196.4349, 141.7611]) cost : 0.217683\n",
            "Epoch : 23500/100000\n",
            "hypothesis : tensor([151.3312, 184.7188, 180.5619, 196.4340, 141.7618]) cost : 0.217434\n",
            "Epoch : 23600/100000\n",
            "hypothesis : tensor([151.3317, 184.7186, 180.5622, 196.4331, 141.7624]) cost : 0.217175\n",
            "Epoch : 23700/100000\n",
            "hypothesis : tensor([151.3321, 184.7184, 180.5624, 196.4322, 141.7631]) cost : 0.216932\n",
            "Epoch : 23800/100000\n",
            "hypothesis : tensor([151.3325, 184.7182, 180.5627, 196.4313, 141.7638]) cost : 0.216677\n",
            "Epoch : 23900/100000\n",
            "hypothesis : tensor([151.3329, 184.7181, 180.5630, 196.4305, 141.7645]) cost : 0.216430\n",
            "Epoch : 24000/100000\n",
            "hypothesis : tensor([151.3334, 184.7179, 180.5632, 196.4296, 141.7652]) cost : 0.216182\n",
            "Epoch : 24100/100000\n",
            "hypothesis : tensor([151.3338, 184.7177, 180.5635, 196.4287, 141.7658]) cost : 0.215941\n",
            "Epoch : 24200/100000\n",
            "hypothesis : tensor([151.3342, 184.7175, 180.5638, 196.4278, 141.7664]) cost : 0.215696\n",
            "Epoch : 24300/100000\n",
            "hypothesis : tensor([151.3347, 184.7173, 180.5640, 196.4269, 141.7671]) cost : 0.215446\n",
            "Epoch : 24400/100000\n",
            "hypothesis : tensor([151.3351, 184.7171, 180.5643, 196.4261, 141.7678]) cost : 0.215209\n",
            "Epoch : 24500/100000\n",
            "hypothesis : tensor([151.3355, 184.7169, 180.5646, 196.4252, 141.7684]) cost : 0.214965\n",
            "Epoch : 24600/100000\n",
            "hypothesis : tensor([151.3360, 184.7168, 180.5648, 196.4244, 141.7691]) cost : 0.214724\n",
            "Epoch : 24700/100000\n",
            "hypothesis : tensor([151.3364, 184.7166, 180.5651, 196.4235, 141.7697]) cost : 0.214489\n",
            "Epoch : 24800/100000\n",
            "hypothesis : tensor([151.3368, 184.7164, 180.5654, 196.4226, 141.7704]) cost : 0.214251\n",
            "Epoch : 24900/100000\n",
            "hypothesis : tensor([151.3372, 184.7162, 180.5656, 196.4218, 141.7710]) cost : 0.214010\n",
            "Epoch : 25000/100000\n",
            "hypothesis : tensor([151.3377, 184.7160, 180.5659, 196.4209, 141.7717]) cost : 0.213777\n",
            "Epoch : 25100/100000\n",
            "hypothesis : tensor([151.3381, 184.7158, 180.5661, 196.4201, 141.7723]) cost : 0.213541\n",
            "Epoch : 25200/100000\n",
            "hypothesis : tensor([151.3385, 184.7156, 180.5664, 196.4192, 141.7729]) cost : 0.213313\n",
            "Epoch : 25300/100000\n",
            "hypothesis : tensor([151.3390, 184.7154, 180.5667, 196.4184, 141.7735]) cost : 0.213084\n",
            "Epoch : 25400/100000\n",
            "hypothesis : tensor([151.3394, 184.7153, 180.5670, 196.4175, 141.7742]) cost : 0.212852\n",
            "Epoch : 25500/100000\n",
            "hypothesis : tensor([151.3398, 184.7151, 180.5672, 196.4167, 141.7748]) cost : 0.212623\n",
            "Epoch : 25600/100000\n",
            "hypothesis : tensor([151.3402, 184.7149, 180.5675, 196.4158, 141.7754]) cost : 0.212389\n",
            "Epoch : 25700/100000\n",
            "hypothesis : tensor([151.3407, 184.7147, 180.5677, 196.4150, 141.7760]) cost : 0.212162\n",
            "Epoch : 25800/100000\n",
            "hypothesis : tensor([151.3411, 184.7145, 180.5680, 196.4142, 141.7767]) cost : 0.211936\n",
            "Epoch : 25900/100000\n",
            "hypothesis : tensor([151.3416, 184.7143, 180.5683, 196.4134, 141.7773]) cost : 0.211713\n",
            "Epoch : 26000/100000\n",
            "hypothesis : tensor([151.3420, 184.7141, 180.5685, 196.4125, 141.7779]) cost : 0.211488\n",
            "Epoch : 26100/100000\n",
            "hypothesis : tensor([151.3424, 184.7139, 180.5688, 196.4117, 141.7785]) cost : 0.211270\n",
            "Epoch : 26200/100000\n",
            "hypothesis : tensor([151.3428, 184.7137, 180.5690, 196.4109, 141.7791]) cost : 0.211048\n",
            "Epoch : 26300/100000\n",
            "hypothesis : tensor([151.3432, 184.7135, 180.5693, 196.4100, 141.7797]) cost : 0.210828\n",
            "Epoch : 26400/100000\n",
            "hypothesis : tensor([151.3437, 184.7133, 180.5695, 196.4092, 141.7803]) cost : 0.210607\n",
            "Epoch : 26500/100000\n",
            "hypothesis : tensor([151.3441, 184.7132, 180.5698, 196.4084, 141.7809]) cost : 0.210383\n",
            "Epoch : 26600/100000\n",
            "hypothesis : tensor([151.3445, 184.7130, 180.5701, 196.4076, 141.7815]) cost : 0.210174\n",
            "Epoch : 26700/100000\n",
            "hypothesis : tensor([151.3450, 184.7128, 180.5703, 196.4068, 141.7821]) cost : 0.209953\n",
            "Epoch : 26800/100000\n",
            "hypothesis : tensor([151.3454, 184.7126, 180.5706, 196.4059, 141.7827]) cost : 0.209736\n",
            "Epoch : 26900/100000\n",
            "hypothesis : tensor([151.3458, 184.7124, 180.5708, 196.4051, 141.7833]) cost : 0.209526\n",
            "Epoch : 27000/100000\n",
            "hypothesis : tensor([151.3463, 184.7122, 180.5711, 196.4043, 141.7840]) cost : 0.209306\n",
            "Epoch : 27100/100000\n",
            "hypothesis : tensor([151.3466, 184.7120, 180.5714, 196.4035, 141.7845]) cost : 0.209100\n",
            "Epoch : 27200/100000\n",
            "hypothesis : tensor([151.3471, 184.7118, 180.5716, 196.4027, 141.7851]) cost : 0.208888\n",
            "Epoch : 27300/100000\n",
            "hypothesis : tensor([151.3475, 184.7116, 180.5719, 196.4019, 141.7857]) cost : 0.208678\n",
            "Epoch : 27400/100000\n",
            "hypothesis : tensor([151.3480, 184.7114, 180.5721, 196.4011, 141.7863]) cost : 0.208464\n",
            "Epoch : 27500/100000\n",
            "hypothesis : tensor([151.3484, 184.7112, 180.5724, 196.4003, 141.7868]) cost : 0.208260\n",
            "Epoch : 27600/100000\n",
            "hypothesis : tensor([151.3488, 184.7110, 180.5726, 196.3995, 141.7874]) cost : 0.208054\n",
            "Epoch : 27700/100000\n",
            "hypothesis : tensor([151.3492, 184.7108, 180.5729, 196.3987, 141.7880]) cost : 0.207844\n",
            "Epoch : 27800/100000\n",
            "hypothesis : tensor([151.3497, 184.7106, 180.5731, 196.3979, 141.7886]) cost : 0.207641\n",
            "Epoch : 27900/100000\n",
            "hypothesis : tensor([151.3501, 184.7104, 180.5734, 196.3971, 141.7892]) cost : 0.207436\n",
            "Epoch : 28000/100000\n",
            "hypothesis : tensor([151.3505, 184.7103, 180.5737, 196.3963, 141.7897]) cost : 0.207237\n",
            "Epoch : 28100/100000\n",
            "hypothesis : tensor([151.3509, 184.7101, 180.5739, 196.3956, 141.7903]) cost : 0.207029\n",
            "Epoch : 28200/100000\n",
            "hypothesis : tensor([151.3513, 184.7099, 180.5741, 196.3948, 141.7909]) cost : 0.206828\n",
            "Epoch : 28300/100000\n",
            "hypothesis : tensor([151.3518, 184.7097, 180.5744, 196.3940, 141.7914]) cost : 0.206630\n",
            "Epoch : 28400/100000\n",
            "hypothesis : tensor([151.3522, 184.7095, 180.5746, 196.3932, 141.7920]) cost : 0.206427\n",
            "Epoch : 28500/100000\n",
            "hypothesis : tensor([151.3526, 184.7093, 180.5749, 196.3924, 141.7925]) cost : 0.206229\n",
            "Epoch : 28600/100000\n",
            "hypothesis : tensor([151.3530, 184.7091, 180.5751, 196.3916, 141.7931]) cost : 0.206034\n",
            "Epoch : 28700/100000\n",
            "hypothesis : tensor([151.3535, 184.7089, 180.5754, 196.3909, 141.7937]) cost : 0.205832\n",
            "Epoch : 28800/100000\n",
            "hypothesis : tensor([151.3539, 184.7087, 180.5757, 196.3901, 141.7943]) cost : 0.205637\n",
            "Epoch : 28900/100000\n",
            "hypothesis : tensor([151.3543, 184.7085, 180.5759, 196.3894, 141.7948]) cost : 0.205446\n",
            "Epoch : 29000/100000\n",
            "hypothesis : tensor([151.3547, 184.7083, 180.5762, 196.3886, 141.7953]) cost : 0.205256\n",
            "Epoch : 29100/100000\n",
            "hypothesis : tensor([151.3551, 184.7082, 180.5764, 196.3878, 141.7959]) cost : 0.205064\n",
            "Epoch : 29200/100000\n",
            "hypothesis : tensor([151.3555, 184.7080, 180.5766, 196.3870, 141.7964]) cost : 0.204872\n",
            "Epoch : 29300/100000\n",
            "hypothesis : tensor([151.3560, 184.7078, 180.5769, 196.3863, 141.7970]) cost : 0.204676\n",
            "Epoch : 29400/100000\n",
            "hypothesis : tensor([151.3564, 184.7076, 180.5771, 196.3855, 141.7975]) cost : 0.204487\n",
            "Epoch : 29500/100000\n",
            "hypothesis : tensor([151.3568, 184.7074, 180.5774, 196.3848, 141.7981]) cost : 0.204299\n",
            "Epoch : 29600/100000\n",
            "hypothesis : tensor([151.3572, 184.7072, 180.5776, 196.3840, 141.7986]) cost : 0.204112\n",
            "Epoch : 29700/100000\n",
            "hypothesis : tensor([151.3576, 184.7070, 180.5779, 196.3833, 141.7992]) cost : 0.203925\n",
            "Epoch : 29800/100000\n",
            "hypothesis : tensor([151.3581, 184.7069, 180.5781, 196.3825, 141.7997]) cost : 0.203730\n",
            "Epoch : 29900/100000\n",
            "hypothesis : tensor([151.3584, 184.7066, 180.5784, 196.3817, 141.8002]) cost : 0.203556\n",
            "Epoch : 30000/100000\n",
            "hypothesis : tensor([151.3589, 184.7065, 180.5786, 196.3810, 141.8008]) cost : 0.203364\n",
            "Epoch : 30100/100000\n",
            "hypothesis : tensor([151.3593, 184.7063, 180.5788, 196.3802, 141.8013]) cost : 0.203183\n",
            "Epoch : 30200/100000\n",
            "hypothesis : tensor([151.3597, 184.7061, 180.5791, 196.3795, 141.8019]) cost : 0.202999\n",
            "Epoch : 30300/100000\n",
            "hypothesis : tensor([151.3601, 184.7059, 180.5793, 196.3788, 141.8024]) cost : 0.202820\n",
            "Epoch : 30400/100000\n",
            "hypothesis : tensor([151.3605, 184.7057, 180.5796, 196.3780, 141.8029]) cost : 0.202633\n",
            "Epoch : 30500/100000\n",
            "hypothesis : tensor([151.3609, 184.7055, 180.5798, 196.3773, 141.8034]) cost : 0.202458\n",
            "Epoch : 30600/100000\n",
            "hypothesis : tensor([151.3613, 184.7053, 180.5800, 196.3765, 141.8040]) cost : 0.202277\n",
            "Epoch : 30700/100000\n",
            "hypothesis : tensor([151.3617, 184.7051, 180.5803, 196.3758, 141.8045]) cost : 0.202103\n",
            "Epoch : 30800/100000\n",
            "hypothesis : tensor([151.3622, 184.7050, 180.5805, 196.3751, 141.8050]) cost : 0.201917\n",
            "Epoch : 30900/100000\n",
            "hypothesis : tensor([151.3626, 184.7048, 180.5808, 196.3743, 141.8056]) cost : 0.201743\n",
            "Epoch : 31000/100000\n",
            "hypothesis : tensor([151.3630, 184.7046, 180.5810, 196.3736, 141.8061]) cost : 0.201565\n",
            "Epoch : 31100/100000\n",
            "hypothesis : tensor([151.3634, 184.7044, 180.5813, 196.3729, 141.8066]) cost : 0.201393\n",
            "Epoch : 31200/100000\n",
            "hypothesis : tensor([151.3638, 184.7042, 180.5815, 196.3722, 141.8071]) cost : 0.201221\n",
            "Epoch : 31300/100000\n",
            "hypothesis : tensor([151.3642, 184.7040, 180.5817, 196.3714, 141.8076]) cost : 0.201041\n",
            "Epoch : 31400/100000\n",
            "hypothesis : tensor([151.3646, 184.7038, 180.5820, 196.3707, 141.8081]) cost : 0.200876\n",
            "Epoch : 31500/100000\n",
            "hypothesis : tensor([151.3650, 184.7036, 180.5822, 196.3700, 141.8086]) cost : 0.200699\n",
            "Epoch : 31600/100000\n",
            "hypothesis : tensor([151.3654, 184.7034, 180.5824, 196.3693, 141.8092]) cost : 0.200532\n",
            "Epoch : 31700/100000\n",
            "hypothesis : tensor([151.3658, 184.7033, 180.5827, 196.3686, 141.8097]) cost : 0.200361\n",
            "Epoch : 31800/100000\n",
            "hypothesis : tensor([151.3662, 184.7031, 180.5829, 196.3678, 141.8102]) cost : 0.200192\n",
            "Epoch : 31900/100000\n",
            "hypothesis : tensor([151.3666, 184.7029, 180.5831, 196.3671, 141.8107]) cost : 0.200022\n",
            "Epoch : 32000/100000\n",
            "hypothesis : tensor([151.3670, 184.7027, 180.5834, 196.3664, 141.8112]) cost : 0.199855\n",
            "Epoch : 32100/100000\n",
            "hypothesis : tensor([151.3674, 184.7025, 180.5836, 196.3657, 141.8117]) cost : 0.199689\n",
            "Epoch : 32200/100000\n",
            "hypothesis : tensor([151.3678, 184.7023, 180.5838, 196.3650, 141.8122]) cost : 0.199524\n",
            "Epoch : 32300/100000\n",
            "hypothesis : tensor([151.3682, 184.7022, 180.5841, 196.3643, 141.8127]) cost : 0.199361\n",
            "Epoch : 32400/100000\n",
            "hypothesis : tensor([151.3686, 184.7020, 180.5843, 196.3636, 141.8132]) cost : 0.199192\n",
            "Epoch : 32500/100000\n",
            "hypothesis : tensor([151.3690, 184.7018, 180.5845, 196.3629, 141.8137]) cost : 0.199030\n",
            "Epoch : 32600/100000\n",
            "hypothesis : tensor([151.3694, 184.7016, 180.5847, 196.3622, 141.8142]) cost : 0.198867\n",
            "Epoch : 32700/100000\n",
            "hypothesis : tensor([151.3698, 184.7014, 180.5850, 196.3615, 141.8147]) cost : 0.198707\n",
            "Epoch : 32800/100000\n",
            "hypothesis : tensor([151.3701, 184.7013, 180.5852, 196.3608, 141.8152]) cost : 0.198546\n",
            "Epoch : 32900/100000\n",
            "hypothesis : tensor([151.3705, 184.7011, 180.5854, 196.3601, 141.8157]) cost : 0.198389\n",
            "Epoch : 33000/100000\n",
            "hypothesis : tensor([151.3709, 184.7009, 180.5857, 196.3594, 141.8162]) cost : 0.198224\n",
            "Epoch : 33100/100000\n",
            "hypothesis : tensor([151.3713, 184.7007, 180.5859, 196.3587, 141.8167]) cost : 0.198064\n",
            "Epoch : 33200/100000\n",
            "hypothesis : tensor([151.3717, 184.7006, 180.5861, 196.3580, 141.8172]) cost : 0.197903\n",
            "Epoch : 33300/100000\n",
            "hypothesis : tensor([151.3721, 184.7004, 180.5863, 196.3573, 141.8177]) cost : 0.197749\n",
            "Epoch : 33400/100000\n",
            "hypothesis : tensor([151.3725, 184.7002, 180.5866, 196.3566, 141.8182]) cost : 0.197594\n",
            "Epoch : 33500/100000\n",
            "hypothesis : tensor([151.3729, 184.7000, 180.5868, 196.3559, 141.8187]) cost : 0.197439\n",
            "Epoch : 33600/100000\n",
            "hypothesis : tensor([151.3732, 184.6998, 180.5870, 196.3553, 141.8191]) cost : 0.197289\n",
            "Epoch : 33700/100000\n",
            "hypothesis : tensor([151.3736, 184.6996, 180.5872, 196.3546, 141.8196]) cost : 0.197132\n",
            "Epoch : 33800/100000\n",
            "hypothesis : tensor([151.3740, 184.6995, 180.5875, 196.3539, 141.8201]) cost : 0.196975\n",
            "Epoch : 33900/100000\n",
            "hypothesis : tensor([151.3744, 184.6993, 180.5877, 196.3532, 141.8206]) cost : 0.196820\n",
            "Epoch : 34000/100000\n",
            "hypothesis : tensor([151.3748, 184.6991, 180.5879, 196.3525, 141.8211]) cost : 0.196665\n",
            "Epoch : 34100/100000\n",
            "hypothesis : tensor([151.3752, 184.6990, 180.5881, 196.3519, 141.8216]) cost : 0.196515\n",
            "Epoch : 34200/100000\n",
            "hypothesis : tensor([151.3755, 184.6988, 180.5883, 196.3512, 141.8221]) cost : 0.196366\n",
            "Epoch : 34300/100000\n",
            "hypothesis : tensor([151.3759, 184.6986, 180.5886, 196.3505, 141.8225]) cost : 0.196222\n",
            "Epoch : 34400/100000\n",
            "hypothesis : tensor([151.3763, 184.6984, 180.5888, 196.3498, 141.8230]) cost : 0.196070\n",
            "Epoch : 34500/100000\n",
            "hypothesis : tensor([151.3767, 184.6982, 180.5890, 196.3491, 141.8235]) cost : 0.195917\n",
            "Epoch : 34600/100000\n",
            "hypothesis : tensor([151.3771, 184.6980, 180.5892, 196.3485, 141.8239]) cost : 0.195773\n",
            "Epoch : 34700/100000\n",
            "hypothesis : tensor([151.3775, 184.6979, 180.5894, 196.3478, 141.8244]) cost : 0.195621\n",
            "Epoch : 34800/100000\n",
            "hypothesis : tensor([151.3778, 184.6977, 180.5897, 196.3472, 141.8249]) cost : 0.195478\n",
            "Epoch : 34900/100000\n",
            "hypothesis : tensor([151.3782, 184.6975, 180.5899, 196.3465, 141.8253]) cost : 0.195335\n",
            "Epoch : 35000/100000\n",
            "hypothesis : tensor([151.3786, 184.6974, 180.5901, 196.3458, 141.8258]) cost : 0.195189\n",
            "Epoch : 35100/100000\n",
            "hypothesis : tensor([151.3789, 184.6972, 180.5903, 196.3452, 141.8263]) cost : 0.195042\n",
            "Epoch : 35200/100000\n",
            "hypothesis : tensor([151.3793, 184.6970, 180.5905, 196.3445, 141.8268]) cost : 0.194897\n",
            "Epoch : 35300/100000\n",
            "hypothesis : tensor([151.3797, 184.6969, 180.5907, 196.3439, 141.8272]) cost : 0.194757\n",
            "Epoch : 35400/100000\n",
            "hypothesis : tensor([151.3800, 184.6967, 180.5910, 196.3432, 141.8277]) cost : 0.194617\n",
            "Epoch : 35500/100000\n",
            "hypothesis : tensor([151.3804, 184.6965, 180.5912, 196.3425, 141.8282]) cost : 0.194473\n",
            "Epoch : 35600/100000\n",
            "hypothesis : tensor([151.3808, 184.6964, 180.5914, 196.3419, 141.8286]) cost : 0.194336\n",
            "Epoch : 35700/100000\n",
            "hypothesis : tensor([151.3811, 184.6962, 180.5916, 196.3412, 141.8291]) cost : 0.194187\n",
            "Epoch : 35800/100000\n",
            "hypothesis : tensor([151.3815, 184.6960, 180.5918, 196.3406, 141.8295]) cost : 0.194055\n",
            "Epoch : 35900/100000\n",
            "hypothesis : tensor([151.3819, 184.6958, 180.5920, 196.3400, 141.8300]) cost : 0.193914\n",
            "Epoch : 36000/100000\n",
            "hypothesis : tensor([151.3822, 184.6956, 180.5923, 196.3393, 141.8304]) cost : 0.193778\n",
            "Epoch : 36100/100000\n",
            "hypothesis : tensor([151.3826, 184.6955, 180.5925, 196.3387, 141.8309]) cost : 0.193638\n",
            "Epoch : 36200/100000\n",
            "hypothesis : tensor([151.3830, 184.6953, 180.5927, 196.3380, 141.8313]) cost : 0.193502\n",
            "Epoch : 36300/100000\n",
            "hypothesis : tensor([151.3833, 184.6951, 180.5929, 196.3374, 141.8318]) cost : 0.193366\n",
            "Epoch : 36400/100000\n",
            "hypothesis : tensor([151.3837, 184.6950, 180.5931, 196.3367, 141.8323]) cost : 0.193232\n",
            "Epoch : 36500/100000\n",
            "hypothesis : tensor([151.3841, 184.6948, 180.5933, 196.3361, 141.8327]) cost : 0.193096\n",
            "Epoch : 36600/100000\n",
            "hypothesis : tensor([151.3844, 184.6947, 180.5935, 196.3355, 141.8332]) cost : 0.192956\n",
            "Epoch : 36700/100000\n",
            "hypothesis : tensor([151.3848, 184.6945, 180.5937, 196.3348, 141.8336]) cost : 0.192823\n",
            "Epoch : 36800/100000\n",
            "hypothesis : tensor([151.3851, 184.6943, 180.5939, 196.3342, 141.8341]) cost : 0.192695\n",
            "Epoch : 36900/100000\n",
            "hypothesis : tensor([151.3855, 184.6941, 180.5941, 196.3336, 141.8345]) cost : 0.192561\n",
            "Epoch : 37000/100000\n",
            "hypothesis : tensor([151.3859, 184.6940, 180.5943, 196.3329, 141.8349]) cost : 0.192431\n",
            "Epoch : 37100/100000\n",
            "hypothesis : tensor([151.3862, 184.6938, 180.5946, 196.3323, 141.8354]) cost : 0.192300\n",
            "Epoch : 37200/100000\n",
            "hypothesis : tensor([151.3866, 184.6936, 180.5948, 196.3317, 141.8358]) cost : 0.192170\n",
            "Epoch : 37300/100000\n",
            "hypothesis : tensor([151.3869, 184.6935, 180.5950, 196.3311, 141.8363]) cost : 0.192037\n",
            "Epoch : 37400/100000\n",
            "hypothesis : tensor([151.3873, 184.6933, 180.5952, 196.3305, 141.8367]) cost : 0.191912\n",
            "Epoch : 37500/100000\n",
            "hypothesis : tensor([151.3877, 184.6932, 180.5954, 196.3298, 141.8372]) cost : 0.191777\n",
            "Epoch : 37600/100000\n",
            "hypothesis : tensor([151.3880, 184.6930, 180.5956, 196.3292, 141.8376]) cost : 0.191657\n",
            "Epoch : 37700/100000\n",
            "hypothesis : tensor([151.3884, 184.6928, 180.5958, 196.3286, 141.8380]) cost : 0.191528\n",
            "Epoch : 37800/100000\n",
            "hypothesis : tensor([151.3887, 184.6927, 180.5960, 196.3280, 141.8385]) cost : 0.191401\n",
            "Epoch : 37900/100000\n",
            "hypothesis : tensor([151.3891, 184.6925, 180.5962, 196.3273, 141.8389]) cost : 0.191275\n",
            "Epoch : 38000/100000\n",
            "hypothesis : tensor([151.3894, 184.6923, 180.5964, 196.3267, 141.8394]) cost : 0.191144\n",
            "Epoch : 38100/100000\n",
            "hypothesis : tensor([151.3897, 184.6922, 180.5966, 196.3261, 141.8398]) cost : 0.191025\n",
            "Epoch : 38200/100000\n",
            "hypothesis : tensor([151.3901, 184.6920, 180.5968, 196.3255, 141.8402]) cost : 0.190901\n",
            "Epoch : 38300/100000\n",
            "hypothesis : tensor([151.3904, 184.6918, 180.5970, 196.3249, 141.8406]) cost : 0.190779\n",
            "Epoch : 38400/100000\n",
            "hypothesis : tensor([151.3908, 184.6917, 180.5972, 196.3243, 141.8411]) cost : 0.190650\n",
            "Epoch : 38500/100000\n",
            "hypothesis : tensor([151.3911, 184.6915, 180.5974, 196.3237, 141.8415]) cost : 0.190531\n",
            "Epoch : 38600/100000\n",
            "hypothesis : tensor([151.3915, 184.6914, 180.5976, 196.3231, 141.8419]) cost : 0.190409\n",
            "Epoch : 38700/100000\n",
            "hypothesis : tensor([151.3918, 184.6912, 180.5978, 196.3225, 141.8423]) cost : 0.190287\n",
            "Epoch : 38800/100000\n",
            "hypothesis : tensor([151.3922, 184.6911, 180.5980, 196.3219, 141.8428]) cost : 0.190172\n",
            "Epoch : 38900/100000\n",
            "hypothesis : tensor([151.3925, 184.6909, 180.5982, 196.3213, 141.8432]) cost : 0.190049\n",
            "Epoch : 39000/100000\n",
            "hypothesis : tensor([151.3929, 184.6907, 180.5984, 196.3207, 141.8436]) cost : 0.189931\n",
            "Epoch : 39100/100000\n",
            "hypothesis : tensor([151.3932, 184.6906, 180.5986, 196.3201, 141.8441]) cost : 0.189806\n",
            "Epoch : 39200/100000\n",
            "hypothesis : tensor([151.3935, 184.6904, 180.5988, 196.3195, 141.8445]) cost : 0.189689\n",
            "Epoch : 39300/100000\n",
            "hypothesis : tensor([151.3939, 184.6903, 180.5990, 196.3189, 141.8449]) cost : 0.189573\n",
            "Epoch : 39400/100000\n",
            "hypothesis : tensor([151.3942, 184.6901, 180.5992, 196.3183, 141.8453]) cost : 0.189451\n",
            "Epoch : 39500/100000\n",
            "hypothesis : tensor([151.3945, 184.6899, 180.5994, 196.3177, 141.8457]) cost : 0.189343\n",
            "Epoch : 39600/100000\n",
            "hypothesis : tensor([151.3949, 184.6898, 180.5996, 196.3171, 141.8461]) cost : 0.189220\n",
            "Epoch : 39700/100000\n",
            "hypothesis : tensor([151.3952, 184.6896, 180.5998, 196.3165, 141.8465]) cost : 0.189108\n",
            "Epoch : 39800/100000\n",
            "hypothesis : tensor([151.3956, 184.6895, 180.6000, 196.3159, 141.8470]) cost : 0.188989\n",
            "Epoch : 39900/100000\n",
            "hypothesis : tensor([151.3959, 184.6893, 180.6002, 196.3153, 141.8474]) cost : 0.188873\n",
            "Epoch : 40000/100000\n",
            "hypothesis : tensor([151.3962, 184.6891, 180.6004, 196.3148, 141.8478]) cost : 0.188769\n",
            "Epoch : 40100/100000\n",
            "hypothesis : tensor([151.3966, 184.6890, 180.6006, 196.3142, 141.8482]) cost : 0.188648\n",
            "Epoch : 40200/100000\n",
            "hypothesis : tensor([151.3969, 184.6889, 180.6007, 196.3136, 141.8486]) cost : 0.188533\n",
            "Epoch : 40300/100000\n",
            "hypothesis : tensor([151.3972, 184.6887, 180.6009, 196.3130, 141.8490]) cost : 0.188424\n",
            "Epoch : 40400/100000\n",
            "hypothesis : tensor([151.3976, 184.6886, 180.6011, 196.3124, 141.8495]) cost : 0.188306\n",
            "Epoch : 40500/100000\n",
            "hypothesis : tensor([151.3979, 184.6884, 180.6013, 196.3119, 141.8499]) cost : 0.188201\n",
            "Epoch : 40600/100000\n",
            "hypothesis : tensor([151.3982, 184.6882, 180.6015, 196.3112, 141.8503]) cost : 0.188086\n",
            "Epoch : 40700/100000\n",
            "hypothesis : tensor([151.3985, 184.6881, 180.6017, 196.3107, 141.8507]) cost : 0.187981\n",
            "Epoch : 40800/100000\n",
            "hypothesis : tensor([151.3989, 184.6879, 180.6019, 196.3101, 141.8511]) cost : 0.187869\n",
            "Epoch : 40900/100000\n",
            "hypothesis : tensor([151.3992, 184.6878, 180.6021, 196.3095, 141.8515]) cost : 0.187759\n",
            "Epoch : 41000/100000\n",
            "hypothesis : tensor([151.3995, 184.6876, 180.6023, 196.3090, 141.8519]) cost : 0.187655\n",
            "Epoch : 41100/100000\n",
            "hypothesis : tensor([151.3999, 184.6875, 180.6025, 196.3084, 141.8523]) cost : 0.187547\n",
            "Epoch : 41200/100000\n",
            "hypothesis : tensor([151.4002, 184.6873, 180.6026, 196.3078, 141.8527]) cost : 0.187435\n",
            "Epoch : 41300/100000\n",
            "hypothesis : tensor([151.4005, 184.6872, 180.6028, 196.3073, 141.8531]) cost : 0.187333\n",
            "Epoch : 41400/100000\n",
            "hypothesis : tensor([151.4008, 184.6870, 180.6030, 196.3067, 141.8535]) cost : 0.187226\n",
            "Epoch : 41500/100000\n",
            "hypothesis : tensor([151.4012, 184.6869, 180.6032, 196.3061, 141.8539]) cost : 0.187117\n",
            "Epoch : 41600/100000\n",
            "hypothesis : tensor([151.4015, 184.6867, 180.6034, 196.3056, 141.8543]) cost : 0.187009\n",
            "Epoch : 41700/100000\n",
            "hypothesis : tensor([151.4018, 184.6866, 180.6036, 196.3050, 141.8547]) cost : 0.186909\n",
            "Epoch : 41800/100000\n",
            "hypothesis : tensor([151.4021, 184.6864, 180.6038, 196.3044, 141.8551]) cost : 0.186801\n",
            "Epoch : 41900/100000\n",
            "hypothesis : tensor([151.4024, 184.6863, 180.6039, 196.3039, 141.8555]) cost : 0.186697\n",
            "Epoch : 42000/100000\n",
            "hypothesis : tensor([151.4028, 184.6861, 180.6041, 196.3033, 141.8559]) cost : 0.186594\n",
            "Epoch : 42100/100000\n",
            "hypothesis : tensor([151.4031, 184.6860, 180.6043, 196.3028, 141.8562]) cost : 0.186494\n",
            "Epoch : 42200/100000\n",
            "hypothesis : tensor([151.4034, 184.6858, 180.6045, 196.3022, 141.8566]) cost : 0.186391\n",
            "Epoch : 42300/100000\n",
            "hypothesis : tensor([151.4037, 184.6857, 180.6047, 196.3017, 141.8570]) cost : 0.186289\n",
            "Epoch : 42400/100000\n",
            "hypothesis : tensor([151.4040, 184.6855, 180.6049, 196.3011, 141.8574]) cost : 0.186186\n",
            "Epoch : 42500/100000\n",
            "hypothesis : tensor([151.4044, 184.6854, 180.6051, 196.3006, 141.8578]) cost : 0.186085\n",
            "Epoch : 42600/100000\n",
            "hypothesis : tensor([151.4046, 184.6853, 180.6052, 196.3000, 141.8582]) cost : 0.185984\n",
            "Epoch : 42700/100000\n",
            "hypothesis : tensor([151.4050, 184.6851, 180.6054, 196.2995, 141.8586]) cost : 0.185886\n",
            "Epoch : 42800/100000\n",
            "hypothesis : tensor([151.4053, 184.6850, 180.6056, 196.2989, 141.8589]) cost : 0.185786\n",
            "Epoch : 42900/100000\n",
            "hypothesis : tensor([151.4056, 184.6848, 180.6058, 196.2984, 141.8593]) cost : 0.185688\n",
            "Epoch : 43000/100000\n",
            "hypothesis : tensor([151.4059, 184.6847, 180.6060, 196.2978, 141.8597]) cost : 0.185590\n",
            "Epoch : 43100/100000\n",
            "hypothesis : tensor([151.4062, 184.6845, 180.6061, 196.2973, 141.8601]) cost : 0.185491\n",
            "Epoch : 43200/100000\n",
            "hypothesis : tensor([151.4065, 184.6844, 180.6063, 196.2968, 141.8605]) cost : 0.185392\n",
            "Epoch : 43300/100000\n",
            "hypothesis : tensor([151.4068, 184.6842, 180.6065, 196.2962, 141.8609]) cost : 0.185294\n",
            "Epoch : 43400/100000\n",
            "hypothesis : tensor([151.4071, 184.6841, 180.6067, 196.2957, 141.8613]) cost : 0.185200\n",
            "Epoch : 43500/100000\n",
            "hypothesis : tensor([151.4074, 184.6840, 180.6068, 196.2951, 141.8616]) cost : 0.185101\n",
            "Epoch : 43600/100000\n",
            "hypothesis : tensor([151.4077, 184.6838, 180.6070, 196.2946, 141.8620]) cost : 0.185003\n",
            "Epoch : 43700/100000\n",
            "hypothesis : tensor([151.4081, 184.6837, 180.6072, 196.2941, 141.8624]) cost : 0.184911\n",
            "Epoch : 43800/100000\n",
            "hypothesis : tensor([151.4084, 184.6835, 180.6074, 196.2935, 141.8627]) cost : 0.184818\n",
            "Epoch : 43900/100000\n",
            "hypothesis : tensor([151.4087, 184.6834, 180.6075, 196.2930, 141.8631]) cost : 0.184723\n",
            "Epoch : 44000/100000\n",
            "hypothesis : tensor([151.4090, 184.6832, 180.6077, 196.2925, 141.8635]) cost : 0.184627\n",
            "Epoch : 44100/100000\n",
            "hypothesis : tensor([151.4093, 184.6831, 180.6079, 196.2919, 141.8639]) cost : 0.184533\n",
            "Epoch : 44200/100000\n",
            "hypothesis : tensor([151.4096, 184.6830, 180.6081, 196.2914, 141.8643]) cost : 0.184439\n",
            "Epoch : 44300/100000\n",
            "hypothesis : tensor([151.4099, 184.6828, 180.6082, 196.2909, 141.8646]) cost : 0.184349\n",
            "Epoch : 44400/100000\n",
            "hypothesis : tensor([151.4102, 184.6827, 180.6084, 196.2903, 141.8650]) cost : 0.184257\n",
            "Epoch : 44500/100000\n",
            "hypothesis : tensor([151.4105, 184.6826, 180.6086, 196.2898, 141.8654]) cost : 0.184166\n",
            "Epoch : 44600/100000\n",
            "hypothesis : tensor([151.4108, 184.6824, 180.6087, 196.2893, 141.8657]) cost : 0.184069\n",
            "Epoch : 44700/100000\n",
            "hypothesis : tensor([151.4111, 184.6823, 180.6089, 196.2888, 141.8661]) cost : 0.183984\n",
            "Epoch : 44800/100000\n",
            "hypothesis : tensor([151.4114, 184.6821, 180.6091, 196.2883, 141.8665]) cost : 0.183891\n",
            "Epoch : 44900/100000\n",
            "hypothesis : tensor([151.4117, 184.6820, 180.6093, 196.2877, 141.8669]) cost : 0.183799\n",
            "Epoch : 45000/100000\n",
            "hypothesis : tensor([151.4120, 184.6819, 180.6094, 196.2872, 141.8672]) cost : 0.183710\n",
            "Epoch : 45100/100000\n",
            "hypothesis : tensor([151.4122, 184.6817, 180.6096, 196.2867, 141.8676]) cost : 0.183627\n",
            "Epoch : 45200/100000\n",
            "hypothesis : tensor([151.4125, 184.6816, 180.6098, 196.2862, 141.8679]) cost : 0.183539\n",
            "Epoch : 45300/100000\n",
            "hypothesis : tensor([151.4128, 184.6814, 180.6099, 196.2857, 141.8683]) cost : 0.183445\n",
            "Epoch : 45400/100000\n",
            "hypothesis : tensor([151.4131, 184.6813, 180.6101, 196.2852, 141.8687]) cost : 0.183359\n",
            "Epoch : 45500/100000\n",
            "hypothesis : tensor([151.4134, 184.6812, 180.6103, 196.2847, 141.8690]) cost : 0.183272\n",
            "Epoch : 45600/100000\n",
            "hypothesis : tensor([151.4137, 184.6810, 180.6105, 196.2842, 141.8694]) cost : 0.183185\n",
            "Epoch : 45700/100000\n",
            "hypothesis : tensor([151.4140, 184.6809, 180.6106, 196.2836, 141.8697]) cost : 0.183096\n",
            "Epoch : 45800/100000\n",
            "hypothesis : tensor([151.4143, 184.6808, 180.6108, 196.2831, 141.8701]) cost : 0.183009\n",
            "Epoch : 45900/100000\n",
            "hypothesis : tensor([151.4146, 184.6806, 180.6110, 196.2826, 141.8705]) cost : 0.182929\n",
            "Epoch : 46000/100000\n",
            "hypothesis : tensor([151.4149, 184.6805, 180.6111, 196.2821, 141.8708]) cost : 0.182841\n",
            "Epoch : 46100/100000\n",
            "hypothesis : tensor([151.4152, 184.6803, 180.6113, 196.2816, 141.8712]) cost : 0.182760\n",
            "Epoch : 46200/100000\n",
            "hypothesis : tensor([151.4155, 184.6802, 180.6115, 196.2811, 141.8715]) cost : 0.182672\n",
            "Epoch : 46300/100000\n",
            "hypothesis : tensor([151.4158, 184.6801, 180.6116, 196.2806, 141.8719]) cost : 0.182587\n",
            "Epoch : 46400/100000\n",
            "hypothesis : tensor([151.4160, 184.6800, 180.6118, 196.2801, 141.8722]) cost : 0.182502\n",
            "Epoch : 46500/100000\n",
            "hypothesis : tensor([151.4163, 184.6798, 180.6120, 196.2796, 141.8726]) cost : 0.182420\n",
            "Epoch : 46600/100000\n",
            "hypothesis : tensor([151.4166, 184.6797, 180.6121, 196.2791, 141.8729]) cost : 0.182339\n",
            "Epoch : 46700/100000\n",
            "hypothesis : tensor([151.4169, 184.6796, 180.6123, 196.2786, 141.8733]) cost : 0.182256\n",
            "Epoch : 46800/100000\n",
            "hypothesis : tensor([151.4172, 184.6794, 180.6125, 196.2781, 141.8736]) cost : 0.182173\n",
            "Epoch : 46900/100000\n",
            "hypothesis : tensor([151.4174, 184.6793, 180.6126, 196.2776, 141.8740]) cost : 0.182094\n",
            "Epoch : 47000/100000\n",
            "hypothesis : tensor([151.4178, 184.6792, 180.6128, 196.2771, 141.8743]) cost : 0.182012\n",
            "Epoch : 47100/100000\n",
            "hypothesis : tensor([151.4180, 184.6790, 180.6129, 196.2766, 141.8747]) cost : 0.181931\n",
            "Epoch : 47200/100000\n",
            "hypothesis : tensor([151.4183, 184.6789, 180.6131, 196.2762, 141.8750]) cost : 0.181848\n",
            "Epoch : 47300/100000\n",
            "hypothesis : tensor([151.4186, 184.6788, 180.6133, 196.2757, 141.8754]) cost : 0.181771\n",
            "Epoch : 47400/100000\n",
            "hypothesis : tensor([151.4189, 184.6786, 180.6134, 196.2752, 141.8757]) cost : 0.181692\n",
            "Epoch : 47500/100000\n",
            "hypothesis : tensor([151.4192, 184.6785, 180.6136, 196.2747, 141.8761]) cost : 0.181605\n",
            "Epoch : 47600/100000\n",
            "hypothesis : tensor([151.4194, 184.6784, 180.6138, 196.2742, 141.8764]) cost : 0.181535\n",
            "Epoch : 47700/100000\n",
            "hypothesis : tensor([151.4197, 184.6782, 180.6139, 196.2737, 141.8768]) cost : 0.181452\n",
            "Epoch : 47800/100000\n",
            "hypothesis : tensor([151.4200, 184.6781, 180.6141, 196.2732, 141.8771]) cost : 0.181370\n",
            "Epoch : 47900/100000\n",
            "hypothesis : tensor([151.4203, 184.6780, 180.6142, 196.2727, 141.8774]) cost : 0.181297\n",
            "Epoch : 48000/100000\n",
            "hypothesis : tensor([151.4205, 184.6779, 180.6144, 196.2723, 141.8778]) cost : 0.181219\n",
            "Epoch : 48100/100000\n",
            "hypothesis : tensor([151.4208, 184.6777, 180.6145, 196.2718, 141.8781]) cost : 0.181141\n",
            "Epoch : 48200/100000\n",
            "hypothesis : tensor([151.4211, 184.6776, 180.6147, 196.2713, 141.8785]) cost : 0.181059\n",
            "Epoch : 48300/100000\n",
            "hypothesis : tensor([151.4214, 184.6775, 180.6149, 196.2708, 141.8788]) cost : 0.180989\n",
            "Epoch : 48400/100000\n",
            "hypothesis : tensor([151.4216, 184.6774, 180.6150, 196.2703, 141.8791]) cost : 0.180909\n",
            "Epoch : 48500/100000\n",
            "hypothesis : tensor([151.4219, 184.6772, 180.6152, 196.2699, 141.8795]) cost : 0.180838\n",
            "Epoch : 48600/100000\n",
            "hypothesis : tensor([151.4222, 184.6771, 180.6153, 196.2694, 141.8798]) cost : 0.180759\n",
            "Epoch : 48700/100000\n",
            "hypothesis : tensor([151.4224, 184.6770, 180.6155, 196.2689, 141.8801]) cost : 0.180686\n",
            "Epoch : 48800/100000\n",
            "hypothesis : tensor([151.4227, 184.6768, 180.6156, 196.2684, 141.8804]) cost : 0.180609\n",
            "Epoch : 48900/100000\n",
            "hypothesis : tensor([151.4230, 184.6767, 180.6158, 196.2680, 141.8808]) cost : 0.180537\n",
            "Epoch : 49000/100000\n",
            "hypothesis : tensor([151.4233, 184.6766, 180.6160, 196.2675, 141.8811]) cost : 0.180468\n",
            "Epoch : 49100/100000\n",
            "hypothesis : tensor([151.4235, 184.6765, 180.6161, 196.2670, 141.8814]) cost : 0.180389\n",
            "Epoch : 49200/100000\n",
            "hypothesis : tensor([151.4238, 184.6763, 180.6163, 196.2666, 141.8818]) cost : 0.180314\n",
            "Epoch : 49300/100000\n",
            "hypothesis : tensor([151.4241, 184.6762, 180.6164, 196.2661, 141.8821]) cost : 0.180238\n",
            "Epoch : 49400/100000\n",
            "hypothesis : tensor([151.4243, 184.6761, 180.6166, 196.2656, 141.8824]) cost : 0.180169\n",
            "Epoch : 49500/100000\n",
            "hypothesis : tensor([151.4246, 184.6760, 180.6167, 196.2652, 141.8828]) cost : 0.180098\n",
            "Epoch : 49600/100000\n",
            "hypothesis : tensor([151.4249, 184.6759, 180.6169, 196.2647, 141.8831]) cost : 0.180027\n",
            "Epoch : 49700/100000\n",
            "hypothesis : tensor([151.4251, 184.6757, 180.6170, 196.2642, 141.8834]) cost : 0.179959\n",
            "Epoch : 49800/100000\n",
            "hypothesis : tensor([151.4254, 184.6756, 180.6172, 196.2638, 141.8837]) cost : 0.179882\n",
            "Epoch : 49900/100000\n",
            "hypothesis : tensor([151.4256, 184.6755, 180.6173, 196.2633, 141.8841]) cost : 0.179812\n",
            "Epoch : 50000/100000\n",
            "hypothesis : tensor([151.4259, 184.6753, 180.6175, 196.2628, 141.8844]) cost : 0.179742\n",
            "Epoch : 50100/100000\n",
            "hypothesis : tensor([151.4262, 184.6752, 180.6176, 196.2624, 141.8847]) cost : 0.179672\n",
            "Epoch : 50200/100000\n",
            "hypothesis : tensor([151.4265, 184.6751, 180.6178, 196.2619, 141.8850]) cost : 0.179594\n",
            "Epoch : 50300/100000\n",
            "hypothesis : tensor([151.4267, 184.6750, 180.6180, 196.2615, 141.8854]) cost : 0.179538\n",
            "Epoch : 50400/100000\n",
            "hypothesis : tensor([151.4270, 184.6749, 180.6181, 196.2610, 141.8857]) cost : 0.179465\n",
            "Epoch : 50500/100000\n",
            "hypothesis : tensor([151.4272, 184.6747, 180.6182, 196.2606, 141.8860]) cost : 0.179402\n",
            "Epoch : 50600/100000\n",
            "hypothesis : tensor([151.4275, 184.6746, 180.6184, 196.2601, 141.8863]) cost : 0.179329\n",
            "Epoch : 50700/100000\n",
            "hypothesis : tensor([151.4277, 184.6745, 180.6185, 196.2597, 141.8866]) cost : 0.179262\n",
            "Epoch : 50800/100000\n",
            "hypothesis : tensor([151.4280, 184.6744, 180.6187, 196.2592, 141.8869]) cost : 0.179194\n",
            "Epoch : 50900/100000\n",
            "hypothesis : tensor([151.4283, 184.6743, 180.6188, 196.2588, 141.8873]) cost : 0.179124\n",
            "Epoch : 51000/100000\n",
            "hypothesis : tensor([151.4285, 184.6742, 180.6190, 196.2583, 141.8876]) cost : 0.179057\n",
            "Epoch : 51100/100000\n",
            "hypothesis : tensor([151.4288, 184.6740, 180.6191, 196.2579, 141.8879]) cost : 0.178989\n",
            "Epoch : 51200/100000\n",
            "hypothesis : tensor([151.4290, 184.6739, 180.6193, 196.2574, 141.8882]) cost : 0.178929\n",
            "Epoch : 51300/100000\n",
            "hypothesis : tensor([151.4293, 184.6738, 180.6194, 196.2570, 141.8885]) cost : 0.178857\n",
            "Epoch : 51400/100000\n",
            "hypothesis : tensor([151.4295, 184.6737, 180.6196, 196.2565, 141.8889]) cost : 0.178790\n",
            "Epoch : 51500/100000\n",
            "hypothesis : tensor([151.4298, 184.6736, 180.6197, 196.2561, 141.8892]) cost : 0.178726\n",
            "Epoch : 51600/100000\n",
            "hypothesis : tensor([151.4300, 184.6734, 180.6199, 196.2557, 141.8895]) cost : 0.178662\n",
            "Epoch : 51700/100000\n",
            "hypothesis : tensor([151.4303, 184.6733, 180.6200, 196.2552, 141.8898]) cost : 0.178594\n",
            "Epoch : 51800/100000\n",
            "hypothesis : tensor([151.4305, 184.6732, 180.6201, 196.2548, 141.8901]) cost : 0.178528\n",
            "Epoch : 51900/100000\n",
            "hypothesis : tensor([151.4308, 184.6731, 180.6203, 196.2543, 141.8904]) cost : 0.178465\n",
            "Epoch : 52000/100000\n",
            "hypothesis : tensor([151.4310, 184.6730, 180.6204, 196.2539, 141.8907]) cost : 0.178409\n",
            "Epoch : 52100/100000\n",
            "hypothesis : tensor([151.4313, 184.6729, 180.6206, 196.2535, 141.8910]) cost : 0.178343\n",
            "Epoch : 52200/100000\n",
            "hypothesis : tensor([151.4315, 184.6727, 180.6207, 196.2531, 141.8913]) cost : 0.178276\n",
            "Epoch : 52300/100000\n",
            "hypothesis : tensor([151.4318, 184.6726, 180.6208, 196.2526, 141.8916]) cost : 0.178215\n",
            "Epoch : 52400/100000\n",
            "hypothesis : tensor([151.4320, 184.6725, 180.6210, 196.2522, 141.8919]) cost : 0.178152\n",
            "Epoch : 52500/100000\n",
            "hypothesis : tensor([151.4323, 184.6724, 180.6211, 196.2518, 141.8922]) cost : 0.178092\n",
            "Epoch : 52600/100000\n",
            "hypothesis : tensor([151.4325, 184.6723, 180.6213, 196.2513, 141.8925]) cost : 0.178027\n",
            "Epoch : 52700/100000\n",
            "hypothesis : tensor([151.4328, 184.6721, 180.6214, 196.2509, 141.8928]) cost : 0.177966\n",
            "Epoch : 52800/100000\n",
            "hypothesis : tensor([151.4330, 184.6720, 180.6216, 196.2505, 141.8931]) cost : 0.177904\n",
            "Epoch : 52900/100000\n",
            "hypothesis : tensor([151.4333, 184.6719, 180.6217, 196.2500, 141.8934]) cost : 0.177840\n",
            "Epoch : 53000/100000\n",
            "hypothesis : tensor([151.4335, 184.6718, 180.6219, 196.2496, 141.8938]) cost : 0.177783\n",
            "Epoch : 53100/100000\n",
            "hypothesis : tensor([151.4338, 184.6717, 180.6220, 196.2492, 141.8940]) cost : 0.177721\n",
            "Epoch : 53200/100000\n",
            "hypothesis : tensor([151.4340, 184.6716, 180.6221, 196.2488, 141.8943]) cost : 0.177660\n",
            "Epoch : 53300/100000\n",
            "hypothesis : tensor([151.4342, 184.6715, 180.6223, 196.2483, 141.8947]) cost : 0.177595\n",
            "Epoch : 53400/100000\n",
            "hypothesis : tensor([151.4345, 184.6714, 180.6224, 196.2479, 141.8950]) cost : 0.177540\n",
            "Epoch : 53500/100000\n",
            "hypothesis : tensor([151.4347, 184.6713, 180.6226, 196.2475, 141.8953]) cost : 0.177475\n",
            "Epoch : 53600/100000\n",
            "hypothesis : tensor([151.4349, 184.6712, 180.6227, 196.2471, 141.8956]) cost : 0.177417\n",
            "Epoch : 53700/100000\n",
            "hypothesis : tensor([151.4352, 184.6711, 180.6228, 196.2466, 141.8959]) cost : 0.177364\n",
            "Epoch : 53800/100000\n",
            "hypothesis : tensor([151.4354, 184.6710, 180.6230, 196.2462, 141.8962]) cost : 0.177302\n",
            "Epoch : 53900/100000\n",
            "hypothesis : tensor([151.4356, 184.6709, 180.6231, 196.2458, 141.8965]) cost : 0.177240\n",
            "Epoch : 54000/100000\n",
            "hypothesis : tensor([151.4359, 184.6707, 180.6232, 196.2454, 141.8968]) cost : 0.177182\n",
            "Epoch : 54100/100000\n",
            "hypothesis : tensor([151.4361, 184.6706, 180.6234, 196.2449, 141.8971]) cost : 0.177125\n",
            "Epoch : 54200/100000\n",
            "hypothesis : tensor([151.4363, 184.6705, 180.6235, 196.2445, 141.8974]) cost : 0.177069\n",
            "Epoch : 54300/100000\n",
            "hypothesis : tensor([151.4366, 184.6704, 180.6236, 196.2441, 141.8977]) cost : 0.177014\n",
            "Epoch : 54400/100000\n",
            "hypothesis : tensor([151.4368, 184.6703, 180.6238, 196.2437, 141.8980]) cost : 0.176954\n",
            "Epoch : 54500/100000\n",
            "hypothesis : tensor([151.4370, 184.6702, 180.6239, 196.2433, 141.8983]) cost : 0.176897\n",
            "Epoch : 54600/100000\n",
            "hypothesis : tensor([151.4373, 184.6701, 180.6240, 196.2429, 141.8985]) cost : 0.176842\n",
            "Epoch : 54700/100000\n",
            "hypothesis : tensor([151.4375, 184.6700, 180.6242, 196.2425, 141.8988]) cost : 0.176788\n",
            "Epoch : 54800/100000\n",
            "hypothesis : tensor([151.4378, 184.6698, 180.6243, 196.2421, 141.8991]) cost : 0.176728\n",
            "Epoch : 54900/100000\n",
            "hypothesis : tensor([151.4380, 184.6697, 180.6244, 196.2417, 141.8994]) cost : 0.176675\n",
            "Epoch : 55000/100000\n",
            "hypothesis : tensor([151.4382, 184.6696, 180.6246, 196.2413, 141.8997]) cost : 0.176616\n",
            "Epoch : 55100/100000\n",
            "hypothesis : tensor([151.4385, 184.6695, 180.6247, 196.2408, 141.8999]) cost : 0.176560\n",
            "Epoch : 55200/100000\n",
            "hypothesis : tensor([151.4387, 184.6694, 180.6248, 196.2405, 141.9002]) cost : 0.176507\n",
            "Epoch : 55300/100000\n",
            "hypothesis : tensor([151.4390, 184.6693, 180.6250, 196.2401, 141.9005]) cost : 0.176455\n",
            "Epoch : 55400/100000\n",
            "hypothesis : tensor([151.4392, 184.6691, 180.6251, 196.2397, 141.9008]) cost : 0.176404\n",
            "Epoch : 55500/100000\n",
            "hypothesis : tensor([151.4394, 184.6690, 180.6252, 196.2393, 141.9010]) cost : 0.176349\n",
            "Epoch : 55600/100000\n",
            "hypothesis : tensor([151.4397, 184.6689, 180.6254, 196.2389, 141.9013]) cost : 0.176294\n",
            "Epoch : 55700/100000\n",
            "hypothesis : tensor([151.4399, 184.6688, 180.6255, 196.2385, 141.9016]) cost : 0.176239\n",
            "Epoch : 55800/100000\n",
            "hypothesis : tensor([151.4402, 184.6687, 180.6256, 196.2381, 141.9018]) cost : 0.176186\n",
            "Epoch : 55900/100000\n",
            "hypothesis : tensor([151.4404, 184.6686, 180.6258, 196.2377, 141.9021]) cost : 0.176131\n",
            "Epoch : 56000/100000\n",
            "hypothesis : tensor([151.4406, 184.6685, 180.6259, 196.2373, 141.9024]) cost : 0.176082\n",
            "Epoch : 56100/100000\n",
            "hypothesis : tensor([151.4409, 184.6684, 180.6260, 196.2369, 141.9026]) cost : 0.176028\n",
            "Epoch : 56200/100000\n",
            "hypothesis : tensor([151.4411, 184.6683, 180.6262, 196.2365, 141.9029]) cost : 0.175978\n",
            "Epoch : 56300/100000\n",
            "hypothesis : tensor([151.4413, 184.6681, 180.6263, 196.2361, 141.9032]) cost : 0.175931\n",
            "Epoch : 56400/100000\n",
            "hypothesis : tensor([151.4415, 184.6680, 180.6264, 196.2357, 141.9035]) cost : 0.175876\n",
            "Epoch : 56500/100000\n",
            "hypothesis : tensor([151.4418, 184.6680, 180.6266, 196.2353, 141.9038]) cost : 0.175822\n",
            "Epoch : 56600/100000\n",
            "hypothesis : tensor([151.4420, 184.6678, 180.6267, 196.2350, 141.9041]) cost : 0.175774\n",
            "Epoch : 56700/100000\n",
            "hypothesis : tensor([151.4422, 184.6677, 180.6268, 196.2346, 141.9043]) cost : 0.175720\n",
            "Epoch : 56800/100000\n",
            "hypothesis : tensor([151.4424, 184.6676, 180.6270, 196.2342, 141.9046]) cost : 0.175671\n",
            "Epoch : 56900/100000\n",
            "hypothesis : tensor([151.4427, 184.6675, 180.6271, 196.2338, 141.9049]) cost : 0.175618\n",
            "Epoch : 57000/100000\n",
            "hypothesis : tensor([151.4429, 184.6674, 180.6272, 196.2334, 141.9052]) cost : 0.175569\n",
            "Epoch : 57100/100000\n",
            "hypothesis : tensor([151.4431, 184.6674, 180.6273, 196.2330, 141.9055]) cost : 0.175518\n",
            "Epoch : 57200/100000\n",
            "hypothesis : tensor([151.4433, 184.6673, 180.6275, 196.2326, 141.9057]) cost : 0.175464\n",
            "Epoch : 57300/100000\n",
            "hypothesis : tensor([151.4435, 184.6672, 180.6276, 196.2322, 141.9060]) cost : 0.175412\n",
            "Epoch : 57400/100000\n",
            "hypothesis : tensor([151.4437, 184.6671, 180.6277, 196.2318, 141.9063]) cost : 0.175368\n",
            "Epoch : 57500/100000\n",
            "hypothesis : tensor([151.4439, 184.6670, 180.6278, 196.2314, 141.9066]) cost : 0.175313\n",
            "Epoch : 57600/100000\n",
            "hypothesis : tensor([151.4441, 184.6669, 180.6279, 196.2311, 141.9069]) cost : 0.175265\n",
            "Epoch : 57700/100000\n",
            "hypothesis : tensor([151.4444, 184.6668, 180.6281, 196.2307, 141.9072]) cost : 0.175218\n",
            "Epoch : 57800/100000\n",
            "hypothesis : tensor([151.4446, 184.6667, 180.6282, 196.2303, 141.9074]) cost : 0.175164\n",
            "Epoch : 57900/100000\n",
            "hypothesis : tensor([151.4448, 184.6666, 180.6283, 196.2299, 141.9077]) cost : 0.175123\n",
            "Epoch : 58000/100000\n",
            "hypothesis : tensor([151.4450, 184.6665, 180.6284, 196.2295, 141.9080]) cost : 0.175076\n",
            "Epoch : 58100/100000\n",
            "hypothesis : tensor([151.4452, 184.6664, 180.6286, 196.2291, 141.9082]) cost : 0.175025\n",
            "Epoch : 58200/100000\n",
            "hypothesis : tensor([151.4454, 184.6663, 180.6287, 196.2288, 141.9085]) cost : 0.174979\n",
            "Epoch : 58300/100000\n",
            "hypothesis : tensor([151.4456, 184.6662, 180.6288, 196.2284, 141.9088]) cost : 0.174928\n",
            "Epoch : 58400/100000\n",
            "hypothesis : tensor([151.4458, 184.6661, 180.6289, 196.2280, 141.9090]) cost : 0.174881\n",
            "Epoch : 58500/100000\n",
            "hypothesis : tensor([151.4461, 184.6660, 180.6291, 196.2276, 141.9093]) cost : 0.174836\n",
            "Epoch : 58600/100000\n",
            "hypothesis : tensor([151.4463, 184.6659, 180.6292, 196.2273, 141.9096]) cost : 0.174789\n",
            "Epoch : 58700/100000\n",
            "hypothesis : tensor([151.4465, 184.6658, 180.6293, 196.2269, 141.9098]) cost : 0.174744\n",
            "Epoch : 58800/100000\n",
            "hypothesis : tensor([151.4467, 184.6657, 180.6294, 196.2265, 141.9101]) cost : 0.174695\n",
            "Epoch : 58900/100000\n",
            "hypothesis : tensor([151.4469, 184.6656, 180.6295, 196.2262, 141.9103]) cost : 0.174650\n",
            "Epoch : 59000/100000\n",
            "hypothesis : tensor([151.4471, 184.6655, 180.6297, 196.2258, 141.9106]) cost : 0.174603\n",
            "Epoch : 59100/100000\n",
            "hypothesis : tensor([151.4474, 184.6654, 180.6298, 196.2254, 141.9108]) cost : 0.174565\n",
            "Epoch : 59200/100000\n",
            "hypothesis : tensor([151.4476, 184.6653, 180.6299, 196.2251, 141.9111]) cost : 0.174519\n",
            "Epoch : 59300/100000\n",
            "hypothesis : tensor([151.4478, 184.6651, 180.6300, 196.2247, 141.9113]) cost : 0.174471\n",
            "Epoch : 59400/100000\n",
            "hypothesis : tensor([151.4480, 184.6651, 180.6302, 196.2243, 141.9116]) cost : 0.174426\n",
            "Epoch : 59500/100000\n",
            "hypothesis : tensor([151.4482, 184.6649, 180.6303, 196.2240, 141.9118]) cost : 0.174383\n",
            "Epoch : 59600/100000\n",
            "hypothesis : tensor([151.4484, 184.6648, 180.6304, 196.2236, 141.9121]) cost : 0.174336\n",
            "Epoch : 59700/100000\n",
            "hypothesis : tensor([151.4487, 184.6647, 180.6305, 196.2233, 141.9123]) cost : 0.174293\n",
            "Epoch : 59800/100000\n",
            "hypothesis : tensor([151.4489, 184.6646, 180.6306, 196.2229, 141.9126]) cost : 0.174250\n",
            "Epoch : 59900/100000\n",
            "hypothesis : tensor([151.4491, 184.6645, 180.6308, 196.2226, 141.9128]) cost : 0.174207\n",
            "Epoch : 60000/100000\n",
            "hypothesis : tensor([151.4493, 184.6644, 180.6309, 196.2222, 141.9131]) cost : 0.174168\n",
            "Epoch : 60100/100000\n",
            "hypothesis : tensor([151.4495, 184.6643, 180.6310, 196.2219, 141.9133]) cost : 0.174122\n",
            "Epoch : 60200/100000\n",
            "hypothesis : tensor([151.4497, 184.6642, 180.6311, 196.2215, 141.9135]) cost : 0.174077\n",
            "Epoch : 60300/100000\n",
            "hypothesis : tensor([151.4499, 184.6641, 180.6312, 196.2211, 141.9138]) cost : 0.174035\n",
            "Epoch : 60400/100000\n",
            "hypothesis : tensor([151.4501, 184.6640, 180.6313, 196.2208, 141.9140]) cost : 0.173990\n",
            "Epoch : 60500/100000\n",
            "hypothesis : tensor([151.4503, 184.6639, 180.6315, 196.2204, 141.9143]) cost : 0.173950\n",
            "Epoch : 60600/100000\n",
            "hypothesis : tensor([151.4505, 184.6638, 180.6316, 196.2201, 141.9145]) cost : 0.173913\n",
            "Epoch : 60700/100000\n",
            "hypothesis : tensor([151.4507, 184.6638, 180.6317, 196.2198, 141.9148]) cost : 0.173863\n",
            "Epoch : 60800/100000\n",
            "hypothesis : tensor([151.4510, 184.6637, 180.6318, 196.2194, 141.9150]) cost : 0.173822\n",
            "Epoch : 60900/100000\n",
            "hypothesis : tensor([151.4511, 184.6636, 180.6319, 196.2190, 141.9153]) cost : 0.173783\n",
            "Epoch : 61000/100000\n",
            "hypothesis : tensor([151.4514, 184.6635, 180.6320, 196.2187, 141.9155]) cost : 0.173742\n",
            "Epoch : 61100/100000\n",
            "hypothesis : tensor([151.4516, 184.6634, 180.6322, 196.2183, 141.9158]) cost : 0.173697\n",
            "Epoch : 61200/100000\n",
            "hypothesis : tensor([151.4518, 184.6633, 180.6323, 196.2180, 141.9160]) cost : 0.173659\n",
            "Epoch : 61300/100000\n",
            "hypothesis : tensor([151.4520, 184.6632, 180.6324, 196.2176, 141.9163]) cost : 0.173615\n",
            "Epoch : 61400/100000\n",
            "hypothesis : tensor([151.4521, 184.6631, 180.6325, 196.2173, 141.9165]) cost : 0.173578\n",
            "Epoch : 61500/100000\n",
            "hypothesis : tensor([151.4523, 184.6630, 180.6326, 196.2169, 141.9168]) cost : 0.173531\n",
            "Epoch : 61600/100000\n",
            "hypothesis : tensor([151.4526, 184.6630, 180.6328, 196.2166, 141.9171]) cost : 0.173490\n",
            "Epoch : 61700/100000\n",
            "hypothesis : tensor([151.4527, 184.6629, 180.6329, 196.2162, 141.9173]) cost : 0.173455\n",
            "Epoch : 61800/100000\n",
            "hypothesis : tensor([151.4529, 184.6628, 180.6330, 196.2159, 141.9176]) cost : 0.173413\n",
            "Epoch : 61900/100000\n",
            "hypothesis : tensor([151.4531, 184.6627, 180.6331, 196.2155, 141.9178]) cost : 0.173365\n",
            "Epoch : 62000/100000\n",
            "hypothesis : tensor([151.4533, 184.6626, 180.6332, 196.2152, 141.9181]) cost : 0.173324\n",
            "Epoch : 62100/100000\n",
            "hypothesis : tensor([151.4535, 184.6625, 180.6333, 196.2148, 141.9183]) cost : 0.173290\n",
            "Epoch : 62200/100000\n",
            "hypothesis : tensor([151.4537, 184.6624, 180.6334, 196.2145, 141.9186]) cost : 0.173249\n",
            "Epoch : 62300/100000\n",
            "hypothesis : tensor([151.4538, 184.6624, 180.6335, 196.2141, 141.9188]) cost : 0.173214\n",
            "Epoch : 62400/100000\n",
            "hypothesis : tensor([151.4540, 184.6623, 180.6336, 196.2138, 141.9191]) cost : 0.173171\n",
            "Epoch : 62500/100000\n",
            "hypothesis : tensor([151.4542, 184.6622, 180.6337, 196.2135, 141.9194]) cost : 0.173130\n",
            "Epoch : 62600/100000\n",
            "hypothesis : tensor([151.4544, 184.6621, 180.6338, 196.2131, 141.9196]) cost : 0.173092\n",
            "Epoch : 62700/100000\n",
            "hypothesis : tensor([151.4546, 184.6620, 180.6339, 196.2128, 141.9198]) cost : 0.173056\n",
            "Epoch : 62800/100000\n",
            "hypothesis : tensor([151.4548, 184.6619, 180.6341, 196.2124, 141.9201]) cost : 0.173019\n",
            "Epoch : 62900/100000\n",
            "hypothesis : tensor([151.4550, 184.6618, 180.6342, 196.2121, 141.9203]) cost : 0.172975\n",
            "Epoch : 63000/100000\n",
            "hypothesis : tensor([151.4552, 184.6617, 180.6343, 196.2117, 141.9205]) cost : 0.172944\n",
            "Epoch : 63100/100000\n",
            "hypothesis : tensor([151.4554, 184.6616, 180.6344, 196.2114, 141.9208]) cost : 0.172904\n",
            "Epoch : 63200/100000\n",
            "hypothesis : tensor([151.4556, 184.6615, 180.6345, 196.2111, 141.9210]) cost : 0.172865\n",
            "Epoch : 63300/100000\n",
            "hypothesis : tensor([151.4558, 184.6615, 180.6346, 196.2108, 141.9212]) cost : 0.172829\n",
            "Epoch : 63400/100000\n",
            "hypothesis : tensor([151.4560, 184.6614, 180.6347, 196.2104, 141.9214]) cost : 0.172791\n",
            "Epoch : 63500/100000\n",
            "hypothesis : tensor([151.4562, 184.6613, 180.6348, 196.2101, 141.9217]) cost : 0.172756\n",
            "Epoch : 63600/100000\n",
            "hypothesis : tensor([151.4564, 184.6612, 180.6349, 196.2098, 141.9219]) cost : 0.172719\n",
            "Epoch : 63700/100000\n",
            "hypothesis : tensor([151.4565, 184.6611, 180.6350, 196.2094, 141.9221]) cost : 0.172683\n",
            "Epoch : 63800/100000\n",
            "hypothesis : tensor([151.4568, 184.6610, 180.6351, 196.2091, 141.9223]) cost : 0.172646\n",
            "Epoch : 63900/100000\n",
            "hypothesis : tensor([151.4570, 184.6609, 180.6353, 196.2088, 141.9226]) cost : 0.172609\n",
            "Epoch : 64000/100000\n",
            "hypothesis : tensor([151.4571, 184.6608, 180.6354, 196.2085, 141.9228]) cost : 0.172573\n",
            "Epoch : 64100/100000\n",
            "hypothesis : tensor([151.4573, 184.6607, 180.6355, 196.2082, 141.9230]) cost : 0.172540\n",
            "Epoch : 64200/100000\n",
            "hypothesis : tensor([151.4575, 184.6606, 180.6356, 196.2079, 141.9232]) cost : 0.172504\n",
            "Epoch : 64300/100000\n",
            "hypothesis : tensor([151.4577, 184.6605, 180.6357, 196.2075, 141.9234]) cost : 0.172465\n",
            "Epoch : 64400/100000\n",
            "hypothesis : tensor([151.4579, 184.6604, 180.6358, 196.2072, 141.9237]) cost : 0.172437\n",
            "Epoch : 64500/100000\n",
            "hypothesis : tensor([151.4581, 184.6603, 180.6359, 196.2069, 141.9239]) cost : 0.172395\n",
            "Epoch : 64600/100000\n",
            "hypothesis : tensor([151.4583, 184.6602, 180.6360, 196.2066, 141.9241]) cost : 0.172365\n",
            "Epoch : 64700/100000\n",
            "hypothesis : tensor([151.4585, 184.6601, 180.6361, 196.2063, 141.9243]) cost : 0.172329\n",
            "Epoch : 64800/100000\n",
            "hypothesis : tensor([151.4587, 184.6600, 180.6362, 196.2059, 141.9245]) cost : 0.172299\n",
            "Epoch : 64900/100000\n",
            "hypothesis : tensor([151.4589, 184.6600, 180.6363, 196.2056, 141.9247]) cost : 0.172262\n",
            "Epoch : 65000/100000\n",
            "hypothesis : tensor([151.4590, 184.6599, 180.6364, 196.2053, 141.9250]) cost : 0.172231\n",
            "Epoch : 65100/100000\n",
            "hypothesis : tensor([151.4592, 184.6598, 180.6365, 196.2050, 141.9252]) cost : 0.172195\n",
            "Epoch : 65200/100000\n",
            "hypothesis : tensor([151.4594, 184.6597, 180.6366, 196.2047, 141.9254]) cost : 0.172161\n",
            "Epoch : 65300/100000\n",
            "hypothesis : tensor([151.4596, 184.6596, 180.6367, 196.2044, 141.9256]) cost : 0.172123\n",
            "Epoch : 65400/100000\n",
            "hypothesis : tensor([151.4598, 184.6595, 180.6368, 196.2041, 141.9259]) cost : 0.172090\n",
            "Epoch : 65500/100000\n",
            "hypothesis : tensor([151.4600, 184.6595, 180.6369, 196.2038, 141.9261]) cost : 0.172059\n",
            "Epoch : 65600/100000\n",
            "hypothesis : tensor([151.4601, 184.6594, 180.6371, 196.2034, 141.9263]) cost : 0.172028\n",
            "Epoch : 65700/100000\n",
            "hypothesis : tensor([151.4603, 184.6593, 180.6372, 196.2031, 141.9265]) cost : 0.171989\n",
            "Epoch : 65800/100000\n",
            "hypothesis : tensor([151.4605, 184.6592, 180.6373, 196.2028, 141.9267]) cost : 0.171961\n",
            "Epoch : 65900/100000\n",
            "hypothesis : tensor([151.4607, 184.6591, 180.6374, 196.2025, 141.9270]) cost : 0.171930\n",
            "Epoch : 66000/100000\n",
            "hypothesis : tensor([151.4609, 184.6591, 180.6375, 196.2022, 141.9272]) cost : 0.171892\n",
            "Epoch : 66100/100000\n",
            "hypothesis : tensor([151.4610, 184.6590, 180.6376, 196.2019, 141.9274]) cost : 0.171861\n",
            "Epoch : 66200/100000\n",
            "hypothesis : tensor([151.4612, 184.6589, 180.6376, 196.2016, 141.9277]) cost : 0.171825\n",
            "Epoch : 66300/100000\n",
            "hypothesis : tensor([151.4614, 184.6588, 180.6378, 196.2013, 141.9279]) cost : 0.171792\n",
            "Epoch : 66400/100000\n",
            "hypothesis : tensor([151.4615, 184.6588, 180.6379, 196.2009, 141.9281]) cost : 0.171758\n",
            "Epoch : 66500/100000\n",
            "hypothesis : tensor([151.4617, 184.6587, 180.6380, 196.2006, 141.9283]) cost : 0.171728\n",
            "Epoch : 66600/100000\n",
            "hypothesis : tensor([151.4619, 184.6586, 180.6381, 196.2003, 141.9286]) cost : 0.171693\n",
            "Epoch : 66700/100000\n",
            "hypothesis : tensor([151.4621, 184.6585, 180.6382, 196.2000, 141.9288]) cost : 0.171662\n",
            "Epoch : 66800/100000\n",
            "hypothesis : tensor([151.4622, 184.6584, 180.6383, 196.1997, 141.9290]) cost : 0.171635\n",
            "Epoch : 66900/100000\n",
            "hypothesis : tensor([151.4624, 184.6584, 180.6384, 196.1994, 141.9293]) cost : 0.171601\n",
            "Epoch : 67000/100000\n",
            "hypothesis : tensor([151.4625, 184.6583, 180.6385, 196.1991, 141.9295]) cost : 0.171571\n",
            "Epoch : 67100/100000\n",
            "hypothesis : tensor([151.4627, 184.6582, 180.6386, 196.1988, 141.9297]) cost : 0.171536\n",
            "Epoch : 67200/100000\n",
            "hypothesis : tensor([151.4629, 184.6581, 180.6386, 196.1985, 141.9299]) cost : 0.171508\n",
            "Epoch : 67300/100000\n",
            "hypothesis : tensor([151.4630, 184.6581, 180.6387, 196.1981, 141.9302]) cost : 0.171471\n",
            "Epoch : 67400/100000\n",
            "hypothesis : tensor([151.4632, 184.6580, 180.6388, 196.1978, 141.9304]) cost : 0.171439\n",
            "Epoch : 67500/100000\n",
            "hypothesis : tensor([151.4634, 184.6579, 180.6389, 196.1975, 141.9306]) cost : 0.171419\n",
            "Epoch : 67600/100000\n",
            "hypothesis : tensor([151.4635, 184.6578, 180.6390, 196.1972, 141.9308]) cost : 0.171377\n",
            "Epoch : 67700/100000\n",
            "hypothesis : tensor([151.4637, 184.6578, 180.6391, 196.1969, 141.9310]) cost : 0.171354\n",
            "Epoch : 67800/100000\n",
            "hypothesis : tensor([151.4639, 184.6577, 180.6392, 196.1966, 141.9312]) cost : 0.171320\n",
            "Epoch : 67900/100000\n",
            "hypothesis : tensor([151.4640, 184.6576, 180.6393, 196.1963, 141.9314]) cost : 0.171294\n",
            "Epoch : 68000/100000\n",
            "hypothesis : tensor([151.4642, 184.6575, 180.6394, 196.1960, 141.9317]) cost : 0.171260\n",
            "Epoch : 68100/100000\n",
            "hypothesis : tensor([151.4644, 184.6575, 180.6395, 196.1958, 141.9319]) cost : 0.171231\n",
            "Epoch : 68200/100000\n",
            "hypothesis : tensor([151.4646, 184.6574, 180.6396, 196.1955, 141.9321]) cost : 0.171203\n",
            "Epoch : 68300/100000\n",
            "hypothesis : tensor([151.4647, 184.6573, 180.6397, 196.1952, 141.9323]) cost : 0.171174\n",
            "Epoch : 68400/100000\n",
            "hypothesis : tensor([151.4649, 184.6572, 180.6398, 196.1949, 141.9325]) cost : 0.171147\n",
            "Epoch : 68500/100000\n",
            "hypothesis : tensor([151.4651, 184.6571, 180.6399, 196.1946, 141.9327]) cost : 0.171112\n",
            "Epoch : 68600/100000\n",
            "hypothesis : tensor([151.4653, 184.6570, 180.6400, 196.1943, 141.9329]) cost : 0.171087\n",
            "Epoch : 68700/100000\n",
            "hypothesis : tensor([151.4654, 184.6569, 180.6401, 196.1940, 141.9331]) cost : 0.171054\n",
            "Epoch : 68800/100000\n",
            "hypothesis : tensor([151.4656, 184.6569, 180.6402, 196.1937, 141.9333]) cost : 0.171030\n",
            "Epoch : 68900/100000\n",
            "hypothesis : tensor([151.4658, 184.6568, 180.6403, 196.1934, 141.9335]) cost : 0.170999\n",
            "Epoch : 69000/100000\n",
            "hypothesis : tensor([151.4660, 184.6567, 180.6404, 196.1931, 141.9337]) cost : 0.170967\n",
            "Epoch : 69100/100000\n",
            "hypothesis : tensor([151.4661, 184.6566, 180.6405, 196.1929, 141.9339]) cost : 0.170941\n",
            "Epoch : 69200/100000\n",
            "hypothesis : tensor([151.4663, 184.6565, 180.6406, 196.1926, 141.9341]) cost : 0.170920\n",
            "Epoch : 69300/100000\n",
            "hypothesis : tensor([151.4665, 184.6564, 180.6407, 196.1923, 141.9342]) cost : 0.170891\n",
            "Epoch : 69400/100000\n",
            "hypothesis : tensor([151.4666, 184.6563, 180.6407, 196.1920, 141.9344]) cost : 0.170858\n",
            "Epoch : 69500/100000\n",
            "hypothesis : tensor([151.4668, 184.6563, 180.6409, 196.1917, 141.9346]) cost : 0.170834\n",
            "Epoch : 69600/100000\n",
            "hypothesis : tensor([151.4670, 184.6562, 180.6409, 196.1914, 141.9348]) cost : 0.170805\n",
            "Epoch : 69700/100000\n",
            "hypothesis : tensor([151.4671, 184.6561, 180.6411, 196.1912, 141.9350]) cost : 0.170784\n",
            "Epoch : 69800/100000\n",
            "hypothesis : tensor([151.4673, 184.6560, 180.6411, 196.1909, 141.9352]) cost : 0.170756\n",
            "Epoch : 69900/100000\n",
            "hypothesis : tensor([151.4675, 184.6559, 180.6412, 196.1906, 141.9354]) cost : 0.170728\n",
            "Epoch : 70000/100000\n",
            "hypothesis : tensor([151.4677, 184.6559, 180.6413, 196.1904, 141.9356]) cost : 0.170692\n",
            "Epoch : 70100/100000\n",
            "hypothesis : tensor([151.4678, 184.6558, 180.6414, 196.1901, 141.9358]) cost : 0.170673\n",
            "Epoch : 70200/100000\n",
            "hypothesis : tensor([151.4680, 184.6557, 180.6415, 196.1898, 141.9360]) cost : 0.170641\n",
            "Epoch : 70300/100000\n",
            "hypothesis : tensor([151.4681, 184.6556, 180.6416, 196.1895, 141.9362]) cost : 0.170617\n",
            "Epoch : 70400/100000\n",
            "hypothesis : tensor([151.4683, 184.6556, 180.6417, 196.1892, 141.9363]) cost : 0.170594\n",
            "Epoch : 70500/100000\n",
            "hypothesis : tensor([151.4685, 184.6555, 180.6418, 196.1890, 141.9366]) cost : 0.170562\n",
            "Epoch : 70600/100000\n",
            "hypothesis : tensor([151.4686, 184.6554, 180.6419, 196.1887, 141.9368]) cost : 0.170537\n",
            "Epoch : 70700/100000\n",
            "hypothesis : tensor([151.4688, 184.6554, 180.6420, 196.1884, 141.9370]) cost : 0.170514\n",
            "Epoch : 70800/100000\n",
            "hypothesis : tensor([151.4689, 184.6553, 180.6421, 196.1881, 141.9371]) cost : 0.170488\n",
            "Epoch : 70900/100000\n",
            "hypothesis : tensor([151.4691, 184.6552, 180.6422, 196.1879, 141.9373]) cost : 0.170462\n",
            "Epoch : 71000/100000\n",
            "hypothesis : tensor([151.4693, 184.6551, 180.6422, 196.1876, 141.9375]) cost : 0.170435\n",
            "Epoch : 71100/100000\n",
            "hypothesis : tensor([151.4694, 184.6551, 180.6423, 196.1873, 141.9377]) cost : 0.170404\n",
            "Epoch : 71200/100000\n",
            "hypothesis : tensor([151.4696, 184.6550, 180.6424, 196.1870, 141.9379]) cost : 0.170382\n",
            "Epoch : 71300/100000\n",
            "hypothesis : tensor([151.4698, 184.6549, 180.6425, 196.1868, 141.9381]) cost : 0.170358\n",
            "Epoch : 71400/100000\n",
            "hypothesis : tensor([151.4699, 184.6548, 180.6426, 196.1865, 141.9383]) cost : 0.170336\n",
            "Epoch : 71500/100000\n",
            "hypothesis : tensor([151.4701, 184.6548, 180.6427, 196.1862, 141.9385]) cost : 0.170304\n",
            "Epoch : 71600/100000\n",
            "hypothesis : tensor([151.4702, 184.6547, 180.6428, 196.1859, 141.9387]) cost : 0.170281\n",
            "Epoch : 71700/100000\n",
            "hypothesis : tensor([151.4704, 184.6546, 180.6429, 196.1857, 141.9389]) cost : 0.170258\n",
            "Epoch : 71800/100000\n",
            "hypothesis : tensor([151.4705, 184.6546, 180.6429, 196.1854, 141.9391]) cost : 0.170228\n",
            "Epoch : 71900/100000\n",
            "hypothesis : tensor([151.4707, 184.6545, 180.6430, 196.1851, 141.9393]) cost : 0.170205\n",
            "Epoch : 72000/100000\n",
            "hypothesis : tensor([151.4708, 184.6544, 180.6431, 196.1848, 141.9395]) cost : 0.170179\n",
            "Epoch : 72100/100000\n",
            "hypothesis : tensor([151.4709, 184.6544, 180.6432, 196.1846, 141.9397]) cost : 0.170157\n",
            "Epoch : 72200/100000\n",
            "hypothesis : tensor([151.4711, 184.6543, 180.6433, 196.1843, 141.9399]) cost : 0.170127\n",
            "Epoch : 72300/100000\n",
            "hypothesis : tensor([151.4712, 184.6542, 180.6434, 196.1840, 141.9401]) cost : 0.170109\n",
            "Epoch : 72400/100000\n",
            "hypothesis : tensor([151.4714, 184.6542, 180.6435, 196.1838, 141.9403]) cost : 0.170082\n",
            "Epoch : 72500/100000\n",
            "hypothesis : tensor([151.4715, 184.6541, 180.6436, 196.1835, 141.9405]) cost : 0.170058\n",
            "Epoch : 72600/100000\n",
            "hypothesis : tensor([151.4717, 184.6540, 180.6436, 196.1832, 141.9407]) cost : 0.170035\n",
            "Epoch : 72700/100000\n",
            "hypothesis : tensor([151.4718, 184.6540, 180.6437, 196.1830, 141.9409]) cost : 0.170012\n",
            "Epoch : 72800/100000\n",
            "hypothesis : tensor([151.4720, 184.6539, 180.6438, 196.1827, 141.9411]) cost : 0.169980\n",
            "Epoch : 72900/100000\n",
            "hypothesis : tensor([151.4721, 184.6539, 180.6439, 196.1824, 141.9413]) cost : 0.169962\n",
            "Epoch : 73000/100000\n",
            "hypothesis : tensor([151.4722, 184.6538, 180.6440, 196.1821, 141.9415]) cost : 0.169934\n",
            "Epoch : 73100/100000\n",
            "hypothesis : tensor([151.4724, 184.6537, 180.6440, 196.1819, 141.9417]) cost : 0.169912\n",
            "Epoch : 73200/100000\n",
            "hypothesis : tensor([151.4725, 184.6536, 180.6441, 196.1816, 141.9419]) cost : 0.169891\n",
            "Epoch : 73300/100000\n",
            "hypothesis : tensor([151.4727, 184.6536, 180.6442, 196.1813, 141.9421]) cost : 0.169860\n",
            "Epoch : 73400/100000\n",
            "hypothesis : tensor([151.4728, 184.6535, 180.6443, 196.1811, 141.9423]) cost : 0.169844\n",
            "Epoch : 73500/100000\n",
            "hypothesis : tensor([151.4730, 184.6534, 180.6444, 196.1808, 141.9425]) cost : 0.169819\n",
            "Epoch : 73600/100000\n",
            "hypothesis : tensor([151.4731, 184.6534, 180.6445, 196.1806, 141.9426]) cost : 0.169796\n",
            "Epoch : 73700/100000\n",
            "hypothesis : tensor([151.4733, 184.6533, 180.6446, 196.1803, 141.9428]) cost : 0.169774\n",
            "Epoch : 73800/100000\n",
            "hypothesis : tensor([151.4734, 184.6532, 180.6446, 196.1801, 141.9430]) cost : 0.169747\n",
            "Epoch : 73900/100000\n",
            "hypothesis : tensor([151.4736, 184.6532, 180.6447, 196.1798, 141.9432]) cost : 0.169727\n",
            "Epoch : 74000/100000\n",
            "hypothesis : tensor([151.4737, 184.6531, 180.6448, 196.1795, 141.9434]) cost : 0.169705\n",
            "Epoch : 74100/100000\n",
            "hypothesis : tensor([151.4739, 184.6530, 180.6449, 196.1793, 141.9435]) cost : 0.169687\n",
            "Epoch : 74200/100000\n",
            "hypothesis : tensor([151.4740, 184.6530, 180.6450, 196.1790, 141.9437]) cost : 0.169659\n",
            "Epoch : 74300/100000\n",
            "hypothesis : tensor([151.4742, 184.6529, 180.6451, 196.1788, 141.9439]) cost : 0.169640\n",
            "Epoch : 74400/100000\n",
            "hypothesis : tensor([151.4743, 184.6528, 180.6451, 196.1785, 141.9441]) cost : 0.169617\n",
            "Epoch : 74500/100000\n",
            "hypothesis : tensor([151.4745, 184.6527, 180.6452, 196.1783, 141.9442]) cost : 0.169592\n",
            "Epoch : 74600/100000\n",
            "hypothesis : tensor([151.4746, 184.6526, 180.6453, 196.1780, 141.9444]) cost : 0.169574\n",
            "Epoch : 74700/100000\n",
            "hypothesis : tensor([151.4748, 184.6526, 180.6454, 196.1778, 141.9446]) cost : 0.169552\n",
            "Epoch : 74800/100000\n",
            "hypothesis : tensor([151.4749, 184.6525, 180.6455, 196.1775, 141.9447]) cost : 0.169532\n",
            "Epoch : 74900/100000\n",
            "hypothesis : tensor([151.4751, 184.6524, 180.6456, 196.1773, 141.9449]) cost : 0.169511\n",
            "Epoch : 75000/100000\n",
            "hypothesis : tensor([151.4752, 184.6524, 180.6456, 196.1770, 141.9451]) cost : 0.169487\n",
            "Epoch : 75100/100000\n",
            "hypothesis : tensor([151.4754, 184.6523, 180.6457, 196.1768, 141.9452]) cost : 0.169473\n",
            "Epoch : 75200/100000\n",
            "hypothesis : tensor([151.4755, 184.6522, 180.6458, 196.1765, 141.9454]) cost : 0.169444\n",
            "Epoch : 75300/100000\n",
            "hypothesis : tensor([151.4757, 184.6521, 180.6459, 196.1763, 141.9456]) cost : 0.169422\n",
            "Epoch : 75400/100000\n",
            "hypothesis : tensor([151.4758, 184.6521, 180.6460, 196.1761, 141.9457]) cost : 0.169400\n",
            "Epoch : 75500/100000\n",
            "hypothesis : tensor([151.4760, 184.6520, 180.6461, 196.1758, 141.9459]) cost : 0.169384\n",
            "Epoch : 75600/100000\n",
            "hypothesis : tensor([151.4761, 184.6519, 180.6461, 196.1756, 141.9461]) cost : 0.169365\n",
            "Epoch : 75700/100000\n",
            "hypothesis : tensor([151.4763, 184.6518, 180.6462, 196.1753, 141.9462]) cost : 0.169341\n",
            "Epoch : 75800/100000\n",
            "hypothesis : tensor([151.4764, 184.6518, 180.6463, 196.1751, 141.9464]) cost : 0.169321\n",
            "Epoch : 75900/100000\n",
            "hypothesis : tensor([151.4766, 184.6517, 180.6464, 196.1749, 141.9465]) cost : 0.169303\n",
            "Epoch : 76000/100000\n",
            "hypothesis : tensor([151.4767, 184.6516, 180.6464, 196.1746, 141.9467]) cost : 0.169278\n",
            "Epoch : 76100/100000\n",
            "hypothesis : tensor([151.4769, 184.6516, 180.6465, 196.1744, 141.9469]) cost : 0.169260\n",
            "Epoch : 76200/100000\n",
            "hypothesis : tensor([151.4770, 184.6515, 180.6466, 196.1741, 141.9470]) cost : 0.169240\n",
            "Epoch : 76300/100000\n",
            "hypothesis : tensor([151.4772, 184.6514, 180.6467, 196.1739, 141.9472]) cost : 0.169217\n",
            "Epoch : 76400/100000\n",
            "hypothesis : tensor([151.4773, 184.6514, 180.6468, 196.1737, 141.9474]) cost : 0.169200\n",
            "Epoch : 76500/100000\n",
            "hypothesis : tensor([151.4774, 184.6513, 180.6469, 196.1734, 141.9475]) cost : 0.169183\n",
            "Epoch : 76600/100000\n",
            "hypothesis : tensor([151.4776, 184.6513, 180.6469, 196.1732, 141.9477]) cost : 0.169163\n",
            "Epoch : 76700/100000\n",
            "hypothesis : tensor([151.4777, 184.6512, 180.6470, 196.1730, 141.9479]) cost : 0.169146\n",
            "Epoch : 76800/100000\n",
            "hypothesis : tensor([151.4778, 184.6511, 180.6471, 196.1727, 141.9481]) cost : 0.169122\n",
            "Epoch : 76900/100000\n",
            "hypothesis : tensor([151.4780, 184.6511, 180.6471, 196.1725, 141.9482]) cost : 0.169098\n",
            "Epoch : 77000/100000\n",
            "hypothesis : tensor([151.4781, 184.6510, 180.6472, 196.1723, 141.9484]) cost : 0.169080\n",
            "Epoch : 77100/100000\n",
            "hypothesis : tensor([151.4783, 184.6509, 180.6473, 196.1720, 141.9486]) cost : 0.169066\n",
            "Epoch : 77200/100000\n",
            "hypothesis : tensor([151.4784, 184.6509, 180.6474, 196.1718, 141.9487]) cost : 0.169046\n",
            "Epoch : 77300/100000\n",
            "hypothesis : tensor([151.4785, 184.6508, 180.6475, 196.1716, 141.9489]) cost : 0.169027\n",
            "Epoch : 77400/100000\n",
            "hypothesis : tensor([151.4787, 184.6507, 180.6475, 196.1713, 141.9491]) cost : 0.169006\n",
            "Epoch : 77500/100000\n",
            "hypothesis : tensor([151.4788, 184.6507, 180.6476, 196.1711, 141.9492]) cost : 0.168991\n",
            "Epoch : 77600/100000\n",
            "hypothesis : tensor([151.4789, 184.6506, 180.6477, 196.1708, 141.9494]) cost : 0.168967\n",
            "Epoch : 77700/100000\n",
            "hypothesis : tensor([151.4791, 184.6506, 180.6478, 196.1706, 141.9496]) cost : 0.168946\n",
            "Epoch : 77800/100000\n",
            "hypothesis : tensor([151.4792, 184.6505, 180.6479, 196.1704, 141.9498]) cost : 0.168926\n",
            "Epoch : 77900/100000\n",
            "hypothesis : tensor([151.4793, 184.6505, 180.6479, 196.1702, 141.9499]) cost : 0.168910\n",
            "Epoch : 78000/100000\n",
            "hypothesis : tensor([151.4795, 184.6504, 180.6480, 196.1699, 141.9501]) cost : 0.168889\n",
            "Epoch : 78100/100000\n",
            "hypothesis : tensor([151.4796, 184.6503, 180.6481, 196.1697, 141.9503]) cost : 0.168875\n",
            "Epoch : 78200/100000\n",
            "hypothesis : tensor([151.4797, 184.6503, 180.6481, 196.1694, 141.9504]) cost : 0.168853\n",
            "Epoch : 78300/100000\n",
            "hypothesis : tensor([151.4798, 184.6502, 180.6482, 196.1692, 141.9506]) cost : 0.168834\n",
            "Epoch : 78400/100000\n",
            "hypothesis : tensor([151.4800, 184.6501, 180.6483, 196.1690, 141.9508]) cost : 0.168815\n",
            "Epoch : 78500/100000\n",
            "hypothesis : tensor([151.4801, 184.6501, 180.6484, 196.1687, 141.9510]) cost : 0.168800\n",
            "Epoch : 78600/100000\n",
            "hypothesis : tensor([151.4802, 184.6500, 180.6485, 196.1685, 141.9511]) cost : 0.168781\n",
            "Epoch : 78700/100000\n",
            "hypothesis : tensor([151.4803, 184.6500, 180.6485, 196.1683, 141.9513]) cost : 0.168756\n",
            "Epoch : 78800/100000\n",
            "hypothesis : tensor([151.4805, 184.6499, 180.6486, 196.1680, 141.9515]) cost : 0.168747\n",
            "Epoch : 78900/100000\n",
            "hypothesis : tensor([151.4806, 184.6499, 180.6487, 196.1678, 141.9517]) cost : 0.168725\n",
            "Epoch : 79000/100000\n",
            "hypothesis : tensor([151.4807, 184.6498, 180.6487, 196.1676, 141.9518]) cost : 0.168706\n",
            "Epoch : 79100/100000\n",
            "hypothesis : tensor([151.4808, 184.6498, 180.6488, 196.1673, 141.9520]) cost : 0.168690\n",
            "Epoch : 79200/100000\n",
            "hypothesis : tensor([151.4809, 184.6497, 180.6489, 196.1671, 141.9522]) cost : 0.168672\n",
            "Epoch : 79300/100000\n",
            "hypothesis : tensor([151.4811, 184.6497, 180.6489, 196.1669, 141.9524]) cost : 0.168655\n",
            "Epoch : 79400/100000\n",
            "hypothesis : tensor([151.4812, 184.6496, 180.6490, 196.1666, 141.9525]) cost : 0.168632\n",
            "Epoch : 79500/100000\n",
            "hypothesis : tensor([151.4813, 184.6496, 180.6491, 196.1664, 141.9527]) cost : 0.168618\n",
            "Epoch : 79600/100000\n",
            "hypothesis : tensor([151.4814, 184.6495, 180.6492, 196.1662, 141.9529]) cost : 0.168601\n",
            "Epoch : 79700/100000\n",
            "hypothesis : tensor([151.4816, 184.6494, 180.6492, 196.1660, 141.9530]) cost : 0.168582\n",
            "Epoch : 79800/100000\n",
            "hypothesis : tensor([151.4817, 184.6494, 180.6493, 196.1657, 141.9532]) cost : 0.168565\n",
            "Epoch : 79900/100000\n",
            "hypothesis : tensor([151.4818, 184.6493, 180.6494, 196.1655, 141.9533]) cost : 0.168550\n",
            "Epoch : 80000/100000\n",
            "hypothesis : tensor([151.4819, 184.6493, 180.6494, 196.1653, 141.9535]) cost : 0.168534\n",
            "Epoch : 80100/100000\n",
            "hypothesis : tensor([151.4821, 184.6492, 180.6495, 196.1651, 141.9537]) cost : 0.168510\n",
            "Epoch : 80200/100000\n",
            "hypothesis : tensor([151.4822, 184.6491, 180.6496, 196.1648, 141.9538]) cost : 0.168499\n",
            "Epoch : 80300/100000\n",
            "hypothesis : tensor([151.4823, 184.6491, 180.6497, 196.1646, 141.9540]) cost : 0.168479\n",
            "Epoch : 80400/100000\n",
            "hypothesis : tensor([151.4824, 184.6490, 180.6497, 196.1644, 141.9541]) cost : 0.168465\n",
            "Epoch : 80500/100000\n",
            "hypothesis : tensor([151.4826, 184.6489, 180.6498, 196.1642, 141.9543]) cost : 0.168450\n",
            "Epoch : 80600/100000\n",
            "hypothesis : tensor([151.4827, 184.6489, 180.6499, 196.1639, 141.9544]) cost : 0.168431\n",
            "Epoch : 80700/100000\n",
            "hypothesis : tensor([151.4828, 184.6488, 180.6499, 196.1637, 141.9546]) cost : 0.168413\n",
            "Epoch : 80800/100000\n",
            "hypothesis : tensor([151.4830, 184.6488, 180.6500, 196.1635, 141.9547]) cost : 0.168396\n",
            "Epoch : 80900/100000\n",
            "hypothesis : tensor([151.4831, 184.6487, 180.6501, 196.1633, 141.9549]) cost : 0.168379\n",
            "Epoch : 81000/100000\n",
            "hypothesis : tensor([151.4832, 184.6487, 180.6501, 196.1631, 141.9550]) cost : 0.168366\n",
            "Epoch : 81100/100000\n",
            "hypothesis : tensor([151.4834, 184.6486, 180.6502, 196.1629, 141.9552]) cost : 0.168347\n",
            "Epoch : 81200/100000\n",
            "hypothesis : tensor([151.4835, 184.6486, 180.6503, 196.1627, 141.9553]) cost : 0.168329\n",
            "Epoch : 81300/100000\n",
            "hypothesis : tensor([151.4836, 184.6485, 180.6504, 196.1625, 141.9555]) cost : 0.168320\n",
            "Epoch : 81400/100000\n",
            "hypothesis : tensor([151.4837, 184.6484, 180.6504, 196.1622, 141.9556]) cost : 0.168298\n",
            "Epoch : 81500/100000\n",
            "hypothesis : tensor([151.4839, 184.6483, 180.6505, 196.1620, 141.9557]) cost : 0.168287\n",
            "Epoch : 81600/100000\n",
            "hypothesis : tensor([151.4840, 184.6483, 180.6506, 196.1618, 141.9559]) cost : 0.168269\n",
            "Epoch : 81700/100000\n",
            "hypothesis : tensor([151.4841, 184.6482, 180.6506, 196.1616, 141.9560]) cost : 0.168249\n",
            "Epoch : 81800/100000\n",
            "hypothesis : tensor([151.4843, 184.6482, 180.6507, 196.1614, 141.9562]) cost : 0.168234\n",
            "Epoch : 81900/100000\n",
            "hypothesis : tensor([151.4844, 184.6481, 180.6508, 196.1612, 141.9563]) cost : 0.168222\n",
            "Epoch : 82000/100000\n",
            "hypothesis : tensor([151.4845, 184.6480, 180.6508, 196.1610, 141.9565]) cost : 0.168210\n",
            "Epoch : 82100/100000\n",
            "hypothesis : tensor([151.4846, 184.6480, 180.6509, 196.1608, 141.9566]) cost : 0.168195\n",
            "Epoch : 82200/100000\n",
            "hypothesis : tensor([151.4848, 184.6479, 180.6510, 196.1606, 141.9567]) cost : 0.168178\n",
            "Epoch : 82300/100000\n",
            "hypothesis : tensor([151.4849, 184.6478, 180.6511, 196.1604, 141.9569]) cost : 0.168163\n",
            "Epoch : 82400/100000\n",
            "hypothesis : tensor([151.4850, 184.6478, 180.6511, 196.1602, 141.9570]) cost : 0.168143\n",
            "Epoch : 82500/100000\n",
            "hypothesis : tensor([151.4852, 184.6477, 180.6512, 196.1600, 141.9572]) cost : 0.168131\n",
            "Epoch : 82600/100000\n",
            "hypothesis : tensor([151.4853, 184.6477, 180.6513, 196.1598, 141.9573]) cost : 0.168118\n",
            "Epoch : 82700/100000\n",
            "hypothesis : tensor([151.4854, 184.6476, 180.6513, 196.1596, 141.9574]) cost : 0.168104\n",
            "Epoch : 82800/100000\n",
            "hypothesis : tensor([151.4855, 184.6475, 180.6514, 196.1594, 141.9576]) cost : 0.168083\n",
            "Epoch : 82900/100000\n",
            "hypothesis : tensor([151.4857, 184.6475, 180.6515, 196.1592, 141.9577]) cost : 0.168071\n",
            "Epoch : 83000/100000\n",
            "hypothesis : tensor([151.4858, 184.6474, 180.6515, 196.1590, 141.9578]) cost : 0.168060\n",
            "Epoch : 83100/100000\n",
            "hypothesis : tensor([151.4859, 184.6474, 180.6516, 196.1588, 141.9580]) cost : 0.168045\n",
            "Epoch : 83200/100000\n",
            "hypothesis : tensor([151.4860, 184.6473, 180.6517, 196.1586, 141.9581]) cost : 0.168028\n",
            "Epoch : 83300/100000\n",
            "hypothesis : tensor([151.4861, 184.6472, 180.6517, 196.1584, 141.9583]) cost : 0.168015\n",
            "Epoch : 83400/100000\n",
            "hypothesis : tensor([151.4862, 184.6472, 180.6518, 196.1582, 141.9584]) cost : 0.168003\n",
            "Epoch : 83500/100000\n",
            "hypothesis : tensor([151.4864, 184.6471, 180.6518, 196.1580, 141.9585]) cost : 0.167979\n",
            "Epoch : 83600/100000\n",
            "hypothesis : tensor([151.4865, 184.6471, 180.6519, 196.1578, 141.9587]) cost : 0.167974\n",
            "Epoch : 83700/100000\n",
            "hypothesis : tensor([151.4866, 184.6470, 180.6520, 196.1576, 141.9588]) cost : 0.167956\n",
            "Epoch : 83800/100000\n",
            "hypothesis : tensor([151.4867, 184.6470, 180.6521, 196.1574, 141.9590]) cost : 0.167943\n",
            "Epoch : 83900/100000\n",
            "hypothesis : tensor([151.4868, 184.6469, 180.6521, 196.1572, 141.9591]) cost : 0.167927\n",
            "Epoch : 84000/100000\n",
            "hypothesis : tensor([151.4869, 184.6469, 180.6522, 196.1570, 141.9592]) cost : 0.167915\n",
            "Epoch : 84100/100000\n",
            "hypothesis : tensor([151.4871, 184.6468, 180.6523, 196.1568, 141.9594]) cost : 0.167900\n",
            "Epoch : 84200/100000\n",
            "hypothesis : tensor([151.4872, 184.6468, 180.6523, 196.1566, 141.9595]) cost : 0.167890\n",
            "Epoch : 84300/100000\n",
            "hypothesis : tensor([151.4873, 184.6467, 180.6524, 196.1564, 141.9597]) cost : 0.167868\n",
            "Epoch : 84400/100000\n",
            "hypothesis : tensor([151.4874, 184.6467, 180.6524, 196.1562, 141.9598]) cost : 0.167861\n",
            "Epoch : 84500/100000\n",
            "hypothesis : tensor([151.4875, 184.6466, 180.6525, 196.1560, 141.9599]) cost : 0.167846\n",
            "Epoch : 84600/100000\n",
            "hypothesis : tensor([151.4876, 184.6465, 180.6526, 196.1558, 141.9601]) cost : 0.167836\n",
            "Epoch : 84700/100000\n",
            "hypothesis : tensor([151.4877, 184.6465, 180.6526, 196.1556, 141.9602]) cost : 0.167822\n",
            "Epoch : 84800/100000\n",
            "hypothesis : tensor([151.4879, 184.6465, 180.6527, 196.1554, 141.9604]) cost : 0.167804\n",
            "Epoch : 84900/100000\n",
            "hypothesis : tensor([151.4880, 184.6464, 180.6528, 196.1552, 141.9605]) cost : 0.167794\n",
            "Epoch : 85000/100000\n",
            "hypothesis : tensor([151.4881, 184.6463, 180.6528, 196.1550, 141.9606]) cost : 0.167775\n",
            "Epoch : 85100/100000\n",
            "hypothesis : tensor([151.4882, 184.6463, 180.6529, 196.1548, 141.9608]) cost : 0.167763\n",
            "Epoch : 85200/100000\n",
            "hypothesis : tensor([151.4883, 184.6462, 180.6530, 196.1546, 141.9610]) cost : 0.167755\n",
            "Epoch : 85300/100000\n",
            "hypothesis : tensor([151.4884, 184.6462, 180.6530, 196.1544, 141.9611]) cost : 0.167738\n",
            "Epoch : 85400/100000\n",
            "hypothesis : tensor([151.4885, 184.6462, 180.6531, 196.1542, 141.9612]) cost : 0.167722\n",
            "Epoch : 85500/100000\n",
            "hypothesis : tensor([151.4886, 184.6461, 180.6531, 196.1540, 141.9614]) cost : 0.167706\n",
            "Epoch : 85600/100000\n",
            "hypothesis : tensor([151.4888, 184.6460, 180.6532, 196.1538, 141.9615]) cost : 0.167694\n",
            "Epoch : 85700/100000\n",
            "hypothesis : tensor([151.4889, 184.6460, 180.6533, 196.1537, 141.9617]) cost : 0.167684\n",
            "Epoch : 85800/100000\n",
            "hypothesis : tensor([151.4890, 184.6460, 180.6533, 196.1534, 141.9618]) cost : 0.167669\n",
            "Epoch : 85900/100000\n",
            "hypothesis : tensor([151.4891, 184.6459, 180.6534, 196.1533, 141.9620]) cost : 0.167658\n",
            "Epoch : 86000/100000\n",
            "hypothesis : tensor([151.4892, 184.6459, 180.6535, 196.1531, 141.9621]) cost : 0.167647\n",
            "Epoch : 86100/100000\n",
            "hypothesis : tensor([151.4893, 184.6458, 180.6535, 196.1529, 141.9622]) cost : 0.167628\n",
            "Epoch : 86200/100000\n",
            "hypothesis : tensor([151.4894, 184.6458, 180.6536, 196.1526, 141.9624]) cost : 0.167616\n",
            "Epoch : 86300/100000\n",
            "hypothesis : tensor([151.4895, 184.6457, 180.6536, 196.1525, 141.9626]) cost : 0.167606\n",
            "Epoch : 86400/100000\n",
            "hypothesis : tensor([151.4896, 184.6457, 180.6537, 196.1523, 141.9627]) cost : 0.167593\n",
            "Epoch : 86500/100000\n",
            "hypothesis : tensor([151.4897, 184.6456, 180.6537, 196.1521, 141.9628]) cost : 0.167576\n",
            "Epoch : 86600/100000\n",
            "hypothesis : tensor([151.4898, 184.6456, 180.6538, 196.1519, 141.9630]) cost : 0.167566\n",
            "Epoch : 86700/100000\n",
            "hypothesis : tensor([151.4899, 184.6456, 180.6539, 196.1517, 141.9632]) cost : 0.167552\n",
            "Epoch : 86800/100000\n",
            "hypothesis : tensor([151.4900, 184.6455, 180.6539, 196.1515, 141.9633]) cost : 0.167536\n",
            "Epoch : 86900/100000\n",
            "hypothesis : tensor([151.4901, 184.6455, 180.6540, 196.1513, 141.9634]) cost : 0.167526\n",
            "Epoch : 87000/100000\n",
            "hypothesis : tensor([151.4902, 184.6454, 180.6540, 196.1511, 141.9636]) cost : 0.167516\n",
            "Epoch : 87100/100000\n",
            "hypothesis : tensor([151.4903, 184.6454, 180.6541, 196.1509, 141.9637]) cost : 0.167500\n",
            "Epoch : 87200/100000\n",
            "hypothesis : tensor([151.4904, 184.6453, 180.6541, 196.1507, 141.9639]) cost : 0.167490\n",
            "Epoch : 87300/100000\n",
            "hypothesis : tensor([151.4905, 184.6453, 180.6542, 196.1505, 141.9640]) cost : 0.167478\n",
            "Epoch : 87400/100000\n",
            "hypothesis : tensor([151.4906, 184.6452, 180.6543, 196.1504, 141.9641]) cost : 0.167464\n",
            "Epoch : 87500/100000\n",
            "hypothesis : tensor([151.4907, 184.6452, 180.6543, 196.1501, 141.9643]) cost : 0.167451\n",
            "Epoch : 87600/100000\n",
            "hypothesis : tensor([151.4908, 184.6451, 180.6544, 196.1500, 141.9644]) cost : 0.167447\n",
            "Epoch : 87700/100000\n",
            "hypothesis : tensor([151.4909, 184.6451, 180.6544, 196.1498, 141.9645]) cost : 0.167425\n",
            "Epoch : 87800/100000\n",
            "hypothesis : tensor([151.4910, 184.6451, 180.6545, 196.1496, 141.9647]) cost : 0.167414\n",
            "Epoch : 87900/100000\n",
            "hypothesis : tensor([151.4911, 184.6450, 180.6546, 196.1494, 141.9648]) cost : 0.167408\n",
            "Epoch : 88000/100000\n",
            "hypothesis : tensor([151.4912, 184.6450, 180.6546, 196.1492, 141.9650]) cost : 0.167389\n",
            "Epoch : 88100/100000\n",
            "hypothesis : tensor([151.4913, 184.6449, 180.6547, 196.1490, 141.9651]) cost : 0.167379\n",
            "Epoch : 88200/100000\n",
            "hypothesis : tensor([151.4914, 184.6448, 180.6547, 196.1489, 141.9652]) cost : 0.167371\n",
            "Epoch : 88300/100000\n",
            "hypothesis : tensor([151.4915, 184.6448, 180.6548, 196.1487, 141.9653]) cost : 0.167357\n",
            "Epoch : 88400/100000\n",
            "hypothesis : tensor([151.4916, 184.6447, 180.6548, 196.1485, 141.9654]) cost : 0.167342\n",
            "Epoch : 88500/100000\n",
            "hypothesis : tensor([151.4917, 184.6447, 180.6549, 196.1483, 141.9656]) cost : 0.167335\n",
            "Epoch : 88600/100000\n",
            "hypothesis : tensor([151.4919, 184.6446, 180.6550, 196.1481, 141.9657]) cost : 0.167323\n",
            "Epoch : 88700/100000\n",
            "hypothesis : tensor([151.4919, 184.6446, 180.6550, 196.1480, 141.9658]) cost : 0.167315\n",
            "Epoch : 88800/100000\n",
            "hypothesis : tensor([151.4921, 184.6445, 180.6551, 196.1478, 141.9659]) cost : 0.167297\n",
            "Epoch : 88900/100000\n",
            "hypothesis : tensor([151.4922, 184.6445, 180.6551, 196.1476, 141.9661]) cost : 0.167285\n",
            "Epoch : 89000/100000\n",
            "hypothesis : tensor([151.4923, 184.6444, 180.6552, 196.1474, 141.9662]) cost : 0.167278\n",
            "Epoch : 89100/100000\n",
            "hypothesis : tensor([151.4924, 184.6444, 180.6553, 196.1473, 141.9663]) cost : 0.167269\n",
            "Epoch : 89200/100000\n",
            "hypothesis : tensor([151.4925, 184.6443, 180.6553, 196.1471, 141.9664]) cost : 0.167257\n",
            "Epoch : 89300/100000\n",
            "hypothesis : tensor([151.4926, 184.6443, 180.6554, 196.1469, 141.9666]) cost : 0.167242\n",
            "Epoch : 89400/100000\n",
            "hypothesis : tensor([151.4927, 184.6442, 180.6554, 196.1467, 141.9667]) cost : 0.167237\n",
            "Epoch : 89500/100000\n",
            "hypothesis : tensor([151.4928, 184.6442, 180.6555, 196.1466, 141.9668]) cost : 0.167222\n",
            "Epoch : 89600/100000\n",
            "hypothesis : tensor([151.4929, 184.6441, 180.6555, 196.1464, 141.9669]) cost : 0.167209\n",
            "Epoch : 89700/100000\n",
            "hypothesis : tensor([151.4930, 184.6441, 180.6556, 196.1462, 141.9670]) cost : 0.167201\n",
            "Epoch : 89800/100000\n",
            "hypothesis : tensor([151.4931, 184.6440, 180.6557, 196.1461, 141.9671]) cost : 0.167191\n",
            "Epoch : 89900/100000\n",
            "hypothesis : tensor([151.4932, 184.6440, 180.6557, 196.1459, 141.9672]) cost : 0.167178\n",
            "Epoch : 90000/100000\n",
            "hypothesis : tensor([151.4933, 184.6439, 180.6558, 196.1457, 141.9674]) cost : 0.167169\n",
            "Epoch : 90100/100000\n",
            "hypothesis : tensor([151.4934, 184.6439, 180.6558, 196.1456, 141.9675]) cost : 0.167157\n",
            "Epoch : 90200/100000\n",
            "hypothesis : tensor([151.4935, 184.6438, 180.6559, 196.1454, 141.9676]) cost : 0.167150\n",
            "Epoch : 90300/100000\n",
            "hypothesis : tensor([151.4937, 184.6438, 180.6559, 196.1452, 141.9677]) cost : 0.167138\n",
            "Epoch : 90400/100000\n",
            "hypothesis : tensor([151.4937, 184.6437, 180.6560, 196.1450, 141.9678]) cost : 0.167131\n",
            "Epoch : 90500/100000\n",
            "hypothesis : tensor([151.4939, 184.6436, 180.6561, 196.1449, 141.9679]) cost : 0.167120\n",
            "Epoch : 90600/100000\n",
            "hypothesis : tensor([151.4940, 184.6436, 180.6561, 196.1447, 141.9680]) cost : 0.167106\n",
            "Epoch : 90700/100000\n",
            "hypothesis : tensor([151.4941, 184.6435, 180.6562, 196.1445, 141.9682]) cost : 0.167097\n",
            "Epoch : 90800/100000\n",
            "hypothesis : tensor([151.4942, 184.6435, 180.6562, 196.1444, 141.9683]) cost : 0.167087\n",
            "Epoch : 90900/100000\n",
            "hypothesis : tensor([151.4943, 184.6435, 180.6563, 196.1442, 141.9684]) cost : 0.167076\n",
            "Epoch : 91000/100000\n",
            "hypothesis : tensor([151.4944, 184.6434, 180.6563, 196.1440, 141.9685]) cost : 0.167065\n",
            "Epoch : 91100/100000\n",
            "hypothesis : tensor([151.4945, 184.6433, 180.6564, 196.1439, 141.9686]) cost : 0.167057\n",
            "Epoch : 91200/100000\n",
            "hypothesis : tensor([151.4946, 184.6433, 180.6564, 196.1437, 141.9687]) cost : 0.167042\n",
            "Epoch : 91300/100000\n",
            "hypothesis : tensor([151.4947, 184.6432, 180.6565, 196.1436, 141.9688]) cost : 0.167039\n",
            "Epoch : 91400/100000\n",
            "hypothesis : tensor([151.4948, 184.6432, 180.6565, 196.1434, 141.9689]) cost : 0.167022\n",
            "Epoch : 91500/100000\n",
            "hypothesis : tensor([151.4949, 184.6432, 180.6566, 196.1432, 141.9691]) cost : 0.167015\n",
            "Epoch : 91600/100000\n",
            "hypothesis : tensor([151.4950, 184.6431, 180.6567, 196.1431, 141.9692]) cost : 0.167005\n",
            "Epoch : 91700/100000\n",
            "hypothesis : tensor([151.4951, 184.6431, 180.6567, 196.1429, 141.9693]) cost : 0.166999\n",
            "Epoch : 91800/100000\n",
            "hypothesis : tensor([151.4952, 184.6430, 180.6568, 196.1428, 141.9694]) cost : 0.166987\n",
            "Epoch : 91900/100000\n",
            "hypothesis : tensor([151.4953, 184.6430, 180.6568, 196.1426, 141.9695]) cost : 0.166978\n",
            "Epoch : 92000/100000\n",
            "hypothesis : tensor([151.4954, 184.6429, 180.6569, 196.1424, 141.9696]) cost : 0.166969\n",
            "Epoch : 92100/100000\n",
            "hypothesis : tensor([151.4955, 184.6429, 180.6569, 196.1423, 141.9697]) cost : 0.166958\n",
            "Epoch : 92200/100000\n",
            "hypothesis : tensor([151.4956, 184.6428, 180.6570, 196.1421, 141.9698]) cost : 0.166951\n",
            "Epoch : 92300/100000\n",
            "hypothesis : tensor([151.4957, 184.6428, 180.6570, 196.1420, 141.9699]) cost : 0.166938\n",
            "Epoch : 92400/100000\n",
            "hypothesis : tensor([151.4958, 184.6427, 180.6571, 196.1418, 141.9700]) cost : 0.166930\n",
            "Epoch : 92500/100000\n",
            "hypothesis : tensor([151.4959, 184.6427, 180.6571, 196.1416, 141.9702]) cost : 0.166919\n",
            "Epoch : 92600/100000\n",
            "hypothesis : tensor([151.4959, 184.6427, 180.6572, 196.1415, 141.9703]) cost : 0.166909\n",
            "Epoch : 92700/100000\n",
            "hypothesis : tensor([151.4960, 184.6426, 180.6572, 196.1413, 141.9704]) cost : 0.166901\n",
            "Epoch : 92800/100000\n",
            "hypothesis : tensor([151.4961, 184.6426, 180.6573, 196.1412, 141.9705]) cost : 0.166889\n",
            "Epoch : 92900/100000\n",
            "hypothesis : tensor([151.4962, 184.6425, 180.6573, 196.1410, 141.9706]) cost : 0.166881\n",
            "Epoch : 93000/100000\n",
            "hypothesis : tensor([151.4963, 184.6425, 180.6574, 196.1409, 141.9707]) cost : 0.166874\n",
            "Epoch : 93100/100000\n",
            "hypothesis : tensor([151.4964, 184.6424, 180.6575, 196.1407, 141.9709]) cost : 0.166870\n",
            "Epoch : 93200/100000\n",
            "hypothesis : tensor([151.4965, 184.6424, 180.6575, 196.1405, 141.9710]) cost : 0.166857\n",
            "Epoch : 93300/100000\n",
            "hypothesis : tensor([151.4966, 184.6423, 180.6575, 196.1404, 141.9711]) cost : 0.166847\n",
            "Epoch : 93400/100000\n",
            "hypothesis : tensor([151.4967, 184.6423, 180.6576, 196.1402, 141.9712]) cost : 0.166834\n",
            "Epoch : 93500/100000\n",
            "hypothesis : tensor([151.4968, 184.6423, 180.6577, 196.1400, 141.9713]) cost : 0.166829\n",
            "Epoch : 93600/100000\n",
            "hypothesis : tensor([151.4969, 184.6422, 180.6577, 196.1399, 141.9714]) cost : 0.166820\n",
            "Epoch : 93700/100000\n",
            "hypothesis : tensor([151.4970, 184.6422, 180.6578, 196.1398, 141.9716]) cost : 0.166809\n",
            "Epoch : 93800/100000\n",
            "hypothesis : tensor([151.4971, 184.6421, 180.6578, 196.1396, 141.9717]) cost : 0.166799\n",
            "Epoch : 93900/100000\n",
            "hypothesis : tensor([151.4971, 184.6421, 180.6579, 196.1394, 141.9718]) cost : 0.166791\n",
            "Epoch : 94000/100000\n",
            "hypothesis : tensor([151.4973, 184.6421, 180.6579, 196.1393, 141.9719]) cost : 0.166778\n",
            "Epoch : 94100/100000\n",
            "hypothesis : tensor([151.4973, 184.6420, 180.6580, 196.1391, 141.9720]) cost : 0.166770\n",
            "Epoch : 94200/100000\n",
            "hypothesis : tensor([151.4974, 184.6420, 180.6580, 196.1390, 141.9721]) cost : 0.166763\n",
            "Epoch : 94300/100000\n",
            "hypothesis : tensor([151.4975, 184.6419, 180.6581, 196.1388, 141.9722]) cost : 0.166757\n",
            "Epoch : 94400/100000\n",
            "hypothesis : tensor([151.4976, 184.6419, 180.6581, 196.1386, 141.9724]) cost : 0.166745\n",
            "Epoch : 94500/100000\n",
            "hypothesis : tensor([151.4977, 184.6419, 180.6582, 196.1385, 141.9725]) cost : 0.166735\n",
            "Epoch : 94600/100000\n",
            "hypothesis : tensor([151.4978, 184.6418, 180.6582, 196.1383, 141.9726]) cost : 0.166728\n",
            "Epoch : 94700/100000\n",
            "hypothesis : tensor([151.4979, 184.6418, 180.6583, 196.1382, 141.9727]) cost : 0.166720\n",
            "Epoch : 94800/100000\n",
            "hypothesis : tensor([151.4980, 184.6418, 180.6583, 196.1380, 141.9728]) cost : 0.166710\n",
            "Epoch : 94900/100000\n",
            "hypothesis : tensor([151.4980, 184.6417, 180.6584, 196.1378, 141.9729]) cost : 0.166701\n",
            "Epoch : 95000/100000\n",
            "hypothesis : tensor([151.4981, 184.6417, 180.6584, 196.1377, 141.9731]) cost : 0.166688\n",
            "Epoch : 95100/100000\n",
            "hypothesis : tensor([151.4982, 184.6416, 180.6584, 196.1375, 141.9732]) cost : 0.166681\n",
            "Epoch : 95200/100000\n",
            "hypothesis : tensor([151.4983, 184.6416, 180.6585, 196.1374, 141.9733]) cost : 0.166678\n",
            "Epoch : 95300/100000\n",
            "hypothesis : tensor([151.4984, 184.6416, 180.6586, 196.1372, 141.9734]) cost : 0.166667\n",
            "Epoch : 95400/100000\n",
            "hypothesis : tensor([151.4984, 184.6415, 180.6586, 196.1371, 141.9735]) cost : 0.166654\n",
            "Epoch : 95500/100000\n",
            "hypothesis : tensor([151.4985, 184.6415, 180.6586, 196.1369, 141.9737]) cost : 0.166647\n",
            "Epoch : 95600/100000\n",
            "hypothesis : tensor([151.4986, 184.6415, 180.6587, 196.1367, 141.9738]) cost : 0.166642\n",
            "Epoch : 95700/100000\n",
            "hypothesis : tensor([151.4987, 184.6414, 180.6587, 196.1366, 141.9739]) cost : 0.166629\n",
            "Epoch : 95800/100000\n",
            "hypothesis : tensor([151.4988, 184.6414, 180.6588, 196.1364, 141.9740]) cost : 0.166622\n",
            "Epoch : 95900/100000\n",
            "hypothesis : tensor([151.4988, 184.6414, 180.6588, 196.1363, 141.9741]) cost : 0.166612\n",
            "Epoch : 96000/100000\n",
            "hypothesis : tensor([151.4989, 184.6413, 180.6589, 196.1361, 141.9743]) cost : 0.166603\n",
            "Epoch : 96100/100000\n",
            "hypothesis : tensor([151.4990, 184.6413, 180.6589, 196.1360, 141.9744]) cost : 0.166594\n",
            "Epoch : 96200/100000\n",
            "hypothesis : tensor([151.4991, 184.6413, 180.6590, 196.1358, 141.9745]) cost : 0.166590\n",
            "Epoch : 96300/100000\n",
            "hypothesis : tensor([151.4991, 184.6412, 180.6590, 196.1356, 141.9746]) cost : 0.166583\n",
            "Epoch : 96400/100000\n",
            "hypothesis : tensor([151.4992, 184.6412, 180.6590, 196.1355, 141.9747]) cost : 0.166569\n",
            "Epoch : 96500/100000\n",
            "hypothesis : tensor([151.4993, 184.6412, 180.6591, 196.1353, 141.9749]) cost : 0.166563\n",
            "Epoch : 96600/100000\n",
            "hypothesis : tensor([151.4994, 184.6411, 180.6591, 196.1352, 141.9750]) cost : 0.166555\n",
            "Epoch : 96700/100000\n",
            "hypothesis : tensor([151.4995, 184.6411, 180.6592, 196.1350, 141.9751]) cost : 0.166546\n",
            "Epoch : 96800/100000\n",
            "hypothesis : tensor([151.4996, 184.6411, 180.6593, 196.1349, 141.9752]) cost : 0.166541\n",
            "Epoch : 96900/100000\n",
            "hypothesis : tensor([151.4996, 184.6410, 180.6593, 196.1347, 141.9753]) cost : 0.166531\n",
            "Epoch : 97000/100000\n",
            "hypothesis : tensor([151.4997, 184.6410, 180.6593, 196.1346, 141.9754]) cost : 0.166520\n",
            "Epoch : 97100/100000\n",
            "hypothesis : tensor([151.4998, 184.6409, 180.6594, 196.1344, 141.9755]) cost : 0.166519\n",
            "Epoch : 97200/100000\n",
            "hypothesis : tensor([151.4998, 184.6409, 180.6594, 196.1343, 141.9756]) cost : 0.166511\n",
            "Epoch : 97300/100000\n",
            "hypothesis : tensor([151.4999, 184.6409, 180.6595, 196.1341, 141.9757]) cost : 0.166501\n",
            "Epoch : 97400/100000\n",
            "hypothesis : tensor([151.5000, 184.6408, 180.6595, 196.1340, 141.9759]) cost : 0.166489\n",
            "Epoch : 97500/100000\n",
            "hypothesis : tensor([151.5001, 184.6408, 180.6596, 196.1338, 141.9760]) cost : 0.166489\n",
            "Epoch : 97600/100000\n",
            "hypothesis : tensor([151.5002, 184.6407, 180.6596, 196.1337, 141.9761]) cost : 0.166474\n",
            "Epoch : 97700/100000\n",
            "hypothesis : tensor([151.5003, 184.6407, 180.6596, 196.1335, 141.9762]) cost : 0.166468\n",
            "Epoch : 97800/100000\n",
            "hypothesis : tensor([151.5004, 184.6407, 180.6597, 196.1334, 141.9763]) cost : 0.166456\n",
            "Epoch : 97900/100000\n",
            "hypothesis : tensor([151.5005, 184.6406, 180.6597, 196.1333, 141.9764]) cost : 0.166450\n",
            "Epoch : 98000/100000\n",
            "hypothesis : tensor([151.5005, 184.6406, 180.6598, 196.1331, 141.9765]) cost : 0.166442\n",
            "Epoch : 98100/100000\n",
            "hypothesis : tensor([151.5006, 184.6405, 180.6598, 196.1330, 141.9766]) cost : 0.166440\n",
            "Epoch : 98200/100000\n",
            "hypothesis : tensor([151.5007, 184.6405, 180.6599, 196.1328, 141.9767]) cost : 0.166428\n",
            "Epoch : 98300/100000\n",
            "hypothesis : tensor([151.5008, 184.6405, 180.6599, 196.1327, 141.9768]) cost : 0.166423\n",
            "Epoch : 98400/100000\n",
            "hypothesis : tensor([151.5009, 184.6404, 180.6600, 196.1326, 141.9769]) cost : 0.166411\n",
            "Epoch : 98500/100000\n",
            "hypothesis : tensor([151.5010, 184.6404, 180.6600, 196.1324, 141.9770]) cost : 0.166405\n",
            "Epoch : 98600/100000\n",
            "hypothesis : tensor([151.5011, 184.6403, 180.6600, 196.1323, 141.9771]) cost : 0.166399\n",
            "Epoch : 98700/100000\n",
            "hypothesis : tensor([151.5011, 184.6403, 180.6601, 196.1321, 141.9771]) cost : 0.166387\n",
            "Epoch : 98800/100000\n",
            "hypothesis : tensor([151.5012, 184.6402, 180.6601, 196.1320, 141.9772]) cost : 0.166385\n",
            "Epoch : 98900/100000\n",
            "hypothesis : tensor([151.5013, 184.6402, 180.6602, 196.1319, 141.9773]) cost : 0.166376\n",
            "Epoch : 99000/100000\n",
            "hypothesis : tensor([151.5014, 184.6402, 180.6602, 196.1317, 141.9774]) cost : 0.166371\n",
            "Epoch : 99100/100000\n",
            "hypothesis : tensor([151.5015, 184.6401, 180.6603, 196.1316, 141.9775]) cost : 0.166359\n",
            "Epoch : 99200/100000\n",
            "hypothesis : tensor([151.5016, 184.6401, 180.6603, 196.1314, 141.9776]) cost : 0.166363\n",
            "Epoch : 99300/100000\n",
            "hypothesis : tensor([151.5017, 184.6400, 180.6604, 196.1313, 141.9777]) cost : 0.166347\n",
            "Epoch : 99400/100000\n",
            "hypothesis : tensor([151.5017, 184.6400, 180.6604, 196.1312, 141.9778]) cost : 0.166347\n",
            "Epoch : 99500/100000\n",
            "hypothesis : tensor([151.5018, 184.6400, 180.6605, 196.1310, 141.9779]) cost : 0.166335\n",
            "Epoch : 99600/100000\n",
            "hypothesis : tensor([151.5019, 184.6399, 180.6605, 196.1309, 141.9780]) cost : 0.166326\n",
            "Epoch : 99700/100000\n",
            "hypothesis : tensor([151.5020, 184.6399, 180.6606, 196.1308, 141.9781]) cost : 0.166321\n",
            "Epoch : 99800/100000\n",
            "hypothesis : tensor([151.5021, 184.6398, 180.6606, 196.1306, 141.9782]) cost : 0.166319\n",
            "Epoch : 99900/100000\n",
            "hypothesis : tensor([151.5022, 184.6398, 180.6606, 196.1305, 141.9783]) cost : 0.166305\n",
            "Epoch : 100000/100000\n",
            "hypothesis : tensor([151.5023, 184.6398, 180.6607, 196.1304, 141.9784]) cost : 0.166299\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iWrnYicWmNeS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "30541b92-8d30-42f2-a7bc-94f6e781ef39"
      },
      "source": [
        "hypo"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[151.5023],\n",
              "        [184.6398],\n",
              "        [180.6607],\n",
              "        [196.1304],\n",
              "        [141.9784]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lf0C7r6pmquL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "c54822ac-3a6d-407b-9781-1d9c440d1064"
      },
      "source": [
        "W"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0357],\n",
              "        [0.5156],\n",
              "        [0.4611]], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XngB8Rz4negq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e4b1fdf7-5eb9-41b2-bbf2-778d84c82861"
      },
      "source": [
        "b"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0695], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_sQbdelngLq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0458e082-a110-4f20-c04f-a63a0d362292"
      },
      "source": [
        "cost"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.1663, grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kAmEMhVUnnRz",
        "colab_type": "text"
      },
      "source": [
        "## 04. nn.Module로 구현하는 선형 회귀\n",
        "\n",
        "- 위까지는 선형 회귀 모델을 직접 구현\n",
        "- 지금부터는 PyTorch에 구현된 함수 사용\n",
        "\n",
        "### 04-01. 단순 선형 회귀 구현\n",
        "\n",
        "간단하게 $y = 2x$ 구현\n",
        "- _여기서는 x_train이 두자리만 넘어가도 nan 폭발.._"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wLzHAhvxX1b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "efad4539-003c-4d60-dc83-8606970feb4d"
      },
      "source": [
        "x_train = torch.randint(1,10, (10,1), dtype=torch.float32)\n",
        "y_train = x_train*2\n",
        "\n",
        "print (x_train)\n",
        "\n",
        "print (y_train)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "tensor([[9.],\n        [7.],\n        [3.],\n        [3.],\n        [7.],\n        [6.],\n        [6.],\n        [6.],\n        [1.],\n        [7.]])\ntensor([[18.],\n        [14.],\n        [ 6.],\n        [ 6.],\n        [14.],\n        [12.],\n        [12.],\n        [12.],\n        [ 2.],\n        [14.]])\n"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzNBadrgyKzd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# nn.Linear?"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enZDiBjJxtZt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = nn.Linear(in_features=1, out_features=1)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcCkcLPOybSk",
        "colab_type": "text"
      },
      "source": [
        "model의 W, b 값은 random하게 초기화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prUcoaMjyEdl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "43c3f378-12c9-44b6-c22b-49b8ab720057"
      },
      "source": [
        "list (model.parameters())"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "[Parameter containing:\n tensor([[-0.9614]], requires_grad=True),\n Parameter containing:\n tensor([-0.4768], requires_grad=True)]"
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SM5zbUxQyGpH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "5d3243a8-29eb-4f44-f436-4d29d4037ed3"
      },
      "source": [
        "opti = torch.optim.SGD(model.parameters(), lr = 0.01)\n",
        "opti"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "SGD (\nParameter Group 0\n    dampening: 0\n    lr: 0.01\n    momentum: 0\n    nesterov: False\n    weight_decay: 0\n)"
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuxIlO9jzth6",
        "colab_type": "text"
      },
      "source": [
        "train data는 float type으로 입력해야함\n",
        "- long, double 모두 에러 발생\n",
        "\n",
        "> *Expected object of scalar type Float but got scalar type Double for argument #2 'mat1' in call to _th_addmm*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9jdwfokTyttv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f83d1521-beec-4a78-ee25-a0db19a416bd",
        "tags": [
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend"
        ]
      },
      "source": [
        "epochs = int( 1e+3 ) * 3\n",
        "\n",
        "for epo in range(1, epochs+1) :\n",
        "    hypo = model(x_train)\n",
        "    cost = F.mse_loss(hypo, y_train)\n",
        "\n",
        "    opti.zero_grad()\n",
        "    cost.backward()\n",
        "    opti.step()\n",
        "\n",
        "    if epo % 100 == 0 : \n",
        "        print (f\"Epoch {epo:4d} / {epochs} \\tCost : {cost.item():.6f}\\nHypothesis : {hypo}\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch  100 / 3000 \tCost : 0.000021\nHypothesis : tensor([[18.0047],\n        [14.0011],\n        [ 5.9937],\n        [ 5.9937],\n        [14.0011],\n        [11.9992],\n        [11.9992],\n        [11.9992],\n        [ 1.9900],\n        [14.0011]], grad_fn=<AddmmBackward>)\nEpoch  200 / 3000 \tCost : 0.000012\nHypothesis : tensor([[18.0035],\n        [14.0008],\n        [ 5.9953],\n        [ 5.9953],\n        [14.0008],\n        [11.9994],\n        [11.9994],\n        [11.9994],\n        [ 1.9925],\n        [14.0008]], grad_fn=<AddmmBackward>)\nEpoch  300 / 3000 \tCost : 0.000006\nHypothesis : tensor([[18.0026],\n        [14.0006],\n        [ 5.9965],\n        [ 5.9965],\n        [14.0006],\n        [11.9996],\n        [11.9996],\n        [11.9996],\n        [ 1.9944],\n        [14.0006]], grad_fn=<AddmmBackward>)\nEpoch  400 / 3000 \tCost : 0.000004\nHypothesis : tensor([[18.0020],\n        [14.0004],\n        [ 5.9974],\n        [ 5.9974],\n        [14.0004],\n        [11.9997],\n        [11.9997],\n        [11.9997],\n        [ 1.9958],\n        [14.0004]], grad_fn=<AddmmBackward>)\nEpoch  500 / 3000 \tCost : 0.000002\nHypothesis : tensor([[18.0015],\n        [14.0003],\n        [ 5.9980],\n        [ 5.9980],\n        [14.0003],\n        [11.9998],\n        [11.9998],\n        [11.9998],\n        [ 1.9969],\n        [14.0003]], grad_fn=<AddmmBackward>)\nEpoch  600 / 3000 \tCost : 0.000001\nHypothesis : tensor([[18.0011],\n        [14.0002],\n        [ 5.9985],\n        [ 5.9985],\n        [14.0002],\n        [11.9998],\n        [11.9998],\n        [11.9998],\n        [ 1.9977],\n        [14.0002]], grad_fn=<AddmmBackward>)\nEpoch  700 / 3000 \tCost : 0.000001\nHypothesis : tensor([[18.0008],\n        [14.0002],\n        [ 5.9989],\n        [ 5.9989],\n        [14.0002],\n        [11.9999],\n        [11.9999],\n        [11.9999],\n        [ 1.9982],\n        [14.0002]], grad_fn=<AddmmBackward>)\nEpoch  800 / 3000 \tCost : 0.000000\nHypothesis : tensor([[18.0006],\n        [14.0001],\n        [ 5.9992],\n        [ 5.9992],\n        [14.0001],\n        [11.9999],\n        [11.9999],\n        [11.9999],\n        [ 1.9987],\n        [14.0001]], grad_fn=<AddmmBackward>)\nEpoch  900 / 3000 \tCost : 0.000000\nHypothesis : tensor([[18.0005],\n        [14.0001],\n        [ 5.9994],\n        [ 5.9994],\n        [14.0001],\n        [11.9999],\n        [11.9999],\n        [11.9999],\n        [ 1.9990],\n        [14.0001]], grad_fn=<AddmmBackward>)\nEpoch 1000 / 3000 \tCost : 0.000000\nHypothesis : tensor([[18.0003],\n        [14.0001],\n        [ 5.9995],\n        [ 5.9995],\n        [14.0001],\n        [11.9999],\n        [11.9999],\n        [11.9999],\n        [ 1.9993],\n        [14.0001]], grad_fn=<AddmmBackward>)\nEpoch 1100 / 3000 \tCost : 0.000000\nHypothesis : tensor([[18.0003],\n        [14.0001],\n        [ 5.9997],\n        [ 5.9997],\n        [14.0001],\n        [12.0000],\n        [12.0000],\n        [12.0000],\n        [ 1.9994],\n        [14.0001]], grad_fn=<AddmmBackward>)\nEpoch 1200 / 3000 \tCost : 0.000000\nHypothesis : tensor([[18.0002],\n        [14.0000],\n        [ 5.9997],\n        [ 5.9997],\n        [14.0000],\n        [12.0000],\n        [12.0000],\n        [12.0000],\n        [ 1.9996],\n        [14.0000]], grad_fn=<AddmmBackward>)\nEpoch 1300 / 3000 \tCost : 0.000000\nHypothesis : tensor([[18.0001],\n        [14.0000],\n        [ 5.9998],\n        [ 5.9998],\n        [14.0000],\n        [12.0000],\n        [12.0000],\n        [12.0000],\n        [ 1.9997],\n        [14.0000]], grad_fn=<AddmmBackward>)\nEpoch 1400 / 3000 \tCost : 0.000000\nHypothesis : tensor([[18.0001],\n        [14.0000],\n        [ 5.9999],\n        [ 5.9999],\n        [14.0000],\n        [12.0000],\n        [12.0000],\n        [12.0000],\n        [ 1.9998],\n        [14.0000]], grad_fn=<AddmmBackward>)\nEpoch 1500 / 3000 \tCost : 0.000000\nHypothesis : tensor([[18.0001],\n        [14.0000],\n        [ 5.9999],\n        [ 5.9999],\n        [14.0000],\n        [12.0000],\n        [12.0000],\n        [12.0000],\n        [ 1.9998],\n        [14.0000]], grad_fn=<AddmmBackward>)\nEpoch 1600 / 3000 \tCost : 0.000000\nHypothesis : tensor([[18.0001],\n        [14.0000],\n        [ 5.9999],\n        [ 5.9999],\n        [14.0000],\n        [12.0000],\n        [12.0000],\n        [12.0000],\n        [ 1.9999],\n        [14.0000]], grad_fn=<AddmmBackward>)\nEpoch 1700 / 3000 \tCost : 0.000000\nHypothesis : tensor([[18.0000],\n        [14.0000],\n        [ 5.9999],\n        [ 5.9999],\n        [14.0000],\n        [12.0000],\n        [12.0000],\n        [12.0000],\n        [ 1.9999],\n        [14.0000]], grad_fn=<AddmmBackward>)\nEpoch 1800 / 3000 \tCost : 0.000000\nHypothesis : tensor([[18.0000],\n        [14.0000],\n        [ 6.0000],\n        [ 6.0000],\n        [14.0000],\n        [12.0000],\n        [12.0000],\n        [12.0000],\n        [ 1.9999],\n        [14.0000]], grad_fn=<AddmmBackward>)\nEpoch 1900 / 3000 \tCost : 0.000000\nHypothesis : tensor([[18.0000],\n        [14.0000],\n        [ 6.0000],\n        [ 6.0000],\n        [14.0000],\n        [12.0000],\n        [12.0000],\n        [12.0000],\n        [ 1.9999],\n        [14.0000]], grad_fn=<AddmmBackward>)\nEpoch 2000 / 3000 \tCost : 0.000000\nHypothesis : tensor([[18.0000],\n        [14.0000],\n        [ 6.0000],\n        [ 6.0000],\n        [14.0000],\n        [12.0000],\n        [12.0000],\n        [12.0000],\n        [ 2.0000],\n        [14.0000]], grad_fn=<AddmmBackward>)\nEpoch 2100 / 3000 \tCost : 0.000000\nHypothesis : tensor([[18.0000],\n        [14.0000],\n        [ 6.0000],\n        [ 6.0000],\n        [14.0000],\n        [12.0000],\n        [12.0000],\n        [12.0000],\n        [ 2.0000],\n        [14.0000]], grad_fn=<AddmmBackward>)\nEpoch 2200 / 3000 \tCost : 0.000000\nHypothesis : tensor([[18.0000],\n        [14.0000],\n        [ 6.0000],\n        [ 6.0000],\n        [14.0000],\n        [12.0000],\n        [12.0000],\n        [12.0000],\n        [ 2.0000],\n        [14.0000]], grad_fn=<AddmmBackward>)\nEpoch 2300 / 3000 \tCost : 0.000000\nHypothesis : tensor([[18.0000],\n        [14.0000],\n        [ 6.0000],\n        [ 6.0000],\n        [14.0000],\n        [12.0000],\n        [12.0000],\n        [12.0000],\n        [ 2.0000],\n        [14.0000]], grad_fn=<AddmmBackward>)\nEpoch 2400 / 3000 \tCost : 0.000000\nHypothesis : tensor([[18.0000],\n        [14.0000],\n        [ 6.0000],\n        [ 6.0000],\n        [14.0000],\n        [12.0000],\n        [12.0000],\n        [12.0000],\n        [ 2.0000],\n        [14.0000]], grad_fn=<AddmmBackward>)\nEpoch 2500 / 3000 \tCost : 0.000000\nHypothesis : tensor([[18.0000],\n        [14.0000],\n        [ 6.0000],\n        [ 6.0000],\n        [14.0000],\n        [12.0000],\n        [12.0000],\n        [12.0000],\n        [ 2.0000],\n        [14.0000]], grad_fn=<AddmmBackward>)\nEpoch 2600 / 3000 \tCost : 0.000000\nHypothesis : tensor([[18.0000],\n        [14.0000],\n        [ 6.0000],\n        [ 6.0000],\n        [14.0000],\n        [12.0000],\n        [12.0000],\n        [12.0000],\n        [ 2.0000],\n        [14.0000]], grad_fn=<AddmmBackward>)\nEpoch 2700 / 3000 \tCost : 0.000000\nHypothesis : tensor([[18.0000],\n        [14.0000],\n        [ 6.0000],\n        [ 6.0000],\n        [14.0000],\n        [12.0000],\n        [12.0000],\n        [12.0000],\n        [ 2.0000],\n        [14.0000]], grad_fn=<AddmmBackward>)\nEpoch 2800 / 3000 \tCost : 0.000000\nHypothesis : tensor([[18.0000],\n        [14.0000],\n        [ 6.0000],\n        [ 6.0000],\n        [14.0000],\n        [12.0000],\n        [12.0000],\n        [12.0000],\n        [ 2.0000],\n        [14.0000]], grad_fn=<AddmmBackward>)\nEpoch 2900 / 3000 \tCost : 0.000000\nHypothesis : tensor([[18.0000],\n        [14.0000],\n        [ 6.0000],\n        [ 6.0000],\n        [14.0000],\n        [12.0000],\n        [12.0000],\n        [12.0000],\n        [ 2.0000],\n        [14.0000]], grad_fn=<AddmmBackward>)\nEpoch 3000 / 3000 \tCost : 0.000000\nHypothesis : tensor([[18.0000],\n        [14.0000],\n        [ 6.0000],\n        [ 6.0000],\n        [14.0000],\n        [12.0000],\n        [12.0000],\n        [12.0000],\n        [ 2.0000],\n        [14.0000]], grad_fn=<AddmmBackward>)\n"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- forward 연산 ; hypothesis를 통해 x 입력 - y 산출하는 연산\n",
        "\n",
        "- backward 연산 ; 비용 함수 미분하여 기울기 구하는 연산\n",
        "    - `cost.backward()`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqR6lXqMzeYD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_var = torch.FloatTensor([[4.0]])\n",
        "\n",
        "print (f\"임의의 수 4에 대한 예측값 : {model(new_var).item()} \")"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "임의의 수 4에 대한 예측값 : 7.999998569488525 \n"
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "[Parameter containing:\n tensor([[2.0000]], requires_grad=True),\n Parameter containing:\n tensor([-5.2424e-06], requires_grad=True)]"
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "list(model.parameters())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 04-02. 다중 선형 회귀 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train = torch.FloatTensor([[73, 80, 75],\n",
        "                             [93, 88, 93],\n",
        "                             [89, 91, 90],\n",
        "                             [96, 98, 100],\n",
        "                             [73, 66, 70]])\n",
        "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = nn.Linear(3, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "[Parameter containing:\n tensor([[ 0.4202, -0.0856,  0.3247]], requires_grad=True),\n Parameter containing:\n tensor([0.1856], requires_grad=True)]"
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "list (model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "opti = torch.optim.SGD(model.parameters(), lr = 1e-6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "tags": [
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "      [184.6775],\n        [180.5911],\n        [196.2968],\n        [141.8762]], grad_fn=<AddmmBackward>)\n\nEpoch 488600 / 500000 \tCost : 0.178972\nHypo : tensor([[151.4186],\n        [184.6775],\n        [180.5912],\n        [196.2967],\n        [141.8762]], grad_fn=<AddmmBackward>)\n\nEpoch 488700 / 500000 \tCost : 0.178962\nHypo : tensor([[151.4187],\n        [184.6774],\n        [180.5912],\n        [196.2967],\n        [141.8763]], grad_fn=<AddmmBackward>)\n\nEpoch 488800 / 500000 \tCost : 0.178957\nHypo : tensor([[151.4187],\n        [184.6774],\n        [180.5912],\n        [196.2966],\n        [141.8763]], grad_fn=<AddmmBackward>)\n\nEpoch 488900 / 500000 \tCost : 0.178945\nHypo : tensor([[151.4187],\n        [184.6774],\n        [180.5912],\n        [196.2966],\n        [141.8763]], grad_fn=<AddmmBackward>)\n\nEpoch 489000 / 500000 \tCost : 0.178938\nHypo : tensor([[151.4187],\n        [184.6774],\n        [180.5912],\n        [196.2965],\n        [141.8763]], grad_fn=<AddmmBackward>)\n\nEpoch 489100 / 500000 \tCost : 0.178929\nHypo : tensor([[151.4188],\n        [184.6774],\n        [180.5912],\n        [196.2965],\n        [141.8764]], grad_fn=<AddmmBackward>)\n\nEpoch 489200 / 500000 \tCost : 0.178921\nHypo : tensor([[151.4188],\n        [184.6774],\n        [180.5913],\n        [196.2964],\n        [141.8764]], grad_fn=<AddmmBackward>)\n\nEpoch 489300 / 500000 \tCost : 0.178908\nHypo : tensor([[151.4189],\n        [184.6774],\n        [180.5913],\n        [196.2964],\n        [141.8765]], grad_fn=<AddmmBackward>)\n\nEpoch 489400 / 500000 \tCost : 0.178899\nHypo : tensor([[151.4189],\n        [184.6773],\n        [180.5913],\n        [196.2963],\n        [141.8765]], grad_fn=<AddmmBackward>)\n\nEpoch 489500 / 500000 \tCost : 0.178898\nHypo : tensor([[151.4189],\n        [184.6773],\n        [180.5913],\n        [196.2962],\n        [141.8765]], grad_fn=<AddmmBackward>)\n\nEpoch 489600 / 500000 \tCost : 0.178880\nHypo : tensor([[151.4189],\n        [184.6773],\n        [180.5913],\n        [196.2962],\n        [141.8766]], grad_fn=<AddmmBackward>)\n\nEpoch 489700 / 500000 \tCost : 0.178873\nHypo : tensor([[151.4190],\n        [184.6773],\n        [180.5914],\n        [196.2962],\n        [141.8766]], grad_fn=<AddmmBackward>)\n\nEpoch 489800 / 500000 \tCost : 0.178868\nHypo : tensor([[151.4190],\n        [184.6773],\n        [180.5914],\n        [196.2961],\n        [141.8766]], grad_fn=<AddmmBackward>)\n\nEpoch 489900 / 500000 \tCost : 0.178856\nHypo : tensor([[151.4190],\n        [184.6773],\n        [180.5914],\n        [196.2961],\n        [141.8767]], grad_fn=<AddmmBackward>)\n\nEpoch 490000 / 500000 \tCost : 0.178844\nHypo : tensor([[151.4191],\n        [184.6772],\n        [180.5914],\n        [196.2960],\n        [141.8767]], grad_fn=<AddmmBackward>)\n\nEpoch 490100 / 500000 \tCost : 0.178841\nHypo : tensor([[151.4191],\n        [184.6772],\n        [180.5914],\n        [196.2959],\n        [141.8767]], grad_fn=<AddmmBackward>)\n\nEpoch 490200 / 500000 \tCost : 0.178824\nHypo : tensor([[151.4191],\n        [184.6772],\n        [180.5914],\n        [196.2959],\n        [141.8768]], grad_fn=<AddmmBackward>)\n\nEpoch 490300 / 500000 \tCost : 0.178818\nHypo : tensor([[151.4192],\n        [184.6772],\n        [180.5915],\n        [196.2958],\n        [141.8768]], grad_fn=<AddmmBackward>)\n\nEpoch 490400 / 500000 \tCost : 0.178811\nHypo : tensor([[151.4192],\n        [184.6772],\n        [180.5915],\n        [196.2958],\n        [141.8768]], grad_fn=<AddmmBackward>)\n\nEpoch 490500 / 500000 \tCost : 0.178802\nHypo : tensor([[151.4192],\n        [184.6772],\n        [180.5915],\n        [196.2957],\n        [141.8769]], grad_fn=<AddmmBackward>)\n\nEpoch 490600 / 500000 \tCost : 0.178796\nHypo : tensor([[151.4193],\n        [184.6771],\n        [180.5915],\n        [196.2957],\n        [141.8769]], grad_fn=<AddmmBackward>)\n\nEpoch 490700 / 500000 \tCost : 0.178781\nHypo : tensor([[151.4193],\n        [184.6771],\n        [180.5915],\n        [196.2956],\n        [141.8770]], grad_fn=<AddmmBackward>)\n\nEpoch 490800 / 500000 \tCost : 0.178776\nHypo : tensor([[151.4193],\n        [184.6771],\n        [180.5916],\n        [196.2956],\n        [141.8770]], grad_fn=<AddmmBackward>)\n\nEpoch 490900 / 500000 \tCost : 0.178766\nHypo : tensor([[151.4194],\n        [184.6771],\n        [180.5916],\n        [196.2955],\n        [141.8770]], grad_fn=<AddmmBackward>)\n\nEpoch 491000 / 500000 \tCost : 0.178753\nHypo : tensor([[151.4194],\n        [184.6771],\n        [180.5916],\n        [196.2955],\n        [141.8770]], grad_fn=<AddmmBackward>)\n\nEpoch 491100 / 500000 \tCost : 0.178740\nHypo : tensor([[151.4194],\n        [184.6771],\n        [180.5916],\n        [196.2954],\n        [141.8771]], grad_fn=<AddmmBackward>)\n\nEpoch 491200 / 500000 \tCost : 0.178737\nHypo : tensor([[151.4195],\n        [184.6770],\n        [180.5916],\n        [196.2954],\n        [141.8771]], grad_fn=<AddmmBackward>)\n\nEpoch 491300 / 500000 \tCost : 0.178725\nHypo : tensor([[151.4195],\n        [184.6770],\n        [180.5916],\n        [196.2953],\n        [141.8772]], grad_fn=<AddmmBackward>)\n\nEpoch 491400 / 500000 \tCost : 0.178722\nHypo : tensor([[151.4195],\n        [184.6770],\n        [180.5917],\n        [196.2953],\n        [141.8772]], grad_fn=<AddmmBackward>)\n\nEpoch 491500 / 500000 \tCost : 0.178708\nHypo : tensor([[151.4196],\n        [184.6770],\n        [180.5917],\n        [196.2952],\n        [141.8772]], grad_fn=<AddmmBackward>)\n\nEpoch 491600 / 500000 \tCost : 0.178697\nHypo : tensor([[151.4196],\n        [184.6770],\n        [180.5917],\n        [196.2952],\n        [141.8773]], grad_fn=<AddmmBackward>)\n\nEpoch 491700 / 500000 \tCost : 0.178692\nHypo : tensor([[151.4196],\n        [184.6770],\n        [180.5917],\n        [196.2951],\n        [141.8773]], grad_fn=<AddmmBackward>)\n\nEpoch 491800 / 500000 \tCost : 0.178683\nHypo : tensor([[151.4197],\n        [184.6770],\n        [180.5917],\n        [196.2951],\n        [141.8773]], grad_fn=<AddmmBackward>)\n\nEpoch 491900 / 500000 \tCost : 0.178671\nHypo : tensor([[151.4197],\n        [184.6769],\n        [180.5918],\n        [196.2950],\n        [141.8774]], grad_fn=<AddmmBackward>)\n\nEpoch 492000 / 500000 \tCost : 0.178666\nHypo : tensor([[151.4197],\n        [184.6769],\n        [180.5918],\n        [196.2950],\n        [141.8774]], grad_fn=<AddmmBackward>)\n\nEpoch 492100 / 500000 \tCost : 0.178654\nHypo : tensor([[151.4198],\n        [184.6769],\n        [180.5918],\n        [196.2949],\n        [141.8774]], grad_fn=<AddmmBackward>)\n\nEpoch 492200 / 500000 \tCost : 0.178648\nHypo : tensor([[151.4198],\n        [184.6769],\n        [180.5918],\n        [196.2948],\n        [141.8775]], grad_fn=<AddmmBackward>)\n\nEpoch 492300 / 500000 \tCost : 0.178636\nHypo : tensor([[151.4198],\n        [184.6769],\n        [180.5918],\n        [196.2948],\n        [141.8775]], grad_fn=<AddmmBackward>)\n\nEpoch 492400 / 500000 \tCost : 0.178629\nHypo : tensor([[151.4199],\n        [184.6769],\n        [180.5919],\n        [196.2947],\n        [141.8775]], grad_fn=<AddmmBackward>)\n\nEpoch 492500 / 500000 \tCost : 0.178620\nHypo : tensor([[151.4199],\n        [184.6768],\n        [180.5919],\n        [196.2947],\n        [141.8776]], grad_fn=<AddmmBackward>)\n\nEpoch 492600 / 500000 \tCost : 0.178613\nHypo : tensor([[151.4199],\n        [184.6768],\n        [180.5919],\n        [196.2947],\n        [141.8776]], grad_fn=<AddmmBackward>)\n\nEpoch 492700 / 500000 \tCost : 0.178602\nHypo : tensor([[151.4200],\n        [184.6768],\n        [180.5919],\n        [196.2946],\n        [141.8776]], grad_fn=<AddmmBackward>)\n\nEpoch 492800 / 500000 \tCost : 0.178593\nHypo : tensor([[151.4200],\n        [184.6768],\n        [180.5919],\n        [196.2945],\n        [141.8777]], grad_fn=<AddmmBackward>)\n\nEpoch 492900 / 500000 \tCost : 0.178581\nHypo : tensor([[151.4200],\n        [184.6768],\n        [180.5919],\n        [196.2945],\n        [141.8777]], grad_fn=<AddmmBackward>)\n\nEpoch 493000 / 500000 \tCost : 0.178574\nHypo : tensor([[151.4201],\n        [184.6768],\n        [180.5920],\n        [196.2944],\n        [141.8777]], grad_fn=<AddmmBackward>)\n\nEpoch 493100 / 500000 \tCost : 0.178560\nHypo : tensor([[151.4201],\n        [184.6768],\n        [180.5920],\n        [196.2944],\n        [141.8778]], grad_fn=<AddmmBackward>)\n\nEpoch 493200 / 500000 \tCost : 0.178559\nHypo : tensor([[151.4201],\n        [184.6767],\n        [180.5920],\n        [196.2943],\n        [141.8778]], grad_fn=<AddmmBackward>)\n\nEpoch 493300 / 500000 \tCost : 0.178545\nHypo : tensor([[151.4202],\n        [184.6767],\n        [180.5920],\n        [196.2943],\n        [141.8778]], grad_fn=<AddmmBackward>)\n\nEpoch 493400 / 500000 \tCost : 0.178535\nHypo : tensor([[151.4202],\n        [184.6767],\n        [180.5920],\n        [196.2942],\n        [141.8779]], grad_fn=<AddmmBackward>)\n\nEpoch 493500 / 500000 \tCost : 0.178528\nHypo : tensor([[151.4202],\n        [184.6767],\n        [180.5920],\n        [196.2942],\n        [141.8779]], grad_fn=<AddmmBackward>)\n\nEpoch 493600 / 500000 \tCost : 0.178521\nHypo : tensor([[151.4202],\n        [184.6767],\n        [180.5921],\n        [196.2941],\n        [141.8779]], grad_fn=<AddmmBackward>)\n\nEpoch 493700 / 500000 \tCost : 0.178507\nHypo : tensor([[151.4203],\n        [184.6767],\n        [180.5921],\n        [196.2941],\n        [141.8780]], grad_fn=<AddmmBackward>)\n\nEpoch 493800 / 500000 \tCost : 0.178508\nHypo : tensor([[151.4203],\n        [184.6766],\n        [180.5921],\n        [196.2940],\n        [141.8780]], grad_fn=<AddmmBackward>)\n\nEpoch 493900 / 500000 \tCost : 0.178493\nHypo : tensor([[151.4203],\n        [184.6766],\n        [180.5921],\n        [196.2940],\n        [141.8781]], grad_fn=<AddmmBackward>)\n\nEpoch 494000 / 500000 \tCost : 0.178484\nHypo : tensor([[151.4204],\n        [184.6766],\n        [180.5921],\n        [196.2939],\n        [141.8781]], grad_fn=<AddmmBackward>)\n\nEpoch 494100 / 500000 \tCost : 0.178475\nHypo : tensor([[151.4204],\n        [184.6766],\n        [180.5921],\n        [196.2939],\n        [141.8781]], grad_fn=<AddmmBackward>)\n\nEpoch 494200 / 500000 \tCost : 0.178464\nHypo : tensor([[151.4204],\n        [184.6766],\n        [180.5922],\n        [196.2938],\n        [141.8781]], grad_fn=<AddmmBackward>)\n\nEpoch 494300 / 500000 \tCost : 0.178453\nHypo : tensor([[151.4205],\n        [184.6766],\n        [180.5922],\n        [196.2938],\n        [141.8782]], grad_fn=<AddmmBackward>)\n\nEpoch 494400 / 500000 \tCost : 0.178445\nHypo : tensor([[151.4205],\n        [184.6765],\n        [180.5922],\n        [196.2937],\n        [141.8782]], grad_fn=<AddmmBackward>)\n\nEpoch 494500 / 500000 \tCost : 0.178440\nHypo : tensor([[151.4205],\n        [184.6765],\n        [180.5922],\n        [196.2937],\n        [141.8782]], grad_fn=<AddmmBackward>)\n\nEpoch 494600 / 500000 \tCost : 0.178427\nHypo : tensor([[151.4206],\n        [184.6765],\n        [180.5922],\n        [196.2936],\n        [141.8783]], grad_fn=<AddmmBackward>)\n\nEpoch 494700 / 500000 \tCost : 0.178418\nHypo : tensor([[151.4206],\n        [184.6765],\n        [180.5923],\n        [196.2935],\n        [141.8783]], grad_fn=<AddmmBackward>)\n\nEpoch 494800 / 500000 \tCost : 0.178412\nHypo : tensor([[151.4206],\n        [184.6765],\n        [180.5923],\n        [196.2935],\n        [141.8783]], grad_fn=<AddmmBackward>)\n\nEpoch 494900 / 500000 \tCost : 0.178403\nHypo : tensor([[151.4207],\n        [184.6765],\n        [180.5923],\n        [196.2935],\n        [141.8784]], grad_fn=<AddmmBackward>)\n\nEpoch 495000 / 500000 \tCost : 0.178389\nHypo : tensor([[151.4207],\n        [184.6764],\n        [180.5923],\n        [196.2934],\n        [141.8784]], grad_fn=<AddmmBackward>)\n\nEpoch 495100 / 500000 \tCost : 0.178379\nHypo : tensor([[151.4207],\n        [184.6764],\n        [180.5923],\n        [196.2933],\n        [141.8784]], grad_fn=<AddmmBackward>)\n\nEpoch 495200 / 500000 \tCost : 0.178377\nHypo : tensor([[151.4208],\n        [184.6764],\n        [180.5924],\n        [196.2933],\n        [141.8785]], grad_fn=<AddmmBackward>)\n\nEpoch 495300 / 500000 \tCost : 0.178365\nHypo : tensor([[151.4208],\n        [184.6764],\n        [180.5924],\n        [196.2932],\n        [141.8785]], grad_fn=<AddmmBackward>)\n\nEpoch 495400 / 500000 \tCost : 0.178361\nHypo : tensor([[151.4208],\n        [184.6764],\n        [180.5924],\n        [196.2932],\n        [141.8786]], grad_fn=<AddmmBackward>)\n\nEpoch 495500 / 500000 \tCost : 0.178351\nHypo : tensor([[151.4209],\n        [184.6764],\n        [180.5924],\n        [196.2931],\n        [141.8786]], grad_fn=<AddmmBackward>)\n\nEpoch 495600 / 500000 \tCost : 0.178345\nHypo : tensor([[151.4209],\n        [184.6763],\n        [180.5924],\n        [196.2931],\n        [141.8786]], grad_fn=<AddmmBackward>)\n\nEpoch 495700 / 500000 \tCost : 0.178330\nHypo : tensor([[151.4209],\n        [184.6763],\n        [180.5924],\n        [196.2930],\n        [141.8787]], grad_fn=<AddmmBackward>)\n\nEpoch 495800 / 500000 \tCost : 0.178319\nHypo : tensor([[151.4210],\n        [184.6763],\n        [180.5925],\n        [196.2930],\n        [141.8787]], grad_fn=<AddmmBackward>)\n\nEpoch 495900 / 500000 \tCost : 0.178312\nHypo : tensor([[151.4210],\n        [184.6763],\n        [180.5925],\n        [196.2929],\n        [141.8787]], grad_fn=<AddmmBackward>)\n\nEpoch 496000 / 500000 \tCost : 0.178301\nHypo : tensor([[151.4210],\n        [184.6763],\n        [180.5925],\n        [196.2929],\n        [141.8788]], grad_fn=<AddmmBackward>)\n\nEpoch 496100 / 500000 \tCost : 0.178291\nHypo : tensor([[151.4211],\n        [184.6763],\n        [180.5925],\n        [196.2928],\n        [141.8788]], grad_fn=<AddmmBackward>)\n\nEpoch 496200 / 500000 \tCost : 0.178286\nHypo : tensor([[151.4211],\n        [184.6763],\n        [180.5925],\n        [196.2928],\n        [141.8788]], grad_fn=<AddmmBackward>)\n\nEpoch 496300 / 500000 \tCost : 0.178275\nHypo : tensor([[151.4211],\n        [184.6762],\n        [180.5925],\n        [196.2927],\n        [141.8789]], grad_fn=<AddmmBackward>)\n\nEpoch 496400 / 500000 \tCost : 0.178266\nHypo : tensor([[151.4212],\n        [184.6762],\n        [180.5925],\n        [196.2927],\n        [141.8789]], grad_fn=<AddmmBackward>)\n\nEpoch 496500 / 500000 \tCost : 0.178264\nHypo : tensor([[151.4212],\n        [184.6762],\n        [180.5926],\n        [196.2926],\n        [141.8789]], grad_fn=<AddmmBackward>)\n\nEpoch 496600 / 500000 \tCost : 0.178252\nHypo : tensor([[151.4212],\n        [184.6762],\n        [180.5926],\n        [196.2926],\n        [141.8790]], grad_fn=<AddmmBackward>)\n\nEpoch 496700 / 500000 \tCost : 0.178236\nHypo : tensor([[151.4213],\n        [184.6762],\n        [180.5926],\n        [196.2925],\n        [141.8790]], grad_fn=<AddmmBackward>)\n\nEpoch 496800 / 500000 \tCost : 0.178231\nHypo : tensor([[151.4213],\n        [184.6761],\n        [180.5926],\n        [196.2924],\n        [141.8790]], grad_fn=<AddmmBackward>)\n\nEpoch 496900 / 500000 \tCost : 0.178223\nHypo : tensor([[151.4213],\n        [184.6761],\n        [180.5927],\n        [196.2924],\n        [141.8791]], grad_fn=<AddmmBackward>)\n\nEpoch 497000 / 500000 \tCost : 0.178214\nHypo : tensor([[151.4214],\n        [184.6761],\n        [180.5927],\n        [196.2924],\n        [141.8791]], grad_fn=<AddmmBackward>)\n\nEpoch 497100 / 500000 \tCost : 0.178208\nHypo : tensor([[151.4214],\n        [184.6761],\n        [180.5927],\n        [196.2923],\n        [141.8791]], grad_fn=<AddmmBackward>)\n\nEpoch 497200 / 500000 \tCost : 0.178200\nHypo : tensor([[151.4214],\n        [184.6761],\n        [180.5927],\n        [196.2923],\n        [141.8792]], grad_fn=<AddmmBackward>)\n\nEpoch 497300 / 500000 \tCost : 0.178191\nHypo : tensor([[151.4215],\n        [184.6761],\n        [180.5927],\n        [196.2922],\n        [141.8792]], grad_fn=<AddmmBackward>)\n\nEpoch 497400 / 500000 \tCost : 0.178178\nHypo : tensor([[151.4215],\n        [184.6761],\n        [180.5927],\n        [196.2922],\n        [141.8793]], grad_fn=<AddmmBackward>)\n\nEpoch 497500 / 500000 \tCost : 0.178172\nHypo : tensor([[151.4215],\n        [184.6760],\n        [180.5928],\n        [196.2921],\n        [141.8793]], grad_fn=<AddmmBackward>)\n\nEpoch 497600 / 500000 \tCost : 0.178158\nHypo : tensor([[151.4216],\n        [184.6760],\n        [180.5928],\n        [196.2920],\n        [141.8793]], grad_fn=<AddmmBackward>)\n\nEpoch 497700 / 500000 \tCost : 0.178152\nHypo : tensor([[151.4216],\n        [184.6760],\n        [180.5928],\n        [196.2920],\n        [141.8793]], grad_fn=<AddmmBackward>)\n\nEpoch 497800 / 500000 \tCost : 0.178147\nHypo : tensor([[151.4216],\n        [184.6760],\n        [180.5928],\n        [196.2919],\n        [141.8794]], grad_fn=<AddmmBackward>)\n\nEpoch 497900 / 500000 \tCost : 0.178134\nHypo : tensor([[151.4216],\n        [184.6760],\n        [180.5928],\n        [196.2919],\n        [141.8794]], grad_fn=<AddmmBackward>)\n\nEpoch 498000 / 500000 \tCost : 0.178120\nHypo : tensor([[151.4217],\n        [184.6759],\n        [180.5928],\n        [196.2918],\n        [141.8794]], grad_fn=<AddmmBackward>)\n\nEpoch 498100 / 500000 \tCost : 0.178114\nHypo : tensor([[151.4217],\n        [184.6759],\n        [180.5929],\n        [196.2918],\n        [141.8795]], grad_fn=<AddmmBackward>)\n\nEpoch 498200 / 500000 \tCost : 0.178104\nHypo : tensor([[151.4218],\n        [184.6759],\n        [180.5929],\n        [196.2917],\n        [141.8795]], grad_fn=<AddmmBackward>)\n\nEpoch 498300 / 500000 \tCost : 0.178092\nHypo : tensor([[151.4218],\n        [184.6759],\n        [180.5929],\n        [196.2917],\n        [141.8795]], grad_fn=<AddmmBackward>)\n\nEpoch 498400 / 500000 \tCost : 0.178086\nHypo : tensor([[151.4218],\n        [184.6759],\n        [180.5929],\n        [196.2916],\n        [141.8796]], grad_fn=<AddmmBackward>)\n\nEpoch 498500 / 500000 \tCost : 0.178080\nHypo : tensor([[151.4218],\n        [184.6759],\n        [180.5929],\n        [196.2916],\n        [141.8796]], grad_fn=<AddmmBackward>)\n\nEpoch 498600 / 500000 \tCost : 0.178068\nHypo : tensor([[151.4219],\n        [184.6759],\n        [180.5930],\n        [196.2915],\n        [141.8797]], grad_fn=<AddmmBackward>)\n\nEpoch 498700 / 500000 \tCost : 0.178062\nHypo : tensor([[151.4219],\n        [184.6758],\n        [180.5930],\n        [196.2915],\n        [141.8797]], grad_fn=<AddmmBackward>)\n\nEpoch 498800 / 500000 \tCost : 0.178051\nHypo : tensor([[151.4220],\n        [184.6758],\n        [180.5930],\n        [196.2914],\n        [141.8797]], grad_fn=<AddmmBackward>)\n\nEpoch 498900 / 500000 \tCost : 0.178042\nHypo : tensor([[151.4220],\n        [184.6758],\n        [180.5930],\n        [196.2914],\n        [141.8798]], grad_fn=<AddmmBackward>)\n\nEpoch 499000 / 500000 \tCost : 0.178033\nHypo : tensor([[151.4220],\n        [184.6758],\n        [180.5930],\n        [196.2913],\n        [141.8798]], grad_fn=<AddmmBackward>)\n\nEpoch 499100 / 500000 \tCost : 0.178024\nHypo : tensor([[151.4220],\n        [184.6758],\n        [180.5930],\n        [196.2913],\n        [141.8798]], grad_fn=<AddmmBackward>)\n\nEpoch 499200 / 500000 \tCost : 0.178018\nHypo : tensor([[151.4221],\n        [184.6758],\n        [180.5931],\n        [196.2912],\n        [141.8799]], grad_fn=<AddmmBackward>)\n\nEpoch 499300 / 500000 \tCost : 0.178011\nHypo : tensor([[151.4221],\n        [184.6757],\n        [180.5931],\n        [196.2912],\n        [141.8799]], grad_fn=<AddmmBackward>)\n\nEpoch 499400 / 500000 \tCost : 0.178001\nHypo : tensor([[151.4221],\n        [184.6757],\n        [180.5931],\n        [196.2911],\n        [141.8799]], grad_fn=<AddmmBackward>)\n\nEpoch 499500 / 500000 \tCost : 0.177989\nHypo : tensor([[151.4222],\n        [184.6757],\n        [180.5931],\n        [196.2910],\n        [141.8800]], grad_fn=<AddmmBackward>)\n\nEpoch 499600 / 500000 \tCost : 0.177982\nHypo : tensor([[151.4222],\n        [184.6757],\n        [180.5931],\n        [196.2910],\n        [141.8800]], grad_fn=<AddmmBackward>)\n\nEpoch 499700 / 500000 \tCost : 0.177974\nHypo : tensor([[151.4222],\n        [184.6757],\n        [180.5932],\n        [196.2910],\n        [141.8800]], grad_fn=<AddmmBackward>)\n\nEpoch 499800 / 500000 \tCost : 0.177963\nHypo : tensor([[151.4223],\n        [184.6757],\n        [180.5932],\n        [196.2909],\n        [141.8801]], grad_fn=<AddmmBackward>)\n\nEpoch 499900 / 500000 \tCost : 0.177954\nHypo : tensor([[151.4223],\n        [184.6756],\n        [180.5932],\n        [196.2908],\n        [141.8801]], grad_fn=<AddmmBackward>)\n\nEpoch 500000 / 500000 \tCost : 0.177945\nHypo : tensor([[151.4223],\n        [184.6756],\n        [180.5932],\n        [196.2908],\n        [141.8801]], grad_fn=<AddmmBackward>)\n\n"
        }
      ],
      "source": [
        "epochs = int(1e+5) * 5\n",
        "\n",
        "for epo in range(1, epochs+1) :\n",
        "    hypo = model(x_train)\n",
        "    cost = F.mse_loss(hypo, y_train)\n",
        "\n",
        "    opti.zero_grad()\n",
        "    cost.backward()\n",
        "    opti.step()\n",
        "\n",
        "    if epo % 100 == 0 : \n",
        "        print (f\"Epoch {epo:4d} / {epochs} \\tCost : {cost.item():.6f}\\nHypo : {hypo}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "tensor([[46., 49., 87.]])\n훈련 후 tensor([[46., 49., 87.]]) 에 대한 예측 tensor([[116.4236]], grad_fn=<AddmmBackward>)\n"
        }
      ],
      "source": [
        "new_val = torch.randint(11, 100, (1,3), dtype=torch.float32)\n",
        "print (new_val)\n",
        "\n",
        "print (f\"훈련 후 {new_val} 에 대한 예측 {model(new_val)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "[Parameter containing:\n tensor([[0.9783, 0.4874, 0.5438]], requires_grad=True),\n Parameter containing:\n tensor([0.2316], requires_grad=True)]"
          },
          "metadata": {},
          "execution_count": 82
        }
      ],
      "source": [
        "list (model.parameters())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 05. PyTorch Model in Class\n",
        "\n",
        "url : https://wikidocs.net/60036\n",
        "\n",
        "### 05-02. 단순 선형 회귀 -> 클래스로 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "text": "\u001b[0;31mInit signature:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mDocstring:\u001b[0m     \nBase class for all neural network modules.\n\nYour models should also subclass this class.\n\nModules can also contain other Modules, allowing to nest them in\na tree structure. You can assign the submodules as regular attributes::\n\n    import torch.nn as nn\n    import torch.nn.functional as F\n\n    class Model(nn.Module):\n        def __init__(self):\n            super(Model, self).__init__()\n            self.conv1 = nn.Conv2d(1, 20, 5)\n            self.conv2 = nn.Conv2d(20, 20, 5)\n\n        def forward(self, x):\n            x = F.relu(self.conv1(x))\n            return F.relu(self.conv2(x))\n\nSubmodules assigned in this way will be registered, and will have their\nparameters converted too when you call :meth:`to`, etc.\n\u001b[0;31mInit docstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule.\n\u001b[0;31mFile:\u001b[0m           /opt/conda/envs/py37/lib/python3.7/site-packages/torch/nn/modules/module.py\n\u001b[0;31mType:\u001b[0m           type\n\u001b[0;31mSubclasses:\u001b[0m     Identity, Linear, Bilinear, _ConvNd, Threshold, ReLU, RReLU, Hardtanh, Sigmoid, Tanh, ...\n"
        }
      ],
      "source": [
        "nn.Module?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "tensor([[60.],\n        [92.],\n        [28.],\n        [50.],\n        [60.]])"
          },
          "metadata": {},
          "execution_count": 85
        }
      ],
      "source": [
        "x_train = torch.randint(11, 50, (5,1), dtype = torch.float32)\n",
        "y_train = x_train * 2\n",
        "\n",
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleLinearRegressionModel(nn.Module) :\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(1,1)\n",
        "\n",
        "    def forward(self, x) :\n",
        "        return self.linear(x)\n",
        "\n",
        "model_simple = SimpleLinearRegressionModel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "SGD (\nParameter Group 0\n    dampening: 0\n    lr: 0.001\n    momentum: 0\n    nesterov: False\n    weight_decay: 0\n)"
          },
          "metadata": {},
          "execution_count": 91
        }
      ],
      "source": [
        "opti_simple = torch.optim.SGD(model_simple.parameters(), lr=1e-3)\n",
        "opti_simple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "tags": [
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1000 / 100000 \tCost : 0.002441\nHypo : tensor([[60.0120],\n        [91.9398],\n        [28.0842],\n        [50.0346],\n        [60.0120]], grad_fn=<AddmmBackward>)\n\nEpoch 2000 / 100000 \tCost : 0.001559\nHypo : tensor([[60.0096],\n        [91.9519],\n        [28.0673],\n        [50.0276],\n        [60.0096]], grad_fn=<AddmmBackward>)\n\nEpoch 3000 / 100000 \tCost : 0.000995\nHypo : tensor([[60.0077],\n        [91.9616],\n        [28.0538],\n        [50.0221],\n        [60.0077]], grad_fn=<AddmmBackward>)\n\nEpoch 4000 / 100000 \tCost : 0.000635\nHypo : tensor([[60.0061],\n        [91.9693],\n        [28.0430],\n        [50.0176],\n        [60.0061]], grad_fn=<AddmmBackward>)\n\nEpoch 5000 / 100000 \tCost : 0.000405\nHypo : tensor([[60.0049],\n        [91.9755],\n        [28.0343],\n        [50.0141],\n        [60.0049]], grad_fn=<AddmmBackward>)\n\nEpoch 6000 / 100000 \tCost : 0.000259\nHypo : tensor([[60.0039],\n        [91.9804],\n        [28.0274],\n        [50.0113],\n        [60.0039]], grad_fn=<AddmmBackward>)\n\nEpoch 7000 / 100000 \tCost : 0.000165\nHypo : tensor([[60.0031],\n        [91.9843],\n        [28.0219],\n        [50.0090],\n        [60.0031]], grad_fn=<AddmmBackward>)\n\nEpoch 8000 / 100000 \tCost : 0.000106\nHypo : tensor([[60.0025],\n        [91.9875],\n        [28.0175],\n        [50.0072],\n        [60.0025]], grad_fn=<AddmmBackward>)\n\nEpoch 9000 / 100000 \tCost : 0.000067\nHypo : tensor([[60.0020],\n        [91.9900],\n        [28.0140],\n        [50.0057],\n        [60.0020]], grad_fn=<AddmmBackward>)\n\nEpoch 10000 / 100000 \tCost : 0.000043\nHypo : tensor([[60.0016],\n        [91.9920],\n        [28.0112],\n        [50.0046],\n        [60.0016]], grad_fn=<AddmmBackward>)\n\nEpoch 11000 / 100000 \tCost : 0.000027\nHypo : tensor([[60.0013],\n        [91.9936],\n        [28.0089],\n        [50.0036],\n        [60.0013]], grad_fn=<AddmmBackward>)\n\nEpoch 12000 / 100000 \tCost : 0.000018\nHypo : tensor([[60.0010],\n        [91.9949],\n        [28.0071],\n        [50.0029],\n        [60.0010]], grad_fn=<AddmmBackward>)\n\nEpoch 13000 / 100000 \tCost : 0.000011\nHypo : tensor([[60.0008],\n        [91.9959],\n        [28.0057],\n        [50.0023],\n        [60.0008]], grad_fn=<AddmmBackward>)\n\nEpoch 14000 / 100000 \tCost : 0.000007\nHypo : tensor([[60.0007],\n        [91.9967],\n        [28.0046],\n        [50.0019],\n        [60.0007]], grad_fn=<AddmmBackward>)\n\nEpoch 15000 / 100000 \tCost : 0.000005\nHypo : tensor([[60.0005],\n        [91.9974],\n        [28.0036],\n        [50.0015],\n        [60.0005]], grad_fn=<AddmmBackward>)\n\nEpoch 16000 / 100000 \tCost : 0.000003\nHypo : tensor([[60.0004],\n        [91.9979],\n        [28.0029],\n        [50.0012],\n        [60.0004]], grad_fn=<AddmmBackward>)\n\nEpoch 17000 / 100000 \tCost : 0.000002\nHypo : tensor([[60.0003],\n        [91.9983],\n        [28.0023],\n        [50.0010],\n        [60.0003]], grad_fn=<AddmmBackward>)\n\nEpoch 18000 / 100000 \tCost : 0.000001\nHypo : tensor([[60.0003],\n        [91.9987],\n        [28.0019],\n        [50.0008],\n        [60.0003]], grad_fn=<AddmmBackward>)\n\nEpoch 19000 / 100000 \tCost : 0.000001\nHypo : tensor([[60.0002],\n        [91.9989],\n        [28.0015],\n        [50.0006],\n        [60.0002]], grad_fn=<AddmmBackward>)\n\nEpoch 20000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0002],\n        [91.9991],\n        [28.0012],\n        [50.0005],\n        [60.0002]], grad_fn=<AddmmBackward>)\n\nEpoch 21000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0001],\n        [91.9993],\n        [28.0009],\n        [50.0004],\n        [60.0001]], grad_fn=<AddmmBackward>)\n\nEpoch 22000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0001],\n        [91.9995],\n        [28.0008],\n        [50.0003],\n        [60.0001]], grad_fn=<AddmmBackward>)\n\nEpoch 23000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0001],\n        [91.9996],\n        [28.0006],\n        [50.0002],\n        [60.0001]], grad_fn=<AddmmBackward>)\n\nEpoch 24000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0001],\n        [91.9997],\n        [28.0005],\n        [50.0002],\n        [60.0001]], grad_fn=<AddmmBackward>)\n\nEpoch 25000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [91.9997],\n        [28.0004],\n        [50.0002],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 26000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [91.9998],\n        [28.0003],\n        [50.0001],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 27000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [91.9998],\n        [28.0003],\n        [50.0001],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 28000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [91.9999],\n        [28.0002],\n        [50.0001],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 29000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [91.9999],\n        [28.0002],\n        [50.0001],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 30000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [91.9999],\n        [28.0001],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 31000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [91.9999],\n        [28.0001],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 32000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [91.9999],\n        [28.0001],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 33000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [91.9999],\n        [28.0001],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 34000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 35000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 36000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 37000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 38000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 39000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 40000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 41000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 42000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 43000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 44000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 45000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 46000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 47000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 48000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 49000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 50000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 51000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 52000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 53000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 54000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 55000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 56000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 57000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 58000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 59000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 60000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 61000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 62000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 63000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 64000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 65000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 66000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 67000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 68000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 69000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 70000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 71000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 72000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 73000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 74000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 75000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 76000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 77000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 78000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 79000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 80000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 81000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 82000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 83000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 84000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 85000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 86000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 87000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 88000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 89000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 90000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 91000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 92000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 93000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 94000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 95000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 96000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 97000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 98000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 99000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\nEpoch 100000 / 100000 \tCost : 0.000000\nHypo : tensor([[60.0000],\n        [92.0000],\n        [28.0000],\n        [50.0000],\n        [60.0000]], grad_fn=<AddmmBackward>)\n\n"
        }
      ],
      "source": [
        "epochs_simple = int (1e+5)\n",
        "\n",
        "for epo in range(1, epochs_simple+1) :\n",
        "    hypo = model_simple(x_train)\n",
        "\n",
        "    cost = F.mse_loss(hypo, y_train)\n",
        "\n",
        "    opti_simple.zero_grad()\n",
        "    cost.backward()\n",
        "    opti_simple.step()\n",
        "\n",
        "    if epo % 1000 == 0 : \n",
        "        print (f\"Epoch {epo:4d} / {epochs_simple} \\tCost : {cost.item():.6f}\\nHypo : {hypo}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "[Parameter containing:\n tensor([[2.0000]], requires_grad=True),\n Parameter containing:\n tensor([3.8145e-06], requires_grad=True)]"
          },
          "metadata": {},
          "execution_count": 93
        }
      ],
      "source": [
        "list(model_simple.parameters())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 05-03. 다중 선형 회귀 클래스로 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train = torch.FloatTensor([[73, 80, 75],\n",
        "                             [93, 88, 93],\n",
        "                             [89, 91, 90],\n",
        "                             [96, 98, 100],\n",
        "                             [73, 66, 70]])\n",
        "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1000 / 100000 \tCost : 3.945061\nHypo : tensor([[153.8632],\n        [182.9433],\n        [181.3112],\n        [197.3412],\n        [139.0836]], grad_fn=<AddmmBackward>)\n\nEpoch 2000 / 100000 \tCost : 2.433433\nHypo : tensor([[153.2261],\n        [183.3821],\n        [181.1185],\n        [197.1817],\n        [139.6765]], grad_fn=<AddmmBackward>)\n\nEpoch 3000 / 100000 \tCost : 1.551108\nHypo : tensor([[152.7421],\n        [183.7158],\n        [180.9728],\n        [197.0567],\n        [140.1311]], grad_fn=<AddmmBackward>)\n\nEpoch 4000 / 100000 \tCost : 1.034909\nHypo : tensor([[152.3750],\n        [183.9695],\n        [180.8627],\n        [196.9579],\n        [140.4801]], grad_fn=<AddmmBackward>)\n\nEpoch 5000 / 100000 \tCost : 0.731808\nHypo : tensor([[152.0968],\n        [184.1620],\n        [180.7798],\n        [196.8793],\n        [140.7486]], grad_fn=<AddmmBackward>)\n\nEpoch 6000 / 100000 \tCost : 0.552784\nHypo : tensor([[151.8866],\n        [184.3080],\n        [180.7177],\n        [196.8162],\n        [140.9556]], grad_fn=<AddmmBackward>)\n\nEpoch 7000 / 100000 \tCost : 0.446045\nHypo : tensor([[151.7280],\n        [184.4186],\n        [180.6714],\n        [196.7650],\n        [141.1156]], grad_fn=<AddmmBackward>)\n\nEpoch 8000 / 100000 \tCost : 0.381464\nHypo : tensor([[151.6088],\n        [184.5020],\n        [180.6371],\n        [196.7228],\n        [141.2398]], grad_fn=<AddmmBackward>)\n\nEpoch 9000 / 100000 \tCost : 0.341526\nHypo : tensor([[151.5195],\n        [184.5649],\n        [180.6119],\n        [196.6877],\n        [141.3366]], grad_fn=<AddmmBackward>)\n\nEpoch 10000 / 100000 \tCost : 0.316032\nHypo : tensor([[151.4532],\n        [184.6121],\n        [180.5937],\n        [196.6580],\n        [141.4125]], grad_fn=<AddmmBackward>)\n\nEpoch 11000 / 100000 \tCost : 0.299041\nHypo : tensor([[151.4042],\n        [184.6473],\n        [180.5807],\n        [196.6326],\n        [141.4723]], grad_fn=<AddmmBackward>)\n\nEpoch 12000 / 100000 \tCost : 0.287112\nHypo : tensor([[151.3683],\n        [184.6734],\n        [180.5717],\n        [196.6104],\n        [141.5199]], grad_fn=<AddmmBackward>)\n\nEpoch 13000 / 100000 \tCost : 0.278212\nHypo : tensor([[151.3426],\n        [184.6927],\n        [180.5658],\n        [196.5908],\n        [141.5581]], grad_fn=<AddmmBackward>)\n\nEpoch 14000 / 100000 \tCost : 0.271168\nHypo : tensor([[151.3244],\n        [184.7066],\n        [180.5621],\n        [196.5732],\n        [141.5891]], grad_fn=<AddmmBackward>)\n\nEpoch 15000 / 100000 \tCost : 0.265301\nHypo : tensor([[151.3121],\n        [184.7166],\n        [180.5602],\n        [196.5572],\n        [141.6145]], grad_fn=<AddmmBackward>)\n\nEpoch 16000 / 100000 \tCost : 0.260197\nHypo : tensor([[151.3040],\n        [184.7235],\n        [180.5595],\n        [196.5425],\n        [141.6357]], grad_fn=<AddmmBackward>)\n\nEpoch 17000 / 100000 \tCost : 0.255613\nHypo : tensor([[151.2993],\n        [184.7281],\n        [180.5598],\n        [196.5289],\n        [141.6535]], grad_fn=<AddmmBackward>)\n\nEpoch 18000 / 100000 \tCost : 0.251406\nHypo : tensor([[151.2971],\n        [184.7310],\n        [180.5609],\n        [196.5161],\n        [141.6688]], grad_fn=<AddmmBackward>)\n\nEpoch 19000 / 100000 \tCost : 0.247495\nHypo : tensor([[151.2967],\n        [184.7326],\n        [180.5625],\n        [196.5040],\n        [141.6821]], grad_fn=<AddmmBackward>)\n\nEpoch 20000 / 100000 \tCost : 0.243822\nHypo : tensor([[151.2978],\n        [184.7332],\n        [180.5644],\n        [196.4925],\n        [141.6938]], grad_fn=<AddmmBackward>)\n\nEpoch 21000 / 100000 \tCost : 0.240359\nHypo : tensor([[151.2998],\n        [184.7331],\n        [180.5666],\n        [196.4815],\n        [141.7043]], grad_fn=<AddmmBackward>)\n\nEpoch 22000 / 100000 \tCost : 0.237078\nHypo : tensor([[151.3026],\n        [184.7325],\n        [180.5690],\n        [196.4709],\n        [141.7139]], grad_fn=<AddmmBackward>)\n\nEpoch 23000 / 100000 \tCost : 0.233962\nHypo : tensor([[151.3060],\n        [184.7314],\n        [180.5716],\n        [196.4608],\n        [141.7227]], grad_fn=<AddmmBackward>)\n\nEpoch 24000 / 100000 \tCost : 0.230996\nHypo : tensor([[151.3097],\n        [184.7300],\n        [180.5742],\n        [196.4510],\n        [141.7308]], grad_fn=<AddmmBackward>)\n\nEpoch 25000 / 100000 \tCost : 0.228182\nHypo : tensor([[151.3137],\n        [184.7284],\n        [180.5769],\n        [196.4415],\n        [141.7384]], grad_fn=<AddmmBackward>)\n\nEpoch 26000 / 100000 \tCost : 0.225495\nHypo : tensor([[151.3179],\n        [184.7267],\n        [180.5796],\n        [196.4323],\n        [141.7457]], grad_fn=<AddmmBackward>)\n\nEpoch 27000 / 100000 \tCost : 0.222947\nHypo : tensor([[151.3221],\n        [184.7249],\n        [180.5823],\n        [196.4233],\n        [141.7525]], grad_fn=<AddmmBackward>)\n\nEpoch 28000 / 100000 \tCost : 0.220501\nHypo : tensor([[151.3264],\n        [184.7230],\n        [180.5849],\n        [196.4146],\n        [141.7590]], grad_fn=<AddmmBackward>)\n\nEpoch 29000 / 100000 \tCost : 0.218184\nHypo : tensor([[151.3308],\n        [184.7210],\n        [180.5876],\n        [196.4061],\n        [141.7653]], grad_fn=<AddmmBackward>)\n\nEpoch 30000 / 100000 \tCost : 0.215977\nHypo : tensor([[151.3351],\n        [184.7191],\n        [180.5902],\n        [196.3979],\n        [141.7713]], grad_fn=<AddmmBackward>)\n\nEpoch 31000 / 100000 \tCost : 0.213873\nHypo : tensor([[151.3394],\n        [184.7172],\n        [180.5927],\n        [196.3899],\n        [141.7772]], grad_fn=<AddmmBackward>)\n\nEpoch 32000 / 100000 \tCost : 0.211865\nHypo : tensor([[151.3437],\n        [184.7152],\n        [180.5953],\n        [196.3821],\n        [141.7828]], grad_fn=<AddmmBackward>)\n\nEpoch 33000 / 100000 \tCost : 0.209959\nHypo : tensor([[151.3479],\n        [184.7133],\n        [180.5977],\n        [196.3744],\n        [141.7883]], grad_fn=<AddmmBackward>)\n\nEpoch 34000 / 100000 \tCost : 0.208136\nHypo : tensor([[151.3520],\n        [184.7114],\n        [180.6002],\n        [196.3670],\n        [141.7936]], grad_fn=<AddmmBackward>)\n\nEpoch 35000 / 100000 \tCost : 0.206397\nHypo : tensor([[151.3561],\n        [184.7096],\n        [180.6026],\n        [196.3598],\n        [141.7988]], grad_fn=<AddmmBackward>)\n\nEpoch 36000 / 100000 \tCost : 0.204748\nHypo : tensor([[151.3600],\n        [184.7077],\n        [180.6049],\n        [196.3527],\n        [141.8038]], grad_fn=<AddmmBackward>)\n\nEpoch 37000 / 100000 \tCost : 0.203168\nHypo : tensor([[151.3639],\n        [184.7059],\n        [180.6071],\n        [196.3458],\n        [141.8087]], grad_fn=<AddmmBackward>)\n\nEpoch 38000 / 100000 \tCost : 0.201668\nHypo : tensor([[151.3677],\n        [184.7042],\n        [180.6093],\n        [196.3391],\n        [141.8135]], grad_fn=<AddmmBackward>)\n\nEpoch 39000 / 100000 \tCost : 0.200241\nHypo : tensor([[151.3714],\n        [184.7024],\n        [180.6115],\n        [196.3325],\n        [141.8181]], grad_fn=<AddmmBackward>)\n\nEpoch 40000 / 100000 \tCost : 0.198877\nHypo : tensor([[151.3751],\n        [184.7008],\n        [180.6136],\n        [196.3261],\n        [141.8226]], grad_fn=<AddmmBackward>)\n\nEpoch 41000 / 100000 \tCost : 0.197578\nHypo : tensor([[151.3786],\n        [184.6991],\n        [180.6157],\n        [196.3198],\n        [141.8271]], grad_fn=<AddmmBackward>)\n\nEpoch 42000 / 100000 \tCost : 0.196339\nHypo : tensor([[151.3821],\n        [184.6975],\n        [180.6177],\n        [196.3137],\n        [141.8314]], grad_fn=<AddmmBackward>)\n\nEpoch 43000 / 100000 \tCost : 0.195164\nHypo : tensor([[151.3855],\n        [184.6959],\n        [180.6196],\n        [196.3078],\n        [141.8356]], grad_fn=<AddmmBackward>)\n\nEpoch 44000 / 100000 \tCost : 0.194037\nHypo : tensor([[151.3888],\n        [184.6943],\n        [180.6216],\n        [196.3019],\n        [141.8397]], grad_fn=<AddmmBackward>)\n\nEpoch 45000 / 100000 \tCost : 0.192967\nHypo : tensor([[151.3921],\n        [184.6929],\n        [180.6234],\n        [196.2963],\n        [141.8437]], grad_fn=<AddmmBackward>)\n\nEpoch 46000 / 100000 \tCost : 0.191944\nHypo : tensor([[151.3952],\n        [184.6914],\n        [180.6253],\n        [196.2908],\n        [141.8476]], grad_fn=<AddmmBackward>)\n\nEpoch 47000 / 100000 \tCost : 0.190967\nHypo : tensor([[151.3983],\n        [184.6899],\n        [180.6270],\n        [196.2854],\n        [141.8514]], grad_fn=<AddmmBackward>)\n\nEpoch 48000 / 100000 \tCost : 0.190040\nHypo : tensor([[151.4013],\n        [184.6885],\n        [180.6288],\n        [196.2801],\n        [141.8551]], grad_fn=<AddmmBackward>)\n\nEpoch 49000 / 100000 \tCost : 0.189168\nHypo : tensor([[151.4043],\n        [184.6871],\n        [180.6305],\n        [196.2750],\n        [141.8586]], grad_fn=<AddmmBackward>)\n\nEpoch 50000 / 100000 \tCost : 0.188320\nHypo : tensor([[151.4072],\n        [184.6858],\n        [180.6321],\n        [196.2699],\n        [141.8623]], grad_fn=<AddmmBackward>)\n\nEpoch 51000 / 100000 \tCost : 0.187513\nHypo : tensor([[151.4099],\n        [184.6845],\n        [180.6337],\n        [196.2650],\n        [141.8658]], grad_fn=<AddmmBackward>)\n\nEpoch 52000 / 100000 \tCost : 0.186757\nHypo : tensor([[151.4128],\n        [184.6832],\n        [180.6353],\n        [196.2603],\n        [141.8690]], grad_fn=<AddmmBackward>)\n\nEpoch 53000 / 100000 \tCost : 0.186023\nHypo : tensor([[151.4154],\n        [184.6820],\n        [180.6369],\n        [196.2556],\n        [141.8724]], grad_fn=<AddmmBackward>)\n\nEpoch 54000 / 100000 \tCost : 0.185319\nHypo : tensor([[151.4180],\n        [184.6808],\n        [180.6383],\n        [196.2510],\n        [141.8757]], grad_fn=<AddmmBackward>)\n\nEpoch 55000 / 100000 \tCost : 0.184670\nHypo : tensor([[151.4205],\n        [184.6796],\n        [180.6398],\n        [196.2466],\n        [141.8787]], grad_fn=<AddmmBackward>)\n\nEpoch 56000 / 100000 \tCost : 0.184040\nHypo : tensor([[151.4231],\n        [184.6784],\n        [180.6413],\n        [196.2423],\n        [141.8817]], grad_fn=<AddmmBackward>)\n\nEpoch 57000 / 100000 \tCost : 0.183426\nHypo : tensor([[151.4254],\n        [184.6773],\n        [180.6426],\n        [196.2380],\n        [141.8848]], grad_fn=<AddmmBackward>)\n\nEpoch 58000 / 100000 \tCost : 0.182857\nHypo : tensor([[151.4278],\n        [184.6762],\n        [180.6440],\n        [196.2339],\n        [141.8878]], grad_fn=<AddmmBackward>)\n\nEpoch 59000 / 100000 \tCost : 0.182311\nHypo : tensor([[151.4302],\n        [184.6751],\n        [180.6453],\n        [196.2299],\n        [141.8905]], grad_fn=<AddmmBackward>)\n\nEpoch 60000 / 100000 \tCost : 0.181790\nHypo : tensor([[151.4324],\n        [184.6740],\n        [180.6466],\n        [196.2260],\n        [141.8933]], grad_fn=<AddmmBackward>)\n\nEpoch 61000 / 100000 \tCost : 0.181284\nHypo : tensor([[151.4346],\n        [184.6731],\n        [180.6479],\n        [196.2221],\n        [141.8961]], grad_fn=<AddmmBackward>)\n\nEpoch 62000 / 100000 \tCost : 0.180812\nHypo : tensor([[151.4367],\n        [184.6720],\n        [180.6491],\n        [196.2183],\n        [141.8988]], grad_fn=<AddmmBackward>)\n\nEpoch 63000 / 100000 \tCost : 0.180363\nHypo : tensor([[151.4389],\n        [184.6710],\n        [180.6503],\n        [196.2147],\n        [141.9013]], grad_fn=<AddmmBackward>)\n\nEpoch 64000 / 100000 \tCost : 0.179938\nHypo : tensor([[151.4410],\n        [184.6701],\n        [180.6515],\n        [196.2112],\n        [141.9037]], grad_fn=<AddmmBackward>)\n\nEpoch 65000 / 100000 \tCost : 0.179519\nHypo : tensor([[151.4429],\n        [184.6692],\n        [180.6526],\n        [196.2076],\n        [141.9063]], grad_fn=<AddmmBackward>)\n\nEpoch 66000 / 100000 \tCost : 0.179124\nHypo : tensor([[151.4448],\n        [184.6683],\n        [180.6537],\n        [196.2042],\n        [141.9088]], grad_fn=<AddmmBackward>)\n\nEpoch 67000 / 100000 \tCost : 0.178750\nHypo : tensor([[151.4468],\n        [184.6673],\n        [180.6548],\n        [196.2009],\n        [141.9111]], grad_fn=<AddmmBackward>)\n\nEpoch 68000 / 100000 \tCost : 0.178397\nHypo : tensor([[151.4487],\n        [184.6664],\n        [180.6559],\n        [196.1977],\n        [141.9132]], grad_fn=<AddmmBackward>)\n\nEpoch 69000 / 100000 \tCost : 0.178059\nHypo : tensor([[151.4505],\n        [184.6656],\n        [180.6569],\n        [196.1945],\n        [141.9155]], grad_fn=<AddmmBackward>)\n\nEpoch 70000 / 100000 \tCost : 0.177732\nHypo : tensor([[151.4523],\n        [184.6648],\n        [180.6579],\n        [196.1914],\n        [141.9178]], grad_fn=<AddmmBackward>)\n\nEpoch 71000 / 100000 \tCost : 0.177422\nHypo : tensor([[151.4539],\n        [184.6640],\n        [180.6589],\n        [196.1884],\n        [141.9200]], grad_fn=<AddmmBackward>)\n\nEpoch 72000 / 100000 \tCost : 0.177123\nHypo : tensor([[151.4556],\n        [184.6632],\n        [180.6599],\n        [196.1854],\n        [141.9220]], grad_fn=<AddmmBackward>)\n\nEpoch 73000 / 100000 \tCost : 0.176848\nHypo : tensor([[151.4574],\n        [184.6624],\n        [180.6608],\n        [196.1826],\n        [141.9239]], grad_fn=<AddmmBackward>)\n\nEpoch 74000 / 100000 \tCost : 0.176580\nHypo : tensor([[151.4590],\n        [184.6616],\n        [180.6617],\n        [196.1799],\n        [141.9259]], grad_fn=<AddmmBackward>)\n\nEpoch 75000 / 100000 \tCost : 0.176322\nHypo : tensor([[151.4606],\n        [184.6609],\n        [180.6626],\n        [196.1771],\n        [141.9279]], grad_fn=<AddmmBackward>)\n\nEpoch 76000 / 100000 \tCost : 0.176075\nHypo : tensor([[151.4620],\n        [184.6603],\n        [180.6635],\n        [196.1744],\n        [141.9299]], grad_fn=<AddmmBackward>)\n\nEpoch 77000 / 100000 \tCost : 0.175841\nHypo : tensor([[151.4635],\n        [184.6596],\n        [180.6643],\n        [196.1718],\n        [141.9318]], grad_fn=<AddmmBackward>)\n\nEpoch 78000 / 100000 \tCost : 0.175612\nHypo : tensor([[151.4650],\n        [184.6589],\n        [180.6651],\n        [196.1692],\n        [141.9335]], grad_fn=<AddmmBackward>)\n\nEpoch 79000 / 100000 \tCost : 0.175406\nHypo : tensor([[151.4665],\n        [184.6581],\n        [180.6660],\n        [196.1668],\n        [141.9352]], grad_fn=<AddmmBackward>)\n\nEpoch 80000 / 100000 \tCost : 0.175212\nHypo : tensor([[151.4679],\n        [184.6575],\n        [180.6668],\n        [196.1644],\n        [141.9368]], grad_fn=<AddmmBackward>)\n\nEpoch 81000 / 100000 \tCost : 0.175011\nHypo : tensor([[151.4693],\n        [184.6569],\n        [180.6675],\n        [196.1621],\n        [141.9385]], grad_fn=<AddmmBackward>)\n\nEpoch 82000 / 100000 \tCost : 0.174827\nHypo : tensor([[151.4705],\n        [184.6563],\n        [180.6683],\n        [196.1597],\n        [141.9403]], grad_fn=<AddmmBackward>)\n\nEpoch 83000 / 100000 \tCost : 0.174640\nHypo : tensor([[151.4718],\n        [184.6557],\n        [180.6690],\n        [196.1574],\n        [141.9420]], grad_fn=<AddmmBackward>)\n\nEpoch 84000 / 100000 \tCost : 0.174473\nHypo : tensor([[151.4730],\n        [184.6551],\n        [180.6697],\n        [196.1552],\n        [141.9435]], grad_fn=<AddmmBackward>)\n\nEpoch 85000 / 100000 \tCost : 0.174316\nHypo : tensor([[151.4743],\n        [184.6545],\n        [180.6704],\n        [196.1531],\n        [141.9450]], grad_fn=<AddmmBackward>)\n\nEpoch 86000 / 100000 \tCost : 0.174166\nHypo : tensor([[151.4756],\n        [184.6539],\n        [180.6711],\n        [196.1511],\n        [141.9464]], grad_fn=<AddmmBackward>)\n\nEpoch 87000 / 100000 \tCost : 0.174020\nHypo : tensor([[151.4768],\n        [184.6533],\n        [180.6717],\n        [196.1491],\n        [141.9478]], grad_fn=<AddmmBackward>)\n\nEpoch 88000 / 100000 \tCost : 0.173878\nHypo : tensor([[151.4780],\n        [184.6528],\n        [180.6724],\n        [196.1471],\n        [141.9492]], grad_fn=<AddmmBackward>)\n\nEpoch 89000 / 100000 \tCost : 0.173745\nHypo : tensor([[151.4790],\n        [184.6523],\n        [180.6730],\n        [196.1451],\n        [141.9506]], grad_fn=<AddmmBackward>)\n\nEpoch 90000 / 100000 \tCost : 0.173610\nHypo : tensor([[151.4801],\n        [184.6519],\n        [180.6736],\n        [196.1432],\n        [141.9521]], grad_fn=<AddmmBackward>)\n\nEpoch 91000 / 100000 \tCost : 0.173491\nHypo : tensor([[151.4811],\n        [184.6514],\n        [180.6742],\n        [196.1413],\n        [141.9535]], grad_fn=<AddmmBackward>)\n\nEpoch 92000 / 100000 \tCost : 0.173371\nHypo : tensor([[151.4822],\n        [184.6509],\n        [180.6748],\n        [196.1395],\n        [141.9548]], grad_fn=<AddmmBackward>)\n\nEpoch 93000 / 100000 \tCost : 0.173259\nHypo : tensor([[151.4832],\n        [184.6504],\n        [180.6753],\n        [196.1377],\n        [141.9559]], grad_fn=<AddmmBackward>)\n\nEpoch 94000 / 100000 \tCost : 0.173155\nHypo : tensor([[151.4843],\n        [184.6498],\n        [180.6759],\n        [196.1360],\n        [141.9571]], grad_fn=<AddmmBackward>)\n\nEpoch 95000 / 100000 \tCost : 0.173057\nHypo : tensor([[151.4853],\n        [184.6494],\n        [180.6765],\n        [196.1344],\n        [141.9582]], grad_fn=<AddmmBackward>)\n\nEpoch 96000 / 100000 \tCost : 0.172962\nHypo : tensor([[151.4862],\n        [184.6489],\n        [180.6770],\n        [196.1328],\n        [141.9593]], grad_fn=<AddmmBackward>)\n\nEpoch 97000 / 100000 \tCost : 0.172860\nHypo : tensor([[151.4872],\n        [184.6485],\n        [180.6775],\n        [196.1312],\n        [141.9605]], grad_fn=<AddmmBackward>)\n\nEpoch 98000 / 100000 \tCost : 0.172773\nHypo : tensor([[151.4881],\n        [184.6481],\n        [180.6780],\n        [196.1296],\n        [141.9616]], grad_fn=<AddmmBackward>)\n\nEpoch 99000 / 100000 \tCost : 0.172683\nHypo : tensor([[151.4889],\n        [184.6477],\n        [180.6785],\n        [196.1281],\n        [141.9628]], grad_fn=<AddmmBackward>)\n\nEpoch 100000 / 100000 \tCost : 0.172601\nHypo : tensor([[151.4897],\n        [184.6474],\n        [180.6789],\n        [196.1265],\n        [141.9640]], grad_fn=<AddmmBackward>)\n\n"
        }
      ],
      "source": [
        "class MultivariableLinearRegressionModel(nn.Module) :\n",
        "    def __init__(self) :\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(3, 1)\n",
        "\n",
        "    def forward(self, x) :\n",
        "        return self.linear(x)\n",
        "\n",
        "model_multiple = MultivariableLinearRegressionModel()\n",
        "\n",
        "opti_multiple = torch.optim.SGD(model_multiple.parameters(), lr = 1e-5)\n",
        "\n",
        "epochs_multiple = int (1e+5)\n",
        "\n",
        "for epo in range( 1, epochs_multiple +1 ) :\n",
        "    hypo = model_multiple(x_train)\n",
        "\n",
        "    cost = F.mse_loss(hypo, y_train)\n",
        "\n",
        "    opti_multiple.zero_grad()\n",
        "    cost.backward()\n",
        "    opti_multiple.step()\n",
        "\n",
        "    if epo % 1000 == 0 : \n",
        "        print (f\"Epoch {epo:4d} / {epochs_multiple} \\tCost : {cost.item():.6f}\\nHypo : {hypo}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 06. Mini Batch & Data Load\n",
        "\n",
        "### 06-01. Mini Batch & Batch Size\n",
        "\n",
        "- mini batch : 전체 데이터를 작은 단위(batch)로 나누어 해당 단위별 학습\n",
        "- 1 epoch 마다 mini batch 개수만큼 GD 수행\n",
        "- mini batch gradient descent\n",
        "\n",
        "### 06-02. Iteration\n",
        "\n",
        "- total data 2000, batch size 200 일 때 1 epoch 당 Iteration 은 10\n",
        "- total data / batch size\n",
        "- 1 epoch 마다 매개변수 update가 10번 이뤄짐\n",
        "\n",
        "### 06-03. Data Load\n",
        "\n",
        "- `TensorDataset` : x_data, y_data\n",
        "- `DataLoader` : \n",
        "    - dataset ; TensorDataset\n",
        "    - batch_size; 일반적으로 2의 배수 사용\n",
        "    - shuffle; True = Epoch 마다 dataset shuffle, 권장\n",
        "\n",
        "```python\n",
        "DataLoader(\n",
        "    dataset,\n",
        "    batch_size=1,\n",
        "    shuffle=False,\n",
        "    sampler=None,\n",
        "    batch_sampler=None,\n",
        "    num_workers=0,\n",
        "    collate_fn=None,\n",
        "    pin_memory=False,\n",
        "    drop_last=False,\n",
        "    timeout=0,\n",
        "    worker_init_fn=None,\n",
        "    multiprocessing_context=None,\n",
        ")\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "text": "\u001b[0;31mInit signature:\u001b[0m\n\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mbatch_sampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mcollate_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mpin_memory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mdrop_last\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mworker_init_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m    \u001b[0mmultiprocessing_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;31mDocstring:\u001b[0m     \nData loader. Combines a dataset and a sampler, and provides an iterable over\nthe given dataset.\n\nThe :class:`~torch.utils.data.DataLoader` supports both map-style and\niterable-style datasets with single- or multi-process loading, customizing\nloading order and optional automatic batching (collation) and memory pinning.\n\nSee :py:mod:`torch.utils.data` documentation page for more details.\n\nArguments:\n    dataset (Dataset): dataset from which to load the data.\n    batch_size (int, optional): how many samples per batch to load\n        (default: ``1``).\n    shuffle (bool, optional): set to ``True`` to have the data reshuffled\n        at every epoch (default: ``False``).\n    sampler (Sampler, optional): defines the strategy to draw samples from\n        the dataset. If specified, :attr:`shuffle` must be ``False``.\n    batch_sampler (Sampler, optional): like :attr:`sampler`, but returns a batch of\n        indices at a time. Mutually exclusive with :attr:`batch_size`,\n        :attr:`shuffle`, :attr:`sampler`, and :attr:`drop_last`.\n    num_workers (int, optional): how many subprocesses to use for data\n        loading. ``0`` means that the data will be loaded in the main process.\n        (default: ``0``)\n    collate_fn (callable, optional): merges a list of samples to form a\n        mini-batch of Tensor(s).  Used when using batched loading from a\n        map-style dataset.\n    pin_memory (bool, optional): If ``True``, the data loader will copy Tensors\n        into CUDA pinned memory before returning them.  If your data elements\n        are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type,\n        see the example below.\n    drop_last (bool, optional): set to ``True`` to drop the last incomplete batch,\n        if the dataset size is not divisible by the batch size. If ``False`` and\n        the size of dataset is not divisible by the batch size, then the last batch\n        will be smaller. (default: ``False``)\n    timeout (numeric, optional): if positive, the timeout value for collecting a batch\n        from workers. Should always be non-negative. (default: ``0``)\n    worker_init_fn (callable, optional): If not ``None``, this will be called on each\n        worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as\n        input, after seeding and before data loading. (default: ``None``)\n\n\n.. warning:: If the ``spawn`` start method is used, :attr:`worker_init_fn`\n             cannot be an unpicklable object, e.g., a lambda function. See\n             :ref:`multiprocessing-best-practices` on more details related\n             to multiprocessing in PyTorch.\n\n.. note:: ``len(dataloader)`` heuristic is based on the length of the sampler used.\n          When :attr:`dataset` is an :class:`~torch.utils.data.IterableDataset`,\n          an infinite sampler is used, whose :meth:`__len__` is not\n          implemented, because the actual length depends on both the\n          iterable as well as multi-process loading configurations. So one\n          should not query this method unless they work with a map-style\n          dataset. See `Dataset Types`_ for more details on these two types\n          of datasets.\n\u001b[0;31mFile:\u001b[0m           /opt/conda/envs/py37/lib/python3.7/site-packages/torch/utils/data/dataloader.py\n\u001b[0;31mType:\u001b[0m           type\n\u001b[0;31mSubclasses:\u001b[0m     \n"
        }
      ],
      "source": [
        "DataLoader?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "x_train  =  torch.FloatTensor([[73,  80,  75], \n",
        "                               [93,  88,  93], \n",
        "                               [89,  91,  90], \n",
        "                               [96,  98,  100],   \n",
        "                               [73,  66,  70]])  \n",
        "y_train  =  torch.FloatTensor([[152],  [185],  [180],  [196],  [142]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = TensorDataset(x_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataloader = DataLoader(dataset, batch_size=3, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "tags": [
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 1000 / 10000 \tBatch : 1/2\tCost : 0.238667\nPrediction : tensor([[196.2700],\n        [151.1987],\n        [141.9671]], grad_fn=<AddmmBackward>)\n\nEpoch 1000 / 10000 \tBatch : 2/2\tCost : 0.165466\nPrediction : tensor([[184.8323],\n        [180.5503]], grad_fn=<AddmmBackward>)\n\nEpoch 2000 / 10000 \tBatch : 1/2\tCost : 0.127606\nPrediction : tensor([[184.7809],\n        [180.5775],\n        [141.9638]], grad_fn=<AddmmBackward>)\n\nEpoch 2000 / 10000 \tBatch : 2/2\tCost : 0.293860\nPrediction : tensor([[151.2896],\n        [196.2883]], grad_fn=<AddmmBackward>)\n\nEpoch 3000 / 10000 \tBatch : 1/2\tCost : 0.181046\nPrediction : tensor([[184.7761],\n        [151.4037],\n        [196.3708]], grad_fn=<AddmmBackward>)\n\nEpoch 3000 / 10000 \tBatch : 2/2\tCost : 0.226318\nPrediction : tensor([[180.6728],\n        [141.9970]], grad_fn=<AddmmBackward>)\n\nEpoch 4000 / 10000 \tBatch : 1/2\tCost : 0.168842\nPrediction : tensor([[151.3706],\n        [184.6863],\n        [141.8903]], grad_fn=<AddmmBackward>)\n\nEpoch 4000 / 10000 \tBatch : 2/2\tCost : 0.353602\nPrediction : tensor([[180.7099],\n        [196.4509]], grad_fn=<AddmmBackward>)\n\nEpoch 5000 / 10000 \tBatch : 1/2\tCost : 0.246003\nPrediction : tensor([[180.5080],\n        [151.3412],\n        [196.2144]], grad_fn=<AddmmBackward>)\n\nEpoch 5000 / 10000 \tBatch : 2/2\tCost : 0.102904\nPrediction : tensor([[141.8177],\n        [184.5846]], grad_fn=<AddmmBackward>)\n\nEpoch 6000 / 10000 \tBatch : 1/2\tCost : 0.269311\nPrediction : tensor([[151.4528],\n        [180.6307],\n        [196.3328]], grad_fn=<AddmmBackward>)\n\nEpoch 6000 / 10000 \tBatch : 2/2\tCost : 0.073927\nPrediction : tensor([[141.8667],\n        [184.6393]], grad_fn=<AddmmBackward>)\n\nEpoch 7000 / 10000 \tBatch : 1/2\tCost : 0.164599\nPrediction : tensor([[184.7333],\n        [141.9457],\n        [180.6479]], grad_fn=<AddmmBackward>)\n\nEpoch 7000 / 10000 \tBatch : 2/2\tCost : 0.202350\nPrediction : tensor([[196.2781],\n        [151.4279]], grad_fn=<AddmmBackward>)\n\nEpoch 8000 / 10000 \tBatch : 1/2\tCost : 0.074606\nPrediction : tensor([[196.1779],\n        [184.5900],\n        [141.8448]], grad_fn=<AddmmBackward>)\n\nEpoch 8000 / 10000 \tBatch : 2/2\tCost : 0.335386\nPrediction : tensor([[180.5692],\n        [151.4111]], grad_fn=<AddmmBackward>)\n\nEpoch 9000 / 10000 \tBatch : 1/2\tCost : 0.059390\nPrediction : tensor([[141.9532],\n        [184.7220],\n        [196.3141]], grad_fn=<AddmmBackward>)\n\nEpoch 9000 / 10000 \tBatch : 2/2\tCost : 0.344162\nPrediction : tensor([[151.4799],\n        [180.6464]], grad_fn=<AddmmBackward>)\n\nEpoch 10000 / 10000 \tBatch : 1/2\tCost : 0.060869\nPrediction : tensor([[196.2449],\n        [184.6607],\n        [141.9133]], grad_fn=<AddmmBackward>)\n\nEpoch 10000 / 10000 \tBatch : 2/2\tCost : 0.336999\nPrediction : tensor([[180.6213],\n        [151.4633]], grad_fn=<AddmmBackward>)\n\n"
        }
      ],
      "source": [
        "model = nn.Linear(3,1)\n",
        "optimi = torch.optim.SGD(model.parameters(), lr = 1e-5)\n",
        "\n",
        "epochs = int (1e+4)\n",
        "for epo in range( 1, epochs+1 ) :\n",
        "    for batch_idx, samples in enumerate (dataloader) :\n",
        "        # print (batch_idx); print (samples)\n",
        "        \n",
        "        x_train, y_train = samples\n",
        "\n",
        "        pred = model(x_train)\n",
        "\n",
        "        cost = F.mse_loss(pred, y_train)\n",
        "\n",
        "        optimi.zero_grad()\n",
        "        cost.backward()\n",
        "        optimi.step()\n",
        "\n",
        "        if epo % 1000 == 0 : \n",
        "            print (f\"Epoch {epo:4d} / {epochs} \\tBatch : {batch_idx+1}/{len(dataloader)}\\tCost : {cost.item():.6f}\\nPrediction : {pred}\\n\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "[Parameter containing:\n tensor([[0.9886, 0.4920, 0.5281]], requires_grad=True),\n Parameter containing:\n tensor([0.2905], requires_grad=True)]"
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "list (model.parameters())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 07. Custom Dataset\n",
        "\n",
        "### 07-01. Custom Dataset\n",
        "\n",
        "- `torch.utils.data.Dataset` : abstract class\n",
        "- `torch.utils.data.DataLoader`\n",
        "\n",
        "basic structure\n",
        "\n",
        "```python\n",
        "class CustomDataset(torch.utils.data.Dataset) :\n",
        "    def __init__(self) :\n",
        "        # for Preprocessing\n",
        "\n",
        "    def __len__(self) : \n",
        "        # length of samples\n",
        "\n",
        "    def __getitem__(self, idx) : \n",
        "        # get an sample from dataset\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 07-02. Linear Regression with Custom dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class CustomDataset(Dataset) :\n",
        "\n",
        "    def __init__(self) :\n",
        "        self.x_data = [[73, 80, 75],\n",
        "                    [93, 88, 93],\n",
        "                    [89, 91, 90],\n",
        "                    [96, 98, 100],\n",
        "                    [73, 66, 70]]\n",
        "        self.y_data = [[152], [185], [180], [196], [142]]\n",
        "\n",
        "    def __len__(self) : \n",
        "        return len(self.x_data)\n",
        "\n",
        "    def __getitem__(self, idx) : \n",
        "        x = torch.FloatTensor(self.x_data[idx])\n",
        "        y = torch.FloatTensor(self.y_data[idx])\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "tags": [
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend",
          "outputPrepend"
        ]
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "AddmmBackward>)\n\nEpoch 47000 / 100000 \tBatch : 1/3\tCost : 0.000388\nPrediction : tensor([[196.0008],\n        [141.9721]], grad_fn=<AddmmBackward>)\n\nEpoch 47000 / 100000 \tBatch : 2/3\tCost : 0.244522\nPrediction : tensor([[184.5369],\n        [151.4760]], grad_fn=<AddmmBackward>)\n\nEpoch 47000 / 100000 \tBatch : 3/3\tCost : 0.611950\nPrediction : tensor([[180.7823]], grad_fn=<AddmmBackward>)\n\nEpoch 48000 / 100000 \tBatch : 1/3\tCost : 0.087570\nPrediction : tensor([[151.6842],\n        [196.2746]], grad_fn=<AddmmBackward>)\n\nEpoch 48000 / 100000 \tBatch : 2/3\tCost : 0.037366\nPrediction : tensor([[142.1613],\n        [184.7793]], grad_fn=<AddmmBackward>)\n\nEpoch 48000 / 100000 \tBatch : 3/3\tCost : 0.677929\nPrediction : tensor([[180.8234]], grad_fn=<AddmmBackward>)\n\nEpoch 49000 / 100000 \tBatch : 1/3\tCost : 0.339987\nPrediction : tensor([[180.7320],\n        [151.6204]], grad_fn=<AddmmBackward>)\n\nEpoch 49000 / 100000 \tBatch : 2/3\tCost : 0.076501\nPrediction : tensor([[184.6102],\n        [142.0331]], grad_fn=<AddmmBackward>)\n\nEpoch 49000 / 100000 \tBatch : 3/3\tCost : 0.032943\nPrediction : tensor([[196.1815]], grad_fn=<AddmmBackward>)\n\nEpoch 50000 / 100000 \tBatch : 1/3\tCost : 0.087754\nPrediction : tensor([[196.2664],\n        [151.6767]], grad_fn=<AddmmBackward>)\n\nEpoch 50000 / 100000 \tBatch : 2/3\tCost : 0.037764\nPrediction : tensor([[142.1622],\n        [184.7782]], grad_fn=<AddmmBackward>)\n\nEpoch 50000 / 100000 \tBatch : 3/3\tCost : 0.672562\nPrediction : tensor([[180.8201]], grad_fn=<AddmmBackward>)\n\nEpoch 51000 / 100000 \tBatch : 1/3\tCost : 0.339121\nPrediction : tensor([[184.7786],\n        [180.7932]], grad_fn=<AddmmBackward>)\n\nEpoch 51000 / 100000 \tBatch : 2/3\tCost : 0.106284\nPrediction : tensor([[196.1091],\n        [151.5520]], grad_fn=<AddmmBackward>)\n\nEpoch 51000 / 100000 \tBatch : 3/3\tCost : 0.011149\nPrediction : tensor([[142.1056]], grad_fn=<AddmmBackward>)\n\nEpoch 52000 / 100000 \tBatch : 1/3\tCost : 0.327245\nPrediction : tensor([[184.7557],\n        [180.7712]], grad_fn=<AddmmBackward>)\n\nEpoch 52000 / 100000 \tBatch : 2/3\tCost : 0.105386\nPrediction : tensor([[142.0475],\n        [151.5434]], grad_fn=<AddmmBackward>)\n\nEpoch 52000 / 100000 \tBatch : 3/3\tCost : 0.035973\nPrediction : tensor([[196.1897]], grad_fn=<AddmmBackward>)\n\nEpoch 53000 / 100000 \tBatch : 1/3\tCost : 0.060407\nPrediction : tensor([[151.7088],\n        [142.1898]], grad_fn=<AddmmBackward>)\n\nEpoch 53000 / 100000 \tBatch : 2/3\tCost : 0.381983\nPrediction : tensor([[184.8394],\n        [180.8592]], grad_fn=<AddmmBackward>)\n\nEpoch 53000 / 100000 \tBatch : 3/3\tCost : 0.021373\nPrediction : tensor([[196.1462]], grad_fn=<AddmmBackward>)\n\nEpoch 54000 / 100000 \tBatch : 1/3\tCost : 0.091679\nPrediction : tensor([[180.3867],\n        [195.8161]], grad_fn=<AddmmBackward>)\n\nEpoch 54000 / 100000 \tBatch : 2/3\tCost : 0.252031\nPrediction : tensor([[184.3159],\n        [141.8099]], grad_fn=<AddmmBackward>)\n\nEpoch 54000 / 100000 \tBatch : 3/3\tCost : 0.288224\nPrediction : tensor([[151.4631]], grad_fn=<AddmmBackward>)\n\nEpoch 55000 / 100000 \tBatch : 1/3\tCost : 0.467842\nPrediction : tensor([[184.3333],\n        [151.2992]], grad_fn=<AddmmBackward>)\n\nEpoch 55000 / 100000 \tBatch : 2/3\tCost : 0.221749\nPrediction : tensor([[142.0632],\n        [180.6629]], grad_fn=<AddmmBackward>)\n\nEpoch 55000 / 100000 \tBatch : 3/3\tCost : 0.005041\nPrediction : tensor([[195.9290]], grad_fn=<AddmmBackward>)\n\nEpoch 56000 / 100000 \tBatch : 1/3\tCost : 0.031884\nPrediction : tensor([[141.8393],\n        [195.8052]], grad_fn=<AddmmBackward>)\n\nEpoch 56000 / 100000 \tBatch : 2/3\tCost : 0.345767\nPrediction : tensor([[184.4362],\n        [151.3887]], grad_fn=<AddmmBackward>)\n\nEpoch 56000 / 100000 \tBatch : 3/3\tCost : 0.522717\nPrediction : tensor([[180.7230]], grad_fn=<AddmmBackward>)\n\nEpoch 57000 / 100000 \tBatch : 1/3\tCost : 0.036945\nPrediction : tensor([[184.7894],\n        [142.1719]], grad_fn=<AddmmBackward>)\n\nEpoch 57000 / 100000 \tBatch : 2/3\tCost : 0.385335\nPrediction : tensor([[180.8267],\n        [196.2953]], grad_fn=<AddmmBackward>)\n\nEpoch 57000 / 100000 \tBatch : 3/3\tCost : 0.286719\nPrediction : tensor([[151.4645]], grad_fn=<AddmmBackward>)\n\nEpoch 58000 / 100000 \tBatch : 1/3\tCost : 0.250403\nPrediction : tensor([[151.4708],\n        [184.5302]], grad_fn=<AddmmBackward>)\n\nEpoch 58000 / 100000 \tBatch : 2/3\tCost : 0.313399\nPrediction : tensor([[180.7778],\n        [142.1476]], grad_fn=<AddmmBackward>)\n\nEpoch 58000 / 100000 \tBatch : 3/3\tCost : 0.000030\nPrediction : tensor([[196.0055]], grad_fn=<AddmmBackward>)\n\nEpoch 59000 / 100000 \tBatch : 1/3\tCost : 0.326405\nPrediction : tensor([[142.1614],\n        [180.7917]], grad_fn=<AddmmBackward>)\n\nEpoch 59000 / 100000 \tBatch : 2/3\tCost : 0.235997\nPrediction : tensor([[184.5486],\n        [151.4821]], grad_fn=<AddmmBackward>)\n\nEpoch 59000 / 100000 \tBatch : 3/3\tCost : 0.063189\nPrediction : tensor([[196.2514]], grad_fn=<AddmmBackward>)\n\nEpoch 60000 / 100000 \tBatch : 1/3\tCost : 0.335103\nPrediction : tensor([[184.7645],\n        [180.7840]], grad_fn=<AddmmBackward>)\n\nEpoch 60000 / 100000 \tBatch : 2/3\tCost : 0.104979\nPrediction : tensor([[196.1042],\n        [151.5538]], grad_fn=<AddmmBackward>)\n\nEpoch 60000 / 100000 \tBatch : 3/3\tCost : 0.009977\nPrediction : tensor([[142.0999]], grad_fn=<AddmmBackward>)\n\nEpoch 61000 / 100000 \tBatch : 1/3\tCost : 0.191999\nPrediction : tensor([[184.5983],\n        [151.5282]], grad_fn=<AddmmBackward>)\n\nEpoch 61000 / 100000 \tBatch : 2/3\tCost : 0.349233\nPrediction : tensor([[142.1778],\n        [180.8166]], grad_fn=<AddmmBackward>)\n\nEpoch 61000 / 100000 \tBatch : 3/3\tCost : 0.000966\nPrediction : tensor([[196.0311]], grad_fn=<AddmmBackward>)\n\nEpoch 62000 / 100000 \tBatch : 1/3\tCost : 0.002708\nPrediction : tensor([[195.9530],\n        [141.9434]], grad_fn=<AddmmBackward>)\n\nEpoch 62000 / 100000 \tBatch : 2/3\tCost : 0.266264\nPrediction : tensor([[184.5127],\n        [151.4568]], grad_fn=<AddmmBackward>)\n\nEpoch 62000 / 100000 \tBatch : 3/3\tCost : 0.589574\nPrediction : tensor([[180.7678]], grad_fn=<AddmmBackward>)\n\nEpoch 63000 / 100000 \tBatch : 1/3\tCost : 0.068198\nPrediction : tensor([[184.6530],\n        [196.1265]], grad_fn=<AddmmBackward>)\n\nEpoch 63000 / 100000 \tBatch : 2/3\tCost : 0.080524\nPrediction : tensor([[142.1097],\n        [151.6140]], grad_fn=<AddmmBackward>)\n\nEpoch 63000 / 100000 \tBatch : 3/3\tCost : 0.612881\nPrediction : tensor([[180.7829]], grad_fn=<AddmmBackward>)\n\nEpoch 64000 / 100000 \tBatch : 1/3\tCost : 0.247775\nPrediction : tensor([[151.3157],\n        [141.8349]], grad_fn=<AddmmBackward>)\n\nEpoch 64000 / 100000 \tBatch : 2/3\tCost : 0.261986\nPrediction : tensor([[180.5417],\n        [184.5199]], grad_fn=<AddmmBackward>)\n\nEpoch 64000 / 100000 \tBatch : 3/3\tCost : 0.000895\nPrediction : tensor([[195.9701]], grad_fn=<AddmmBackward>)\n\nEpoch 65000 / 100000 \tBatch : 1/3\tCost : 0.307299\nPrediction : tensor([[180.6325],\n        [151.5369]], grad_fn=<AddmmBackward>)\n\nEpoch 65000 / 100000 \tBatch : 2/3\tCost : 0.099828\nPrediction : tensor([[184.5532],\n        [141.9932]], grad_fn=<AddmmBackward>)\n\nEpoch 65000 / 100000 \tBatch : 3/3\tCost : 0.019904\nPrediction : tensor([[196.1411]], grad_fn=<AddmmBackward>)\n\nEpoch 66000 / 100000 \tBatch : 1/3\tCost : 0.024745\nPrediction : tensor([[195.8298],\n        [141.8567]], grad_fn=<AddmmBackward>)\n\nEpoch 66000 / 100000 \tBatch : 2/3\tCost : 0.289797\nPrediction : tensor([[180.4717],\n        [151.4024]], grad_fn=<AddmmBackward>)\n\nEpoch 66000 / 100000 \tBatch : 3/3\tCost : 0.296059\nPrediction : tensor([[184.4559]], grad_fn=<AddmmBackward>)\n\nEpoch 67000 / 100000 \tBatch : 1/3\tCost : 0.035336\nPrediction : tensor([[184.8217],\n        [142.1972]], grad_fn=<AddmmBackward>)\n\nEpoch 67000 / 100000 \tBatch : 2/3\tCost : 0.089390\nPrediction : tensor([[151.7215],\n        [196.3181]], grad_fn=<AddmmBackward>)\n\nEpoch 67000 / 100000 \tBatch : 3/3\tCost : 0.674365\nPrediction : tensor([[180.8212]], grad_fn=<AddmmBackward>)\n\nEpoch 68000 / 100000 \tBatch : 1/3\tCost : 0.047595\nPrediction : tensor([[184.7137],\n        [142.1150]], grad_fn=<AddmmBackward>)\n\nEpoch 68000 / 100000 \tBatch : 2/3\tCost : 0.362692\nPrediction : tensor([[180.7843],\n        [151.6678]], grad_fn=<AddmmBackward>)\n\nEpoch 68000 / 100000 \tBatch : 3/3\tCost : 0.013209\nPrediction : tensor([[196.1149]], grad_fn=<AddmmBackward>)\n\nEpoch 69000 / 100000 \tBatch : 1/3\tCost : 0.181468\nPrediction : tensor([[196.0478],\n        [180.6005]], grad_fn=<AddmmBackward>)\n\nEpoch 69000 / 100000 \tBatch : 2/3\tCost : 0.176796\nPrediction : tensor([[184.4160],\n        [141.8883]], grad_fn=<AddmmBackward>)\n\nEpoch 69000 / 100000 \tBatch : 3/3\tCost : 0.232201\nPrediction : tensor([[151.5181]], grad_fn=<AddmmBackward>)\n\nEpoch 70000 / 100000 \tBatch : 1/3\tCost : 0.121403\nPrediction : tensor([[184.5090],\n        [141.9590]], grad_fn=<AddmmBackward>)\n\nEpoch 70000 / 100000 \tBatch : 2/3\tCost : 0.226730\nPrediction : tensor([[196.1162],\n        [180.6633]], grad_fn=<AddmmBackward>)\n\nEpoch 70000 / 100000 \tBatch : 3/3\tCost : 0.354644\nPrediction : tensor([[151.4045]], grad_fn=<AddmmBackward>)\n\nEpoch 71000 / 100000 \tBatch : 1/3\tCost : 0.154021\nPrediction : tensor([[151.5722],\n        [184.6464]], grad_fn=<AddmmBackward>)\n\nEpoch 71000 / 100000 \tBatch : 2/3\tCost : 0.069062\nPrediction : tensor([[196.3135],\n        [142.1996]], grad_fn=<AddmmBackward>)\n\nEpoch 71000 / 100000 \tBatch : 3/3\tCost : 0.524661\nPrediction : tensor([[180.7243]], grad_fn=<AddmmBackward>)\n\nEpoch 72000 / 100000 \tBatch : 1/3\tCost : 0.086212\nPrediction : tensor([[180.3845],\n        [141.8431]], grad_fn=<AddmmBackward>)\n\nEpoch 72000 / 100000 \tBatch : 2/3\tCost : 0.283733\nPrediction : tensor([[195.7429],\n        [184.2919]], grad_fn=<AddmmBackward>)\n\nEpoch 72000 / 100000 \tBatch : 3/3\tCost : 0.268124\nPrediction : tensor([[151.4822]], grad_fn=<AddmmBackward>)\n\nEpoch 73000 / 100000 \tBatch : 1/3\tCost : 0.126539\nPrediction : tensor([[151.4969],\n        [141.9991]], grad_fn=<AddmmBackward>)\n\nEpoch 73000 / 100000 \tBatch : 2/3\tCost : 0.066101\nPrediction : tensor([[196.1414],\n        [184.6650]], grad_fn=<AddmmBackward>)\n\nEpoch 73000 / 100000 \tBatch : 3/3\tCost : 0.534477\nPrediction : tensor([[180.7311]], grad_fn=<AddmmBackward>)\n\nEpoch 74000 / 100000 \tBatch : 1/3\tCost : 0.072521\nPrediction : tensor([[184.7874],\n        [151.6840]], grad_fn=<AddmmBackward>)\n\nEpoch 74000 / 100000 \tBatch : 2/3\tCost : 0.460157\nPrediction : tensor([[142.2639],\n        [180.9223]], grad_fn=<AddmmBackward>)\n\nEpoch 74000 / 100000 \tBatch : 3/3\tCost : 0.010167\nPrediction : tensor([[196.1008]], grad_fn=<AddmmBackward>)\n\nEpoch 75000 / 100000 \tBatch : 1/3\tCost : 0.086342\nPrediction : tensor([[151.6949],\n        [196.2822]], grad_fn=<AddmmBackward>)\n\nEpoch 75000 / 100000 \tBatch : 2/3\tCost : 0.336986\nPrediction : tensor([[180.8034],\n        [142.1689]], grad_fn=<AddmmBackward>)\n\nEpoch 75000 / 100000 \tBatch : 3/3\tCost : 0.200183\nPrediction : tensor([[184.5526]], grad_fn=<AddmmBackward>)\n\nEpoch 76000 / 100000 \tBatch : 1/3\tCost : 0.058950\nPrediction : tensor([[196.2899],\n        [142.1841]], grad_fn=<AddmmBackward>)\n\nEpoch 76000 / 100000 \tBatch : 2/3\tCost : 0.300587\nPrediction : tensor([[180.7106],\n        [184.6898]], grad_fn=<AddmmBackward>)\n\nEpoch 76000 / 100000 \tBatch : 3/3\tCost : 0.226022\nPrediction : tensor([[151.5246]], grad_fn=<AddmmBackward>)\n\nEpoch 77000 / 100000 \tBatch : 1/3\tCost : 0.139882\nPrediction : tensor([[195.9947],\n        [151.4711]], grad_fn=<AddmmBackward>)\n\nEpoch 77000 / 100000 \tBatch : 2/3\tCost : 0.283454\nPrediction : tensor([[184.6390],\n        [180.6608]], grad_fn=<AddmmBackward>)\n\nEpoch 77000 / 100000 \tBatch : 3/3\tCost : 0.000015\nPrediction : tensor([[142.0038]], grad_fn=<AddmmBackward>)\n\nEpoch 78000 / 100000 \tBatch : 1/3\tCost : 0.055767\nPrediction : tensor([[142.1789],\n        [196.2820]], grad_fn=<AddmmBackward>)\n\nEpoch 78000 / 100000 \tBatch : 2/3\tCost : 0.128798\nPrediction : tensor([[151.6013],\n        [184.6859]], grad_fn=<AddmmBackward>)\n\nEpoch 78000 / 100000 \tBatch : 3/3\tCost : 0.748261\nPrediction : tensor([[180.8650]], grad_fn=<AddmmBackward>)\n\nEpoch 79000 / 100000 \tBatch : 1/3\tCost : 0.283990\nPrediction : tensor([[180.3650],\n        [184.3407]], grad_fn=<AddmmBackward>)\n\nEpoch 79000 / 100000 \tBatch : 2/3\tCost : 0.014107\nPrediction : tensor([[195.8737],\n        [141.8893]], grad_fn=<AddmmBackward>)\n\nEpoch 79000 / 100000 \tBatch : 3/3\tCost : 0.334723\nPrediction : tensor([[151.4214]], grad_fn=<AddmmBackward>)\n\nEpoch 80000 / 100000 \tBatch : 1/3\tCost : 0.090543\nPrediction : tensor([[196.2000],\n        [151.6244]], grad_fn=<AddmmBackward>)\n\nEpoch 80000 / 100000 \tBatch : 2/3\tCost : 0.300512\nPrediction : tensor([[142.1443],\n        [180.7617]], grad_fn=<AddmmBackward>)\n\nEpoch 80000 / 100000 \tBatch : 3/3\tCost : 0.218270\nPrediction : tensor([[184.5328]], grad_fn=<AddmmBackward>)\n\nEpoch 81000 / 100000 \tBatch : 1/3\tCost : 0.231982\nPrediction : tensor([[141.8554],\n        [151.3344]], grad_fn=<AddmmBackward>)\n\nEpoch 81000 / 100000 \tBatch : 2/3\tCost : 0.261838\nPrediction : tensor([[180.5555],\n        [184.5362]], grad_fn=<AddmmBackward>)\n\nEpoch 81000 / 100000 \tBatch : 3/3\tCost : 0.000456\nPrediction : tensor([[195.9787]], grad_fn=<AddmmBackward>)\n\nEpoch 82000 / 100000 \tBatch : 1/3\tCost : 0.087464\nPrediction : tensor([[196.0549],\n        [184.5854]], grad_fn=<AddmmBackward>)\n\nEpoch 82000 / 100000 \tBatch : 2/3\tCost : 0.088024\nPrediction : tensor([[142.0873],\n        [151.5896]], grad_fn=<AddmmBackward>)\n\nEpoch 82000 / 100000 \tBatch : 3/3\tCost : 0.578450\nPrediction : tensor([[180.7606]], grad_fn=<AddmmBackward>)\n\nEpoch 83000 / 100000 \tBatch : 1/3\tCost : 0.110552\nPrediction : tensor([[151.5363],\n        [196.0777]], grad_fn=<AddmmBackward>)\n\nEpoch 83000 / 100000 \tBatch : 2/3\tCost : 0.055535\nPrediction : tensor([[142.0892],\n        [184.6789]], grad_fn=<AddmmBackward>)\n\nEpoch 83000 / 100000 \tBatch : 3/3\tCost : 0.582449\nPrediction : tensor([[180.7632]], grad_fn=<AddmmBackward>)\n\nEpoch 84000 / 100000 \tBatch : 1/3\tCost : 0.274066\nPrediction : tensor([[184.3812],\n        [180.4064]], grad_fn=<AddmmBackward>)\n\nEpoch 84000 / 100000 \tBatch : 2/3\tCost : 0.187991\nPrediction : tensor([[195.8973],\n        [151.3955]], grad_fn=<AddmmBackward>)\n\nEpoch 84000 / 100000 \tBatch : 3/3\tCost : 0.000452\nPrediction : tensor([[142.0213]], grad_fn=<AddmmBackward>)\n\nEpoch 85000 / 100000 \tBatch : 1/3\tCost : 0.090381\nPrediction : tensor([[142.0104],\n        [184.5750]], grad_fn=<AddmmBackward>)\n\nEpoch 85000 / 100000 \tBatch : 2/3\tCost : 0.092823\nPrediction : tensor([[151.5996],\n        [196.1591]], grad_fn=<AddmmBackward>)\n\nEpoch 85000 / 100000 \tBatch : 3/3\tCost : 0.549281\nPrediction : tensor([[180.7411]], grad_fn=<AddmmBackward>)\n\nEpoch 86000 / 100000 \tBatch : 1/3\tCost : 0.073352\nPrediction : tensor([[142.1382],\n        [151.6428]], grad_fn=<AddmmBackward>)\n\nEpoch 86000 / 100000 \tBatch : 2/3\tCost : 0.059374\nPrediction : tensor([[184.7884],\n        [196.2720]], grad_fn=<AddmmBackward>)\n\nEpoch 86000 / 100000 \tBatch : 3/3\tCost : 0.614411\nPrediction : tensor([[180.7838]], grad_fn=<AddmmBackward>)\n\nEpoch 87000 / 100000 \tBatch : 1/3\tCost : 0.196162\nPrediction : tensor([[184.3921],\n        [195.8490]], grad_fn=<AddmmBackward>)\n\nEpoch 87000 / 100000 \tBatch : 2/3\tCost : 0.299814\nPrediction : tensor([[180.6051],\n        [151.5168]], grad_fn=<AddmmBackward>)\n\nEpoch 87000 / 100000 \tBatch : 3/3\tCost : 0.000340\nPrediction : tensor([[141.9816]], grad_fn=<AddmmBackward>)\n\nEpoch 88000 / 100000 \tBatch : 1/3\tCost : 0.098364\nPrediction : tensor([[195.8518],\n        [180.4180]], grad_fn=<AddmmBackward>)\n\nEpoch 88000 / 100000 \tBatch : 2/3\tCost : 0.463915\nPrediction : tensor([[184.3314],\n        [151.3066]], grad_fn=<AddmmBackward>)\n\nEpoch 88000 / 100000 \tBatch : 3/3\tCost : 0.004002\nPrediction : tensor([[142.0633]], grad_fn=<AddmmBackward>)\n\nEpoch 89000 / 100000 \tBatch : 1/3\tCost : 0.276465\nPrediction : tensor([[184.6096],\n        [180.6329]], grad_fn=<AddmmBackward>)\n\nEpoch 89000 / 100000 \tBatch : 2/3\tCost : 0.128065\nPrediction : tensor([[196.0227],\n        [151.4944]], grad_fn=<AddmmBackward>)\n\nEpoch 89000 / 100000 \tBatch : 3/3\tCost : 0.004604\nPrediction : tensor([[142.0679]], grad_fn=<AddmmBackward>)\n\nEpoch 90000 / 100000 \tBatch : 1/3\tCost : 0.267773\nPrediction : tensor([[142.1109],\n        [180.7234]], grad_fn=<AddmmBackward>)\n\nEpoch 90000 / 100000 \tBatch : 2/3\tCost : 0.151228\nPrediction : tensor([[195.9704],\n        [151.4508]], grad_fn=<AddmmBackward>)\n\nEpoch 90000 / 100000 \tBatch : 3/3\tCost : 0.138550\nPrediction : tensor([[184.6278]], grad_fn=<AddmmBackward>)\n\nEpoch 91000 / 100000 \tBatch : 1/3\tCost : 0.288098\nPrediction : tensor([[180.3510],\n        [184.3270]], grad_fn=<AddmmBackward>)\n\nEpoch 91000 / 100000 \tBatch : 2/3\tCost : 0.206776\nPrediction : tensor([[195.8670],\n        [151.3708]], grad_fn=<AddmmBackward>)\n\nEpoch 91000 / 100000 \tBatch : 3/3\tCost : 0.000143\nPrediction : tensor([[142.0120]], grad_fn=<AddmmBackward>)\n\nEpoch 92000 / 100000 \tBatch : 1/3\tCost : 0.223632\nPrediction : tensor([[141.8637],\n        [151.3453]], grad_fn=<AddmmBackward>)\n\nEpoch 92000 / 100000 \tBatch : 2/3\tCost : 0.104597\nPrediction : tensor([[184.5427],\n        [196.0096]], grad_fn=<AddmmBackward>)\n\nEpoch 92000 / 100000 \tBatch : 3/3\tCost : 0.452319\nPrediction : tensor([[180.6725]], grad_fn=<AddmmBackward>)\n\nEpoch 93000 / 100000 \tBatch : 1/3\tCost : 0.299899\nPrediction : tensor([[180.7091],\n        [184.6887]], grad_fn=<AddmmBackward>)\n\nEpoch 93000 / 100000 \tBatch : 2/3\tCost : 0.112629\nPrediction : tensor([[151.5260],\n        [142.0242]], grad_fn=<AddmmBackward>)\n\nEpoch 93000 / 100000 \tBatch : 3/3\tCost : 0.027470\nPrediction : tensor([[196.1657]], grad_fn=<AddmmBackward>)\n\nEpoch 94000 / 100000 \tBatch : 1/3\tCost : 0.210869\nPrediction : tensor([[184.5741],\n        [151.5098]], grad_fn=<AddmmBackward>)\n\nEpoch 94000 / 100000 \tBatch : 2/3\tCost : 0.356756\nPrediction : tensor([[180.8007],\n        [196.2690]], grad_fn=<AddmmBackward>)\n\nEpoch 94000 / 100000 \tBatch : 3/3\tCost : 0.001264\nPrediction : tensor([[141.9644]], grad_fn=<AddmmBackward>)\n\nEpoch 95000 / 100000 \tBatch : 1/3\tCost : 0.300282\nPrediction : tensor([[180.6139],\n        [151.5271]], grad_fn=<AddmmBackward>)\n\nEpoch 95000 / 100000 \tBatch : 2/3\tCost : 0.000171\nPrediction : tensor([[196.0083],\n        [141.9835]], grad_fn=<AddmmBackward>)\n\nEpoch 95000 / 100000 \tBatch : 3/3\tCost : 0.212126\nPrediction : tensor([[184.5394]], grad_fn=<AddmmBackward>)\n\nEpoch 96000 / 100000 \tBatch : 1/3\tCost : 0.070796\nPrediction : tensor([[184.7887],\n        [151.6886]], grad_fn=<AddmmBackward>)\n\nEpoch 96000 / 100000 \tBatch : 2/3\tCost : 0.460622\nPrediction : tensor([[180.9226],\n        [142.2648]], grad_fn=<AddmmBackward>)\n\nEpoch 96000 / 100000 \tBatch : 3/3\tCost : 0.010643\nPrediction : tensor([[196.1032]], grad_fn=<AddmmBackward>)\n\nEpoch 97000 / 100000 \tBatch : 1/3\tCost : 0.325735\nPrediction : tensor([[142.1655],\n        [180.7900]], grad_fn=<AddmmBackward>)\n\nEpoch 97000 / 100000 \tBatch : 2/3\tCost : 0.133236\nPrediction : tensor([[196.0150],\n        [151.4840]], grad_fn=<AddmmBackward>)\n\nEpoch 97000 / 100000 \tBatch : 3/3\tCost : 0.121141\nPrediction : tensor([[184.6519]], grad_fn=<AddmmBackward>)\n\nEpoch 98000 / 100000 \tBatch : 1/3\tCost : 0.058838\nPrediction : tensor([[196.2395],\n        [184.7544]], grad_fn=<AddmmBackward>)\n\nEpoch 98000 / 100000 \tBatch : 2/3\tCost : 0.068863\nPrediction : tensor([[142.1464],\n        [151.6590]], grad_fn=<AddmmBackward>)\n\nEpoch 98000 / 100000 \tBatch : 3/3\tCost : 0.661074\nPrediction : tensor([[180.8131]], grad_fn=<AddmmBackward>)\n\nEpoch 99000 / 100000 \tBatch : 1/3\tCost : 0.192438\nPrediction : tensor([[195.8564],\n        [184.3965]], grad_fn=<AddmmBackward>)\n\nEpoch 99000 / 100000 \tBatch : 2/3\tCost : 0.114115\nPrediction : tensor([[142.0198],\n        [151.5227]], grad_fn=<AddmmBackward>)\n\nEpoch 99000 / 100000 \tBatch : 3/3\tCost : 0.493591\nPrediction : tensor([[180.7026]], grad_fn=<AddmmBackward>)\n\nEpoch 100000 / 100000 \tBatch : 1/3\tCost : 0.086664\nPrediction : tensor([[151.6587],\n        [196.2384]], grad_fn=<AddmmBackward>)\n\nEpoch 100000 / 100000 \tBatch : 2/3\tCost : 0.315288\nPrediction : tensor([[180.7788],\n        [142.1551]], grad_fn=<AddmmBackward>)\n\nEpoch 100000 / 100000 \tBatch : 3/3\tCost : 0.211227\nPrediction : tensor([[184.5404]], grad_fn=<AddmmBackward>)\n\n"
        }
      ],
      "source": [
        "dataset = CustomDataset()\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
        "\n",
        "model = torch.nn.Linear(3, 1)\n",
        "opti = torch.optim.SGD(model.parameters(), lr = 1e-5)\n",
        "\n",
        "epochs = int (1e+5)\n",
        "for epo in range(1, epochs+1) :\n",
        "    for batch_idx, samples in enumerate(dataloader) :\n",
        "        x_train, y_train = samples\n",
        "        \n",
        "        pred = model(x_train)\n",
        "        cost = F.mse_loss(pred, y_train)\n",
        "\n",
        "        opti.zero_grad()\n",
        "        cost.backward()\n",
        "        opti.step()\n",
        "\n",
        "        if epo % 1000 == 0 : \n",
        "            print (f\"Epoch {epo:4d} / {epochs} \\tBatch : {batch_idx+1}/{len(dataloader)}\\tCost : {cost.item():.6f}\\nPrediction : {pred}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": "[Parameter containing:\n tensor([[1.0287, 0.5120, 0.4683]], requires_grad=True),\n Parameter containing:\n tensor([0.4940], requires_grad=True)]"
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "list (model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[[73, 80, 75]] 에 대한 예측 값 : tensor([[151.6714]], grad_fn=<AddmmBackward>)\n"
        }
      ],
      "source": [
        "new = [[73, 80, 75]]\n",
        "new_var =  torch.FloatTensor(new) \n",
        "print (f\"{new} 에 대한 예측 값 : {model(new_var)}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "2020-05-19-03_Linear_Regression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}